---
title: "GLMs, overdispersion, and goodness-of-fit"
author: "Peter Ralph"
date: "14 January 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```


# Generalized Linear Models

## Ingredients of a GLM:

1. A probability distribution, $Y$.

  > *(the "family"; describes the output)*

2. A linear predictor, $X \beta$.

  > *(connects input to output)*

3. A link function, $\E[Y] = h(X\beta)$.

  > *(connects linear predictor to probability distribution)*

## Common choices:

- Linear regression:
  $$ Y \sim \Normal(X \beta, \sigma) .$$

- Logistic regression:
  $$ Y \sim \Binom(n, \logistic(X\beta)) .$$

- Gamma regression:
  $$ Y \sim \Gam(\text{scale}=\exp(X\beta)/k, \text{shape}=k) .$$

## Logistic regression, in R

```
glm(y ~ x, family=binomial)
```

. . .

```
family {stats}

Family objects provide a convenient way to specify the details of the models used by functions such as glm.

binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")

Arguments

link: a specification for the model link function. This can be a name/expression, a literal character string, a length-one character vector, or an object of class "link-glm" (such as generated by make.link) provided it is not specified via one of the standard names given next.

The gaussian family accepts the links (as names) identity, log and inverse; the binomial family the links logit, probit, cauchit, (corresponding to logistic, normal and Cauchy CDFs respectively) log and cloglog (complementary log-log); the Gamma family the links inverse, identity and log; the poisson family the links log, identity, and sqrt; and the inverse.gaussian family the links 1/mu^2, inverse, identity and log.
```

# Unpacking logistic regression

## Simulate data

100 trials, where probability of success depends on $x$:
```{r sim_logistic, fig.width=2*fig.dim}
alpha <- -7; beta <- 1.2
x <- runif(100, 0, 10)
y <- alpha + beta * x
p <- 1 / (1 + exp(-y))
z <- rbinom(100, size=1, prob=p)
plot(z ~ x)
curve(1/(1+exp(-(-7 + 1.2 *x))), 0, 10, col='red', add=TRUE)
```

## `glm()`

```{r run_glm}
zz <- cbind(z, 1-z)
summary(glm_fit <- glm(zz ~ x, family='binomial'))
```

## Stan

::: {.columns}
:::::: {.column width=50%}

$$\begin{aligned}
    Z &\sim \Binom(1, P) \\
    P &= \logistic(\alpha + \beta X)
\end{aligned}$$

:::
:::::: {.column width=50%}


```{r stan_logistic, cache=TRUE}
logistic_block <- "
data {
    int N;
    vector[N] X;
    int<lower=0> Z[N];
}
parameters {
    real alpha;
    real beta;
}
model {
    vector[N] p;
    p = inv_logit(alpha + beta * X);
    Z ~ binomial(1, p);
}"
```

:::
::::::

## Stan: fit the model

```{r run_stan_logistic, cache=TRUE, dependson="stan_logistic"}
fit <- stan(model_code=logistic_block,
            data=list(N=100, X=x, Z=z))
rstan::summary(fit)
```

## Stan: posterior distributions

```{r plot_stan_logistic, fig.width=2*fig.dim}
samples <- extract(fit)
layout(t(1:2))
hist(samples$alpha, main=expression(alpha))
abline(v=alpha, col='red'); abline(v=coef(glm_fit)[1], col='green')
hist(samples$beta, main=expression(beta))
abline(v=beta, col='red'); abline(v=coef(glm_fit)[2], col='green')
legend("topright", lty=1, col=c('red', 'green'), legend=c('truth', 'glm'))
```

# Identify the GLM

## 

::: {.columns}
:::::: {.column width=60%}

```
data {
  int N;
  vector[N] X;
  vector[N] Y;
  vector<lower=0> Z[N];
}
parameters {
  real beta[2];
}
model {
  Z ~ gamma(1, exp(- beta[1] * X - beta[2] * Y);
}
```

:::
:::::: {.column width=40%}

What is

1. the probability distribution?
  *(describes the output)*

2. the linear predictor?
  *(connects input to output)*

3. the link function?
  *(connects linear predictor to probability distribution)*


**Then,** simulate from it.

:::
::::::




# Stochastic minute

## The Poisson distribution

If $N \sim \Poisson(\mu)$ then $N \ge 0$ and
$$\begin{aligned}
    \P\{N = k\} = \frac{\mu^k}{k!} e^{-\mu}
\end{aligned}$$

- $N$ is a nonnegative integer (i.e., a *count*)

- $\E[N] = \var[N] = \mu$

- If a machine makes widgets very fast,
  producing on average one broken widget per minute (and many good ones),
  each breaking independent of the others,
  then the number of broken widgets in $\mu$ minutes is $\Poisson(\mu)$.

- If busses arrive randomly every $\Exp(1)$ minutes,
  then the number of busses to arrive in $\mu$ minutes
  is $\Poisson(\mu)$.



# Count data

## A hypothetical situation:

1. We have **counts** of transcript numbers,

2. from some individuals of different **ages**
   and past **exposures** to solar irradiation,

3. of two **genotypes**.

. . .

*Model:*

* Counts are **Poisson**,

* with mean that depends on age and exposure,

* but effect of exposure depends on genotype.



-------------------------

::: {.columns}
::::::::: {.column width="50%"}


1. Counts are **Poisson**,

2. with mean that depends on age and exposure,

3. but effect of exposure depends on genotype.

:::
:::::::::: {.column width="50%"}


$$\begin{aligned}
    Z_i &\sim \Poisson(\mu_i) \\
\end{aligned}$$

:::
:::::::::::



-------------------------

::: {.columns}
::::::::: {.column width="50%"}


1. Counts are **Poisson**,

2. with mean that depends on age and exposure,

3. but effect of exposure depends on genotype.

:::
:::::::::: {.column width="50%"}


$$\begin{aligned}
    Z_i &\sim \Poisson(\mu_i) \\
    \mu_i &= a + b \times \text{age}_i + c \times \text{exposure}_i 
\end{aligned}$$

:::
:::::::::::



-------------------------

::: {.columns}
::::::::: {.column width="50%"}


1. Counts are **Poisson**,

2. with mean that depends on age and exposure,

3. but effect of exposure depends on genotype.

:::
:::::::::: {.column width="50%"}


$$\begin{aligned}
    Z_i &\sim \Poisson(\mu_i) \\
    \mu_i &= \exp\left( a_{g_i} + b \times \text{age}_i + c_{g_i} \times \text{exposure}_i \right)
\end{aligned}$$

:::
:::::::::::

# Poisson regression, in practice

## The data


```{r sim_counts, include=FALSE, cache=TRUE}
true_params <- list(a=c(0, 0.2),
                    b=1/20,
                    c=c(1/30, -1/15),
                    sigma=1.0)
nsamples <- 500
data <- data.frame(genotype=sample(c(1,2), nsamples, 
                                   replace=TRUE),
                   age = rgamma(nsamples, 3, 0.1),
                   exposure = rexp(nsamples, 0.2))
data$y <- with(data, true_params$a[genotype] +
                      true_params$b * age +
                      true_params$c[genotype] * exposure)
data$mu <- exp(rnorm(nrow(data), mean=data$y, 
                     sd=true_params$sigma))
data$counts <- rpois(nsamples, data$mu)
```

```{r save_data, include=FALSE}
write.table(data, file="poisson_counts_data.tsv", row.names=FALSE)
```

```{r plot_counts, echo=FALSE, fig.width=2*fig.dim}
layout(t(1:2))
plot(counts ~ age, col=genotype, data=data, pch=20)
plot(counts ~ exposure, col=genotype, data=data, pch=20)
legend("topright", pch=20, col=1:2,
       legend=paste("genotype", 1:2))
```


## Write a Stan block

::: {.columns}
::::::::: {.column width="50%"}


1. Counts are **Poisson**,

2. with mean that depends on age and exposure,

3. but effect of exposure depends on genotype.

$$\begin{aligned}
    Z_i &\sim \Poisson(y_i) \\
    y_i &= \exp(a_{g_i} + b \times \text{age}_i \\
        &\qquad {} + c_{g_i} \times \text{exposure}_i )
\end{aligned}$$

:::
:::::::::: {.column width="50%"}

<!-- 
    NOTE: do this online, asking class what needs to be included
    and walking through how to include it 
-->

```
poisson_model <- stan_model(model_code="
data {
    int N; // number of data points
    vector[N] age;
    vector[N] exposure;
    int counts[N];
    int genotype[N];
    int ngenotypes;
}
parameters {
    vector[ngenotypes] a; // intercepts
    real b; // slope for age
    vector[ngenotypes] c; // slopes for exposure
}
model {
    vector[N] mu; // mean of the poissons
    mu = exp(a[genotype] + b * age + c[genotype] .* exposure);
    counts ~ poisson(mu);
    a ~ normal(0, 100);
    b ~ normal(0, 10);
    c ~ normal(0, 20);
}")
```

:::
:::::::::::


## The result

*Note:* scaling the data helps Stan find the right scale to move on.

```{r run_simple_pois, cache=TRUE, dependson=c("sim_counts", "simple_poisson")}
scaled_data <- with(data, list(N=length(counts),
                               counts=counts,
                               age=(age - mean(age))/sd(age),
                               expo=(exposure - mean(exposure))/sd(exposure),
                               geno=genotype))
fit1 <- stan(file="simple_poisson.stan",
             data=scaled_data,
             control=list(max_treedepth=12),
             iter=1000, chains=3)
```

-----------

```{r printit}
post1 <- extract(fit1)
print(fit1)
```

## Aside: a look at "warmup"

```{r the_warmup}
stan_trace(fit1, pars=c("a","b","c","lp__"), inc_warmup=TRUE)
```

## The usual plot (without warmup)

```{r not_warmup}
stan_trace(fit1, pars=c("a","b","c","lp__"), inc_warmup=FALSE)
```

## How'd we do?

Here are posterior distributions of the parameters,
with **the true values in red**.
```{r true_fit_1, echo=FALSE}
# a[g] + b * (age - ma)/sa + c[g] * (exp - me)/se
# = a[g] - b * ma / sa - c[g] * me / se + (b/sa) * age  + (c[g]/se) * exp
post1$true_a <- with(data, post1$a - cbind(post1$b,post1$b) * mean(age)/sd(age) - post1$c * mean(exposure)/sd(exposure))
layout(matrix(1:6, nrow=2, byrow=TRUE))
with(data, {
    hist(post1$true_a[,1], breaks=50, main='a[1]', 
         xlim=range(post1$true_a[,1], true_params$a[1]))
    abline(v=true_params$a[1], col='red', lwd=2)
    hist(post1$true_a[,2], breaks=50, main='a[2]',
         xlim=range(post1$true_a[,2], true_params$a[2]))
    abline(v=true_params$a[2], col='red', lwd=2)
    hist(post1$b/sd(age), breaks=50, main='b',
         xlim=range(true_params$b, post1$b/sd(age)))
    abline(v=true_params$b, col='red', lwd=2)
    hist(post1$c[,1]/sd(exposure), breaks=50, main='c[1]',
         xlim=range(post1$c[,1]/sd(exposure), true_params$c[1]))
    abline(v=true_params$c[1], col='red', lwd=2)
    hist(post1$c[,2]/sd(exposure), breaks=50, main='c[2]',
         xlim=range(post1$c[,2]/sd(exposure), true_params$c[2]))
    abline(v=true_params$c[2], col='red', lwd=2)
 })
```

. . .

*What happened?*

# Goodness of fit

## Posterior predictive simulations

Let's simulate up data *under this model* to check for goodness of fit.

. . .

We expect to **not** see a good fit. (*Why?*)


## 100 datasets from the posterior distribution


::: {.columns}
:::::::: {.column width="60%"}

```{r post_sims1}
params1 <- list(a=colMeans(post1$a),
                b=mean(post1$b),
                c=colMeans(post1$c))
mu1 <- with(list2env(scaled_data), 
                exp(params1$a[geno] 
                    + params1$b * age
                    + params1$c[geno] * expo))
# 100 datasets:
sim1 <- replicate(100, rpois(length(mu1), mu1))
```

:::
:::::::::: {.column width="40%"}

```
model {
    vector[N] mu;
    mu = exp(a[geno] 
             + b * age 
             + c[geno] 
               .* expo);
    counts ~ poisson(mu);
```

:::
:::::::::::

## True data are *overdispersed* relative to our simulations

```{r plot_post_sims1}
plot(data$counts[order(mu1)], ylab="counts", ylim=range(sim1), type='n')
segments(x0=seq_len(nrow(data)),
         y0=rowMins(sim1)[order(mu1)],
         y1=rowMaxs(sim1)[order(mu1)])
points(data$counts[order(mu1)], pch=20, col='red')
legend("topleft", pch=c(20,NA), lty=c(NA,1), legend=c("observed", "simulated range"), col=c('red', 'black'))
```


# Stan interlude

## The important program blocks

```
data {
    // what we know: the input
    // declarations only
}
parameters {
    // what we want to know about:
    // defines the space Stan random walks in
    // declarations only
}
model {
    // stuff to calculate the priors and the likelihoods
    // happens every step
}
```


## The program blocks

```
functions {
    // user-defined functions
}
data {
    // what we know: the input
    // declarations only
}
transformed data {
    // calculations to do once, at the start
}
parameters {
    // what we want to know about:
    // defines the space Stan random walks in
    // declarations only
}
transformed parameters {
    // other things that we want the posterior distribution of
    // happens every step
}
model {
    // stuff to calculate the priors and the likelihoods
    // happens every step
}
generated quantities {
    // more things we want the posterior distribution of
    // but that don't affect the random walk
}
```

## On priors

Under the hood,
```
    z ~ poisson(mu);
```
is equivalent to
```
    target += poisson_lpdf(z | mu);
```
(`lpdf` = log posterior density function).

. . .

So, if you *don't* put a prior on something,
it implicitly has a *uniform* prior (i.e., a flat prior).

## Error messages

These are important.
Pay attention to them, and fix the problems.

. . .

Run code in the *console*.
Rstudio often hides useful information.


## Parameterization matters

More on this later.


# Including overdispersion

## How can we include overdispersion?


::: {.columns}
::::::::: {.column width="50%"}


1. Counts are **Poisson**,

2. with mean that depends on age and exposure,

3. but effect of exposure depends on genotype.

4. Actually, counts are *overdispersed*, so make the means *random*,
   and lognormally distributed.

:::
:::::::::: {.column width="50%"}


$$\begin{aligned}
    Z_i &\sim \Poisson(\mu_i) \\
    \mu_i &= \exp\left( a_{g_i} + b \times \text{age}_i + c_{g_i} \times \text{exposure}_i \right)
\end{aligned}$$

:::
:::::::::::


--------------------


::: {.columns}
::::::::: {.column width="50%"}


1. Counts are **Poisson**,

2. with mean that depends on age and exposure,

3. but effect of exposure depends on genotype.

4. Actually, counts are *overdispersed*, so the means are *random*,
   and lognormally distributed.

:::
:::::::::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Poisson(\mu_i) \\
    \mu_i &= \exp(W_i) \\
    W_i &\sim \Normal(y_i, \sigma) \\
    y_i &= a_{g_i} + b \times \text{age}_i + c_{g_i} \times \text{exposure}_i
\end{aligned}$$

:::
:::::::::::


--------------------


::: {.columns}
::::::::: {.column width="50%"}


1. Counts are **Poisson**,

2. with mean that depends on age and exposure,

3. but effect of exposure depends on genotype.

4. Actually, counts are *overdispersed*, so the means are *random*,
   and lognormally distributed.

:::
:::::::::: {.column width="50%"}


$$\begin{aligned}
    Z_i &\sim \Poisson(\mu_i) \\
    \mu_i &\sim \log\Normal(y_i, \sigma) \\
    y_i &= a_{g_i} + b \times \text{age}_i + c_{g_i} \times \text{exposure}_i 
\end{aligned}$$

:::
:::::::::::

## Your turn: add overdispersion

::: {.columns}
::::::::: {.column width="40%"}


$$\begin{aligned}
    Z_i &\sim \Poisson(\mu_i) \\
    \mu_i &\sim \log\Normal(y_i, \sigma) \\
    y_i &= a_{g_i} + b \times \text{age}_i \\
    &\qquad {} + c_{g_i} \times \text{exposure}_i 
\end{aligned}$$

You can download the data [here](poisson_counts_data.tsv).

:::
:::::::::: {.column width="60%"}

```
overdispersed_model <- stan_model(model_code="
data {
    int N; // number of data points
    vector[N] age;
    vector[N] exposure;
    int counts[N];
    int genotype[N];
    int ngenotypes;
}
parameters {
    vector[ngenotypes] a; // intercepts
    real b; // slope for age
    vector[ngenotypes] c; // slopes for exposure
    real<lower=0> sigma; // SD on lognormal
    vector[N] mu; // mean of the poissons
}
model {
    vector[N] y; // mean of the lognormals
    y = a[genotype] + b * age + c[genotype] .* exposure;
    mu ~ lognormal(y, sigma);
    counts ~ poisson(mu);
    a ~ normal(0, 100);
    b ~ normal(0, 10);
    c ~ normal(0, 20);
    sigma ~ normal(0, 10);
}")
```

:::
:::::::::::

---------------

```{r fit_fullmodel, cache=TRUE, dependson=c("sim_counts", "full_poisson")}
fit2 <- stan(file="lognormal_poisson.stan",
             data=scaled_data,
             iter=1000, chains=3)
```

----------------

```{r print_fullmodel}
post2 <- rstan::extract(fit2)
print(fit2)
```


----------------

```{r trace_fullmodel}
stan_trace(fit2, pars=c("a", "b", "c", "sigma", "lp__"))
```


## How'd we do *now*?

Here are posterior distributions from the full model,
with the true values in red.
```{r true_fit, echo=FALSE}
# a[g] + b * (age - ma)/sa + c[g] * (exp - me)/se
# = a[g] - b * ma / sa - c[g] * me / se + (b/sa) * age  + (c[g]/se) * exp
layout(matrix(1:6, nrow=2, byrow=TRUE))
with(data, {
    hist(post2$a[,1] - post2$b * mean(age)/sd(age) - post2$c[,1] * mean(exposure)/sd(exposure), 
         breaks=50, main='a[1]')
    abline(v=true_params$a[1], col='red', lwd=2)
    hist(post2$a[,2] - post2$b * mean(age)/sd(age) - post2$c[,2] * mean(exposure)/sd(exposure), 
         breaks=50, main='a[2]')
    abline(v=true_params$a[2], col='red', lwd=2)
    hist(post2$b/sd(age), breaks=50, main='b')
    abline(v=true_params$b, col='red', lwd=2)
    hist(post2$c[,1]/sd(exposure), breaks=50, main='c[1]')
    abline(v=true_params$c[1], col='red', lwd=2)
    hist(post2$c[,2]/sd(exposure), breaks=50, main='c[2]')
    abline(v=true_params$c[2], col='red', lwd=2)
    hist(post2$sigma, breaks=50, main='sigma')
    abline(v=true_params$sigma, col='red', lwd=2)
 })
```


## Posterior predictive simulations, again


::: {.columns}
:::::::: {.column width="60%"}

```{r post_sims2}
# 100 datasets:
params2 <- list(a=colMeans(post2$a),
                b=mean(post2$b),
                c=colMeans(post2$c),
                sigma=mean(post2$sigma))
y2 <- with(list2env(scaled_data), 
                params2$a[geno] 
                    + params2$b * age
                    + params2$c[geno] * expo)
sim2 <- replicate(100, {
           mu = exp(rnorm(length(y2),
                          mean=y2, 
                          sd=params2$sigma))
           rpois(length(mu), mu)
         })
```

:::
:::::::::: {.column width="40%"}

```
model {
    vector[N] y;
    y = a[geno] + b * age + c[geno] .* expo;
    mu ~ lognormal(y, sigma);
    counts ~ poisson(mu);
}
```

:::
:::::::::::

## Now we cover the true data

```{r plot_post_sims2}
plot(data$counts[order(mu1)], ylab="counts", ylim=range(sim2), type='n')
segments(x0=seq_len(nrow(data)),
         y0=rowMins(sim2)[order(mu1)],
         y1=rowMaxs(sim2)[order(mu1)])
points(data$counts[order(mu1)], pch=20, col='red')
legend("topleft", pch=c(20,NA), lty=c(NA,1), legend=c("observed", "simulated range"), col=c('red', 'black'))
```


## Now we cover the true data

```{r plot_post_sims3}
plot(data$counts[order(mu1)], ylab="counts", ylim=c(0,500), type='n')
segments(x0=seq_len(nrow(data)),
         y0=rowMins(sim2)[order(mu1)],
         y1=rowMaxs(sim2)[order(mu1)])
points(data$counts[order(mu1)], pch=20, col='red')
legend("topleft", pch=c(20,NA), lty=c(NA,1), legend=c("observed", "simulated range"), col=c('red', 'black'))
```


# Model comparison

## How to *compare* the two models?

Two models:

1. `counts ~ poisson(exp(a + b * age + c * exposure))`

2. `counts ~ poisson(logNormal(a + b * age + c * exposure))`

. . .

We just saw some plots that showed that the true data
lay outside the range of the simulated data from (1)
but not (2).

. . .

That was *not* a formal test.


## We need a statistic!

*Brainstorm:* how can we quantify what we just saw?

*Goal:* come up with a single number that quantifies
how much the observed data "looks like" the posterior predictive samples.

*Then,* the model with a better score *fits* better.

```{r plot_model_fit, echo=FALSE, fig.width=3*fig.dim}
layout(t(1:2))
plot(data$counts[order(mu1)], ylab="counts", ylim=c(1,1000), # ylim=range(sim1,data$counts), 
     type='n', main='Poisson regression')
segments(x0=seq_len(nrow(data)),
         y0=rowMins(sim1)[order(mu1)],
         y1=rowMaxs(sim1)[order(mu1)])
points(data$counts[order(mu1)], pch=20, col='red', cex=0.5)
legend("topleft", pch=c(20,NA), lty=c(NA,1), legend=c("observed", "simulated range"), col=c('red', 'black'))

plot(data$counts[order(mu1)], ylab="counts", ylim=c(0,1000), # ylim=range(sim2,data$counts), 
     type='n', main='logNormal-Poisson')
segments(x0=seq_len(nrow(data)),
         y0=rowMins(sim2)[order(mu1)],
         y1=rowMaxs(sim2)[order(mu1)])
points(data$counts[order(mu1)], pch=20, col='red', cex=0.5)
legend("topleft", pch=c(20,NA), lty=c(NA,1), legend=c("observed", "simulated range"), col=c('red', 'black'))
```

-----------------

Same plot, zoomed in:

```{r plot_model_fit2, echo=FALSE, fig.width=3*fig.dim, fig.height=2*fig.dim}
layout(t(1:2))
plot(data$counts[order(mu1)], ylab="counts", ylim=c(1,400), # ylim=range(sim1,data$counts), 
     type='n', main='Poisson regression')
segments(x0=seq_len(nrow(data)),
         y0=rowMins(sim1)[order(mu1)],
         y1=rowMaxs(sim1)[order(mu1)])
points(data$counts[order(mu1)], pch=20, col='red', cex=0.5)
legend("topleft", pch=c(20,NA), lty=c(NA,1), legend=c("observed", "simulated range"), col=c('red', 'black'))

plot(data$counts[order(mu1)], ylab="counts", ylim=c(0,400), # ylim=range(sim2,data$counts), 
     type='n', main='logNormal-Poisson')
segments(x0=seq_len(nrow(data)),
         y0=rowMins(sim2)[order(mu1)],
         y1=rowMaxs(sim2)[order(mu1)])
points(data$counts[order(mu1)], pch=20, col='red', cex=0.5)
legend("topleft", pch=c(20,NA), lty=c(NA,1), legend=c("observed", "simulated range"), col=c('red', 'black'))
```



## A toy problem

Is it plausible that
$$ X \sim \Normal(A, A)?$$
What about $Y$?

```
data.frame(
  A = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10), 
  X = c(-0.45, -0.16, 0.15, 0.43, 0.59, 0.87, 1.1, 1.51, 2.3, 2.42,
         -7.98, -2.49, 5.15, 5.86, 7.36, 8.52, 10.53, 13, 15.51, 18.8), 
  Y = c(-2.18, -1.47, 0.18, 0.64, 0.81, 1.18, 2.52, 2.87, 3.08, 3.1, 
         -49.03, -38.01, -28.81, 0.16, 9.89, 25.92, 39.1, 49.3, 50.87, 92.11))
```

## Goodness-of-fit: design

<!-- IMPLEMENT, IN-CLASS -->

We'll use the fact that if the simulated data
"look just like" the real data,
then the rank of the real data in the simulated data
should be *uniform*.

Since we have 100 simulated datasets,
that means that the probability that the real data is *smaller* than all the simulated data
is $1/101$, and the probability that the real data is bigger than *only one* of the simulated data points
is $1/101$, etcetera.

This is a *distribution-free* measure of goodness of fit.

## Warm-up

To check that we understand what's going on here,
we're doing to simulate data where the "sample" *fits* the model,
so that the "sample" and the "simulations" are from the same distribution.

```{r warmup}
# 1. Simulate 99 draws from a distribution, then one more "sample".

x <- runif(99 + 1) # the "sample" will be the first one

# 2. Find the rank of the "sample" in all 100.

r <- sum(x <= x[1])

# 3. Do that a lot of times: we do it in a matrix to be efficient.

nreps <- 1000
X <- matrix(runif(100 * nreps), ncol=100)
R <- rowSums(X <= X[,1])
```

##

```{r a_hist}
# 4. Histogram that.

hist(R) # this looks pretty uniform, as it should!
```

## Chi-squraed for goodness of fit.

If $N_i \sim \Poisson(P_i)$, then the mean *and* variance of $N_i$ is $P_i$.
This motivates the chi-squared statistic:
$$
\chi^2 = \sum_i \frac{(N_i - P_i)^2}{P_i} .
$$

##

```{r warmup_contd}
# 5. Compute the chi-squared statistic.

gof <- function (x, sim) {
    R <- rowSums(x <= sim) # this is the rank minus 1
    predicted <- length(x) / 10
    chisq <- sum((table(R %/% 10) - predicted)^2 / predicted)
    return(chisq)
}

# 6. Find the null distribution of this statistic

null_distrn <- replicate(1000, {
    nreps <- 1000
    X <- matrix(runif(100 * nreps), ncol=100)
    gof(X[,1], X[,2:100]) })
```

## Goodness-of-fit: implementation

<!-- IMPLEMENT, IN-CLASS -->
```{r get_gof}
gof1 <- gof(data$counts, sim1[, 1:99])
gof2 <- gof(data$counts, sim2[, 1:99])
```

The goodness-of-fitscores obtained by our models were

1. Original model: `r round(gof1)`
2. Second model, with more randomness: `r round(gof2)`


##

Here is the "null distribution" of the goodness-of-fit scores.
The first model is clearly outside this range,
and the second one is on the edge.

```{r show_gof_hist}
hist(null_distrn, main='GOF scores')
```
