---
title: "Sharing power, simulation, and modeling noise"
author: "Peter Ralph"
date: "27 November 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 3
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```


# Baseball

## Baseball

We have [a dataset](BattingAverage.csv) of batting averages of baseball players,
having

1. name
2. position
3. number of at bats
4. number of hits


```{r basedata}
batting <- read.csv("BattingAverage.csv", header=TRUE)
head(batting)
```

------------------------

The *overall* batting average of the `r nrow(batting)` players is `r sum(batting$Hits)/sum(batting$AtBats)`.

Here is the average by position.
```{r by_pos}
batting %>% group_by(PriPos) %>% 
    summarise(num=n(), BatAvg=sum(Hits)/sum(AtBats)) %>% 
    select(PriPos, num, BatAvg)
```

## Questions?

1. What's the overall batting average?

2. Do some positions tend to be better batters?

3. How much variation is there?


## Everyone is the same

```{r start, cache=TRUE}
first_model <- "
data {
    int N;
    int hits[N];
    int at_bats[N];
}
parameters {
    real<lower=0, upper=1> theta;
}
model {
    hits ~ binomial(at_bats, theta);
    theta ~ beta(1, 1);
} "
first_fit <- stan(model_code=first_model, chains=3, iter=1000,
                  data=list(N=nrow(batting),
                            hits=batting$Hits,
                            at_bats=batting$AtBats))
```

-----------------

```{r start_res}
stan_hist(first_fit, bins=20)
```

## Every pitcher is the same

```{r pos_model, cache=TRUE}
pos_model <- "
data {
    int N;
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
}
parameters {
    real<lower=0, upper=1> theta[npos];
}
model {
    real theta_vec[N];
    for (k in 1:N) {
        theta_vec[k] = theta[position[k]];
    }
    hits ~ binomial(at_bats, theta_vec);
    theta ~ beta(1, 1);
} "
pos_fit <- stan(model_code=pos_model, chains=3, iter=1000,
                  data=list(N=nrow(batting),
                            hits=batting$Hits,
                            at_bats=batting$AtBats,
                            npos=nlevels(batting$PriPos),
                            position=as.numeric(batting$PriPos)))
```

-----------------

```{r pos_res, fig.width=3*fig.dim, fig.height=2*fig.dim}
theta_samples <- extract(pos_fit)$theta
layout(matrix(1:9, nrow=3))
for (k in 1:ncol(theta_samples)) {
    hist(theta_samples[,k], main=levels(batting$PriPos)[k], xlim=c(0.1, 0.3),
         col=adjustcolor('red',0.6), xlab='batting avg', freq=FALSE)
}
```



## Your turn : Every individual different.

:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\alpha_{p_i}, \beta_{p_i}) \\
    \alpha_p &= \mu_p \kappa_p \\
    \beta_p &= (1-\mu_p) \kappa_p \\
    \mu_p &\sim \Beta(1, 1) \\
    \kappa_p &\sim \Gam(0.1, 0.1) .
\end{aligned}$$

::::::::::::::
::: {.column width="50%"}

Variable types in Stan:
```
int x;       // an integer
int y[10];   // ten integers
real z;      // a number
real z[2,5]; // a 2x5 array of numbers

vector u[10];      // length 10 vector
matrix v[10,10];   // 10x10 matrix
vector[10] w[10];  // ten length 10 vectors
```

* don't forget the `;`
* make sure R types match
* read the error messages

:::
::::::::::::::


# Stochastic minute

## Exponential, and Gamma

If $T \sim \Exp(\text{rate}=\lambda)$, then

$$\begin{aligned}
   \P\{ T \in dt \} = \lambda e^{-\lambda t} dt .
\end{aligned}$$

1. $T$ can be any nonnegative real number.

2. $T$ is *memoryless*: 
   $$\begin{aligned}
        \P\{ T > x + y \given T > x \} = \P\{ T > y \} .
   \end{aligned}$$

3. A machine produces $n$ widgets per second;
   each widget has probability $\lambda/n$ of being broken.
   The time until the first broken widget appears (in seconds)
   is approximately $\sim \Exp(\lambda)$.

---------------------

If $S \sim \Gam(\text{shape}=\alpha, \text{rate}=\lambda)$, then

$$\begin{aligned}
   \P\{ S \in dt \} = \frac{\alpha^\lambda}{\Gamma(\alpha)} t^{\alpha - 1} e^{-\lambda t} dt .
\end{aligned}$$

1. If $T_1, \ldots, T_k$ are independent $\Exp(\lambda)$, then
   $S = T_1 + \cdots + T_k$ is $\Gam(k, \lambda)$.


2. A machine produces $n$ widgets per second;
   each widget has probability $\lambda/n$ of being broken.
   The time until the $k^\text{th}$ broken widget appears (in seconds)
   is approximately $\sim \Gam(k, \lambda)$.


# Back to baseball

## The model

<!--

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\mu_{p_i} \kappa_{p_i}, (1-\mu_{p_i})\kappa_{p_i}) \\
    \mu &\sim \Beta(1, 1) \\
    \kappa_p &\sim \Gam(0.1, 0.1) .
\end{aligned}$$

-->

:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\mu_{p_i} \kappa_{p_i}, (1-\mu_{p_i})\kappa_{p_i}) \\
    \mu &\sim \Beta(1, 1) \\
    \kappa_p &\sim \Gam(0.1, 0.1) .
\end{aligned}$$

::::::::::::::
::: {.column width="50%"}


```{r fake_stan_model, eval=FALSE}
pos_model <- "
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
}
parameters {
    real<lower=0, upper=1> theta[N];
    real<lower=0, upper=1> mu[npos];
    real<lower=0> kappa[npos];
}
model {
    real alpha;
    real beta;
    hits ~ binomial(at_bats, theta);
    for (i in 1:N) {
        alpha = mu[position[i]] * kappa[position[i]];
        beta = (1 - mu[position[i]]) * kappa[position[i]];
        theta[i] ~ beta(alpha, beta);
    }
    mu ~ beta(1,1);
    kappa ~ gamma(0.1,0.1);
}"
```


```{r stan_model, cache=TRUE, include=FALSE}
pos_model <- "
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
}
parameters {
    real<lower=0, upper=1> theta[N];
    real<lower=0, upper=1> mu[npos];
    real<lower=0> kappa[npos];
}
model {
    real alpha;
    real beta;
    hits ~ binomial(at_bats, theta);
    for (i in 1:N) {
        alpha = mu[position[i]] * kappa[position[i]];
        beta = (1 - mu[position[i]]) * kappa[position[i]];
        theta[i] ~ beta(alpha, beta);
    }
    mu ~ beta(1,1);
    kappa ~ gamma(0.1,0.1);
} "
```

:::
:::::::::::


##


```{r fit_pos_model, cache=TRUE, dependson="stan_model"}
batting <- read.csv("BattingAverage.csv", header=TRUE)
system.time(pos_fit <- stan(model_code=pos_model, chains=3, iter=100,
                            data=list(N=nrow(batting),
                                      hits=batting$Hits,
                                      at_bats=batting$AtBats,
                                      npos=nlevels(batting$PriPos),
                                      position=as.numeric(batting$PriPos))))
```

## Diagnostics

```{r print_fit}
print(pos_fit, pars=c("mu", "kappa"))
```

----------------

Is it mixing?

```{r plot_trace}
stan_trace(pos_fit, pars="mu")
```


## Run longer!

```{r fit_pos_model_again, cache=TRUE, dependson="stan_model"}
system.time(pos_fit <- stan(model_code=pos_model, chains=3, iter=1000,
                            control=list(max_treedepth=15),
                            data=list(N=nrow(batting),
                                      hits=batting$Hits,
                                      at_bats=batting$AtBats,
                                      npos=nlevels(batting$PriPos),
                                      position=as.numeric(batting$PriPos))))
```

-------------

```{r print_fit_again}
print(pos_fit, pars=c("mu", "kappa"))
```


-------------

Is it mixing?

```{r plot_trace_again}
stan_trace(pos_fit, pars="mu")
```

-------------

```{r plot_kappa_again}
stan_trace(pos_fit, pars="kappa")
```

## Let's look at the results!

```{r first_hist}
stan_hist(pos_fit, pars="mu", bins=30) + xlim(0, 0.4)
```

-----------

With labels: position means $\mu_p$:

```{r plot_mu, echo=FALSE, fig.width=2.5*fig.dim, fig.height=2*fig.dim}
param_samples <- extract(pos_fit)
layout(matrix(1:9, ncol=3))
par(mar=c(4,3,1,1)+.1)
for (k in 1:9) {
    hist(param_samples$mu[,k], main=levels(batting$PriPos)[k],
         xlim=c(0, 0.4), xlab='batting average', ylab=expression(mu))
}
```

-----------

Position "concentrations", $\kappa_p$:

```{r plot_kappa, echo=FALSE, fig.width=2.5*fig.dim, fig.height=2*fig.dim}
layout(matrix(1:9, ncol=3))
par(mar=c(4,3,1,1)+.1)
for (k in 1:9) {
    hist(param_samples$kappa[,k], main=levels(batting$PriPos)[k],
         xlab='batting average', ylab=expression(kappa))
}
```



# Simulation

## Wouldn't it be nice if we knew the truth?

. . .

**Discuss:**

Write down (on whiteboards)
a procedure to simulate data
that looks like the baseball data.


------------

```{r simulate_fake_data}
batting$post_mean <- colMeans(param_samples$theta)
batting$fake_hits <- rbinom(nrow(batting), 
                            size=batting$AtBats, 
                            prob=batting$post_mean)
```


:::::::: {.columns}
::: {.column width=50%}

*Option 1:* By simulating draws from the posterior mean on $\theta$,
we can check that our model is accurately describing the data.
Here is the real data against data simulated under the posterior mean $\theta$ values.
They look similar, which is good.

:::::::::::
::: {.column width=50%}

```{r plot_simfakedata, echo=FALSE, fig.width=fig.dim}
with(batting, plot(Hits, fake_hits, xlab='actual hits', ylab='simulated hits'))
```

:::
:::::::::::

------------

**But** if we want to know if we're accurately estimating $\mu$ and $\kappa$, 
then we have to start with them, and *simulate* $\theta$.
```{r simulate_with_thetas}
post_mean_mu <- colMeans(param_samples$mu)
post_mean_kappa <- colMeans(param_samples$kappa)
names(post_mean_mu) <- names(post_mean_kappa) <- levels(batting$PriPos)
batting$sim_theta <- rbeta(nrow(batting),
                           shape1=post_mean_mu[batting$PriPos] *
                                   post_mean_kappa[batting$PriPos],
                           shape2=(1-post_mean_mu[batting$PriPos]) * 
                                   post_mean_kappa[batting$PriPos])
batting$sim_theta_hits <- rbinom(nrow(batting), 
                                 size=batting$AtBats, 
                                 prob=batting$sim_theta)
```

------------

Fit the model to the *simulated* data:

```{r fit_sim_data, cache=TRUE}
sim_fit <- stan(model_code=pos_model, chains=3, iter=1000, control=list(max_treedepth=13),
                data=list(N=nrow(batting),
                          hits=batting$sim_theta_hits,
                          at_bats=batting$AtBats,
                          npos=nlevels(batting$PriPos),
                          position=as.numeric(batting$PriPos)))
```

----------

Can we estimate $\mu$ and $\kappa$?
```{r check_sim_mu}
sim_samples <- extract(sim_fit)
layout(t(1:2))
boxplot(sim_samples$mu, main="posterior distribution of mu")
points(1:9, post_mean_mu, col='red', pch=20, cex=2)
boxplot(sim_samples$kappa, main="posterior distribution of kappa")
points(1:9, post_mean_kappa, col='red', pch=20, cex=2)
```


## General questions with simulated data

1. Does my statistical inference method work?

. . .


2. Do the credible intervals contain the true value? 

    (i.e., Is the method "well-calibrated"?)

    *They usually should.*

. . .

3. How wide are credible intervals, typically?

    This is (one kind of) **statistical power.** 


# Posterior predictive sampling

## How to choose parameter values for simulation?

It depends, but one good choice is
*from the posterior distribution* -
then you can see how the model behaves
with data close to the real data.

This is known as **posterior predictive sampling**.

---------------------------

![Kruschke figure 13.2: posterior predictive sampilng](post_pred_kruschke.png)


# Sharing power // Shrinkage

## Example

Suppose that I have a large pot of coins
that are all similar to each other.
I flip each one ten times, and record the number of Heads.
What is *each coin's* probability of Heads?

- Treated *separately*,
  we would be very uncertain about each coin.

- Together, they should tell us very accurately 
  what are *likely* values of $\theta$.

- This information can improve the estimate of each separate $\theta$.

- The more similar the coins are, the more information we gain.



## Shrinkage and baseball

Some players were at bat very few times.
How does the information about their position affect our inference about their batting averages?

```{r baseball_shrinkage}
batting$post_med <- colMedians(param_samples$theta)
batting$post_Q1 <- colQuantiles(param_samples$theta, probs=0.25)
batting$post_Q3 <- colQuantiles(param_samples$theta, probs=0.75)
pos_means <- colMeans(param_samples$mu)
names(pos_means) <- levels(batting$PriPos)
pos_means
```

--------------

Pitchers had posterior mean $\mu$ of `r pos_means["Pitcher"]`

````{r pitchers}
with(subset(batting[order(batting$post_med),], PriPos=="Pitcher"), {
     plot(Hits / AtBats, main="Pitchers", xaxt='n', ylim=c(0, 0.4),
         xlab='player', ylab="posterior median theta");
     segments(x0=seq_along(Hits), y0=post_Q1, y1=post_Q3,
         col=adjustcolor('red',0.5));
     points(post_med, pch=20) })
abline(h=pos_means["Pitcher"], lwd=2, col=adjustcolor("blue", 0.5))
```

---------------

Catchers had posterior mean $\mu$ of `r pos_means["Catcher"]`

````{r catchers}
with(subset(batting[order(batting$post_med),], PriPos=="Catcher"), {
     plot(Hits / AtBats, main="Catchers", xaxt='n', ylim=c(0, 0.4),
         xlab='player', ylab="posterior median theta");
     segments(x0=seq_along(Hits), y0=post_Q1, y1=post_Q3,
         col=adjustcolor('red',0.5));
     points(post_med, pch=20) })
abline(h=pos_means["Catcher"], lwd=2, col=adjustcolor("blue", 0.5))
```

## Is shrinkage here a good idea?

With *simulated data*, compare median absolute error for

- posterior mean $\theta_i$

- empirical batting average

------------

How'd we do?  Let's look at the *true* values of $\theta$
(we know because we generated them)
against the posterior means.
Red lines are 95% credible intervals.
```{r check_sim_result}
post_sim_theta <- colMeans(sim_samples$theta)
plot(batting$sim_theta, post_sim_theta, type='n', xlab="true theta", ylab="posterior mean theta", ylim=c(0,0.4))
segments(x0=batting$sim_theta,  col='red',
         y0=colQuantiles(sim_samples$theta, probs=0.05),
         y1=colQuantiles(sim_samples$theta, probs=0.95))
points(batting$sim_theta, post_sim_theta, pch=20)
abline(0, 1, lwd=3, col=adjustcolor('blue', 0.5))
```

-------------

Did we do better?

The mean absolute difference between the *truth* and

- posterior mean: `r mean(abs(batting$sim_theta - post_sim_theta))`
- empirical batting average (hits/at bats): `r mean(abs(batting$sim_theta - batting$fake_hits/batting$AtBats))`

Using information about groups **reduced our error by a factor of 4!**


# Logistic regression, a.k.a. metric predictors

## Motivation

There's a good bit [more information available](BattingAveragePlus.csv) about the baseball players.
```{r more_batting}
batplus <- read.csv("BattingAveragePlus.csv")
head(batplus)
```

. . .

Is batting average predicted by weight and height?

-------------

::::::::: {.columns}
::: {.column width=50%}

1. Each player has their own batting avg, $\theta_i$.

2. Batting averages tend to be different for different positions.

3. After accounting for position ($p_i$),
   $\theta$ varies with height ($h_i$) and/or weight ($w_i$).

```{r logistic, echo=FALSE}
curve(1/(1+exp(-x)), from=-5, to=5, main="logistic function")
```

:::::::::
::: {.column width=50%}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &= \Beta(\mu_i \kappa_{p_i}, (1-\mu_i) \kappa_{p_i}) \\
    \mu_i &= \logistic(\gamma_{p_i} + \alpha_h h_i + \alpha_w w_i) \\
    \kappa_p &\sim \Gam(0.1, 0.1) \\
    \gamma_{p} &\sim \Normal(0, \sigma_\gamma) \\
    \alpha_h &\sim \Normal(0, \sigma_h) \\
    \alpha_w &\sim \Normal(0, \sigma_w) \\
    \sigma_\gamma, \sigma_h, \sigma_w &\sim \Normal^+(0, 20)
\end{aligned}$$

:::
:::::::::




## Stan

::::::::: {.columns}
::: {.column width=50%}

```{r stan_logistic, cache=TRUE}
logistic_model <- "
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
    vector[N] height;
    vector[N] weight;
}
parameters {
    real<lower=0, upper=1> theta[N];
    vector[npos] gamma;
    real alpha_w;
    real alpha_h;
    vector<lower=0>[npos] kappa;
}
model {
    vector[N] mu;
    mu = inv_logit(gamma[position] + alpha_h * height + alpha_w * weight);
    theta ~ beta(mu .* kappa[position], (1-mu) .* kappa[position]);
    hits ~ binomial(at_bats, theta);
    kappa ~ gamma(0.1, 0.1);
    gamma ~ normal(0, 5);
    alpha_h ~ normal(0, 5);
    alpha_w ~ normal(0, 5);
} "
```

:::::::::
::: {.column width=50%}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &= \Beta(\mu_i \kappa_{p_i}, (1-\mu_i) \kappa_{p_i}) \\
    \mu_i &= \logistic(\gamma_{p_i} + \alpha_h h_i + \alpha_w w_i) \\
    \kappa_p &\sim \Gam(0.1, 0.1) \\
    \gamma_{p} &\sim \Normal(0, 5) \\
    \alpha_h &\sim \Normal(0, 5) \\
    \alpha_w &\sim \Normal(0, 5) 
\end{aligned}$$

:::
:::::::::

----------

## New features

::::::::: {.columns}
::: {.column width=50%}

```r
logistic_model <- "
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
    vector[N] height;
    vector[N] weight;
}
parameters {
    real<lower=0, upper=1> theta[N];
    vector[npos] gamma;
    real alpha_w;
    real alpha_h;
    vector<lower=0>[npos] kappa;
}
model {
    vector[N] mu;
    mu = inv_logit(gamma[position] + alpha_h * height + alpha_w * weight);
    theta ~ beta(mu .* kappa[position], (1-mu) .* kappa[position]);
    hits ~ binomial(at_bats, theta);
    kappa ~ gamma(0.1, 0.1);
    gamma ~ normal(0, 5);
    alpha_h ~ normal(0, 5);
    alpha_w ~ normal(0, 5);
} "
```

:::::::::
::: {.column width=50%}

1. `vector[N] height;` instead of `real height[N];`: to allow `alpha_h * height`.

2. we will *center* and *scale* height and weight before passing to Stan

3. `x .* y` for component-wise multiplication

:::
:::::::::


## 

```
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
    vector[N] height;
    vector[N] weight;
}
parameters {
    real<lower=0, upper=1> theta[N];
    vector[npos] gamma;
    real alpha_w;
    real alpha_h;
    vector<lower=0>[npos] kappa;
}
model {
    vector[N] mu;
    mu = inv_logit(gamma[position] + alpha_h * height + alpha_w * weight);
    theta ~ beta(mu .* kappa[position], (1-mu) .* kappa[position]);
    hits ~ binomial(at_bats, theta);
    kappa ~ gamma(0.1, 0.1);
    gamma ~ normal(0, 5);
    alpha_h ~ normal(0, 5);
    alpha_w ~ normal(0, 5);
} 
```



-------------------


```{r fit_logistic_model, cache=TRUE, depends="stan_logistic"}
scaled_height <- (batplus$height - mean(batplus$height))/sd(batplus$height)
scaled_weight <- (batplus$weight - mean(batplus$weight))/sd(batplus$weight)
system.time(logistic_fit <- stan(model_code=logistic_model, chains=3, iter=1000,
                               control=list(adapt_delta=0.9, max_treedepth=12),
                               data=list(N=nrow(batplus),
                                      hits=batplus$Hits,
                                      at_bats=batplus$AtBats,
                                      npos=nlevels(batplus$PriPos),
                                      position=as.numeric(batplus$PriPos),
                                      height=scaled_height,
                                      weight=scaled_weight) ))
```

------------------

```{r results}
logistic_summary <- rstan::summary(logistic_fit)$summary
print(logistic_summary)
```

## Pitchers still don't bat well

```{r logistic_gamma}
stan_plot(logistic_fit, pars="gamma")
```

----------------

Recall the *mean* of $\theta_i$ is
$$\begin{aligned}
\mu_i = \frac{1}{1 + \exp(-\gamma_{p_i} - \alpha_h h_i - \alpha_w w_i)} .
\end{aligned}$$

```{r logistic_mu}
gamma_samples <- rstan::extract(logistic_fit)$gamma
colnames(gamma_samples) <- levels(batplus$PriPos)
mu_samples <- 1/(1+exp(-gamma_samples))
boxplot(mu_samples, ylab="posterior distrn, mean batting avg", las=3)
```


## No effect of height or weight

```{r logistic_alpha}
stan_plot(logistic_fit, pars=c("alpha_w", "alpha_h")) + geom_vline(xintercept=0)
```

## Same $\theta$ as before?

```{r log_theta, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim}
new_post_theta <- colMeans(rstan::extract(logistic_fit)$theta)
old_post_theta <- batting$post_mean[match(batplus$Player, batting$Player)]
plot(old_post_theta, new_post_theta, xlab="theta, model 1", ylab="theta, model 2")
abline(0,1, col='red')
```


## Conclusions

1. We don't have good evidence that batting average varies substantially
   with height or weight.

2. Changing the model from a Beta prior on class means
   to a logistic transform of a linear model
   did not substantially affect the results.


# Stochastic minute

---------------

If $X \sim \Cauchy(\text{center}=\mu, \text{scale}=\sigma)$, then $X$ has probability density
$$\begin{aligned}
    f(x \given \mu, \sigma) = \frac{1}{\pi\left( 1 + \left( \frac{x - \mu}{\sigma} \right)^2 \right)} .
\end{aligned}$$

> 1. The Cauchy is a good example of a distribution with "heavy tails":
>    rare, very large values.
> 
> 2. If $Z \sim \Normal(0, 1)$ and $X ~ \Normal(0,1/Z)$ then $X \sim \Cauchy(0,1)$.
>
> 3. If $X_1, X_2, \ldots, X_n$ are independent $\Cauchy(0,1)$ then
>    $\max(X_1, \ldots, X_n)$ is of size $n$.
>
> 4. If $X_1, X_2, \ldots, X_n$ are independent $\Cauchy(0,1)$ then
>    $$\begin{aligned}
>     \frac{1}{n} \left(X_1 + \cdots + X_n\right) \sim \Cauchy(0,1) .
>    \end{aligned}$$

----------------

5. If $X_1, X_2, \ldots, X_n$ are independent $\Cauchy(0,1)$ then
   $$\begin{aligned}
    \frac{1}{n} \left(X_1 + \cdots + X_n\right) \sim \Cauchy(0,1) .
   \end{aligned}$$

*Wait, what?!?*

. . .

A single value has the *same distribution* as the mean of 1,000 of them?

. . .

Let's look:
```{r cauchy_mean}
meanplot <- function (rf, n=1e3, m=100) {
    x <- matrix(rf(n*m), ncol=m)
    layout(t(1:2))
    hist(x[1,][abs(x[1,])<5], breaks=20, freq=FALSE,
         main=sprintf("%d samples", m),
         xlim=c(-5,5))
    hist(colMeans(x)[abs(colMeans(x))<5], breaks=20, freq=FALSE,
         main=sprintf("%d means of %d each", m, n),
         xlim=c(-5,5))
}
```

----------

$X \sim \Normal(0,1)$
```{r normmeans}
meanplot(rnorm)
```

-----------

$X \sim \Cauchy(0,1)$
```{r cauchymeans}
meanplot(rcauchy)
```

## Another way to look at it: extreme values

```{r max_values}
n <- 100
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```

## Another way to look at it: extreme values

```{r max_values2}
n <- 1000
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```

## Another way to look at it: extreme values

```{r max_values3}
n <- 1e6
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```

## Exercise: mixture of error rates.

Suppose you are measuring relative metabolic rates
of mice in the wild.
Because life is complicated,
the *accuracy* of your measurements varies widely.
A model of the measured rate, $R_i$,
for a mouse at temperature $T_i$ is
$$\begin{aligned}
    R &\sim \Normal(120 + 0.7 * (T_i - 37), 1/E) \\
    E &\sim \Normal(0, 1) .
\end{aligned}$$

Simulate 200 measurements from this model,
for temperatures between 36 and 38,
and try to infer the true slope (`0.7`).


# Robust regression


## Standard linear regression

:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    \hat y_i &= b_0 + b_1 x_i \\
    y_i &\sim \Normal(\hat y_i, \sigma^2) .
\end{aligned}$$

Simulate data:

```r
truth <- list(b0=1.0, b1=2.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=1)
y <- ( truth$b0 + truth$b1 * x 
        + rnorm(n, mean=0, sd=truth$sigma) )
```

:::
::: {.column width="50%"}


```{r simdata, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim, echo=FALSE}
truth <- list(b0=1.0, b1=2.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=1)
y <- truth$b0 + truth$b1 * x + rnorm(n, mean=0, sd=truth$sigma)
plot(x,y)
abline(truth$b0, truth$b1, col='red')
```

:::
::::::::::::::



------------------

Standard linear regression
```{r slr}
system.time( slr <- lm(y ~ x) )
summary(slr)
```

-----------------

with Stan
```{r stanlr, cache=TRUE}
slr_block <- "
data {
    int N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model {
    y ~ normal(b0 + b1*x, sigma);
}"
system.time(
    stanlr <- stan(model_code=slr_block,
                   data=list(N=length(x), x=x, y=y), iter=1e3))
```

----------------


```{r summary_stanlr}
print(stanlr)
```


## Cauchy noise?

:::::::::::::: {.columns}
::: {.column width="50%"}


Relative axon growth
for neurons after $x$ hours:

```r
truth <- list(b0=1.0, b1=2.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=1)
y <- ( truth$b0 + truth$b1 * x 
        + rcauchy(n, location=0, 
                  scale=truth$sigma) )
```

:::
::: {.column width="50%"}

```{r simdata_rr, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim, echo=FALSE}
set.seed(12)
truth <- list(b0=1.0, b1=2.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=1)
y <- truth$b0 + truth$b1 * x + rcauchy(n, location=0, scale=truth$sigma)
plot(x,y)
abline(truth$b0, truth$b1, col='red')
```

:::
::::::::::::::


------------------

Standard linear regression
```{r slrr}
system.time( slr2 <- lm(y ~ x) )
summary(slr2)
```

-----------------

with Stan
```{r stanrr, cache=TRUE}
srr_block <- "
data { 
    int N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model {
    y ~ cauchy(b0 + b1*x, sigma);
}"
system.time(
    stanrr <- stan(model_code=srr_block,
                   data=list(N=length(x), x=x, y=y), iter=1e3))
```

----------------


```{r summary_stanrr}
print(stanrr)
```

----------------


```{r plot_simdata_rr, fig.width=3.5*fig.dim, fig.height=1.5*fig.dim, echo=FALSE}
make_lm_poly <- function (min_coefs, max_coefs, xlim, ...) {
    polygon(x=c(xlim[1], 0, xlim[2], xlim[2], 0, xlim[1]),
            y=c(max_coefs[1] + xlim[1] * min_coefs[2],
                max_coefs[1],
                max_coefs[1] + xlim[2] * max_coefs[2],
                min_coefs[1] + xlim[2] * min_coefs[2],
                min_coefs[1],
                min_coefs[1] + xlim[1] * max_coefs[2]) , ...)
}
stancoefs <- extract(stanrr)
layout(t(1:2))
plot(x,y, main='lm()')
make_lm_poly(coef(slr2)-2*summary(slr2)$coefficients[,"Std. Error"], 
             coef(slr2)+2*summary(slr2)$coefficients[,"Std. Error"], 
             range(x),
             col=adjustcolor("purple", 0.2))
abline(coef=coef(slr2), col='purple', lwd=2)
abline(truth$b0, truth$b1, col='red', lwd=2)
plot(x,y, main='stan()')
make_lm_poly(c(quantile(stancoefs$b0, 0.025), quantile(stancoefs$b1, 0.025)),
             c(quantile(stancoefs$b0, 0.975), quantile(stancoefs$b1, 0.975)),
             range(x),
             col=adjustcolor("green", 0.2))
abline(truth$b0, truth$b1, col='red', lwd=2)
abline(mean(stancoefs$b0), mean(stancoefs$b1), col='green', lwd=2)
```

----------------


```{r plot_simdata_rr_again, fig.width=3.5*fig.dim, fig.height=1.5*fig.dim, echo=FALSE}
layout(t(1:2))
plot(x,y, main='lm()', ylim=c(-10,10))
make_lm_poly(coef(slr2)-2*summary(slr2)$coefficients[,"Std. Error"], 
             coef(slr2)+2*summary(slr2)$coefficients[,"Std. Error"], 
             range(x),
             col=adjustcolor("purple", 0.2))
abline(coef=coef(slr2), col='purple', lwd=2)
abline(truth$b0, truth$b1, col='red', lwd=2)
plot(x,y, main='stan()', ylim=c(-10,10))
make_lm_poly(c(quantile(stancoefs$b0, 0.025), quantile(stancoefs$b1, 0.025)),
             c(quantile(stancoefs$b0, 0.975), quantile(stancoefs$b1, 0.975)),
             range(x),
             col=adjustcolor("green", 0.2))
abline(truth$b0, truth$b1, col='red', lwd=2)
abline(mean(stancoefs$b0), mean(stancoefs$b1), col='green', lwd=2)
```




# Wrap-up

## Modeling, and Stan

1. How well a statistical method works depends on the situation.

2. We can describe the "situation" with a *probability model*.

3. Inference usually works best if the probabilistic model reflects reality .

4. Stan lets you do inference using (almost) arbitrary models.

5. Explicit models make it easy to simulate, and therefore test your methods.

## Hierarchical Bayesian models

1. It is often possible to infer things about *populations* that we can't infer about individuals.

2. Doing so leads to *sharing of information* (or, "power") between samples,
   and can improve accuracy.

3. Priors (and hyperpriors) on individual parameters provides a good way to do this.

