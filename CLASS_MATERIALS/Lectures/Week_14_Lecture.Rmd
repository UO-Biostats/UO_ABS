---
title: "Categorical data and crossvalidation"
author: "Peter Ralph"
date: "28 January 2020 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(rstan)
library(brms)
library(bayesplot)
library(matrixStats)
library(tidyverse)
options(mc.cores = parallel::detectCores())
options(digits=2)
```

```{r helpers, include=FALSE}
shadecurve <- function (pf, xlim, plot=TRUE, xlab='', ylab='', main='',
                        border="black", col=adjustcolor(border, 0.25), ...) {
    x <- seq(xlim[1], xlim[2], length.out=400)
    mids <- x[-1] - diff(x)/2
    df <- diff(pf(x, ...))
    if (plot) { plot(0, type='n', xlim=range(x), ylim=range(df),
                     main=main, xlab=xlab, ylab=ylab) }
    polygon(c(mids, x[length(x)], x[1]), c(df, 0, 0), col=col, border=border) 
}
```
```{r misc, include=FALSE}
library(lars)
```



# Gene ontology enrichment

## Recall: the data

```{r the_data}
(goterms <- read.table("data/go_terms.tsv", header=TRUE))
```

## 

Are some categories of genes more likely to be differentially regulated than others?
or, upregulated? downregulated?

## First, look at the data

```{r look_data}
goterms$per_up <- with(goterms, upregulated/sum(upregulated))
goterms$per_down <- with(goterms, downregulated/sum(downregulated))
goterms$per_diff <- with(goterms, (upregulated + downregulated)/sum(upregulated + downregulated))
goterms$per_nodiff <- with(goterms, (num_genes - upregulated + downregulated)/sum(num_genes - upregulated + downregulated))
goterms$diff_enrich <- goterms$per_diff / goterms$per_nodiff
goterms$up_vs_down <- goterms$per_up / goterms$per_down
goterms
```


## The poisson model

Let

$$\begin{aligned}
    N_i &=  (\text{number of no difference genes in category $i$})  \\
        &\sim \Poisson(\exp(\lambda_i^N))  \\
    U_i &=  (\text{number of upregulated genes in category $i$})  \\
        &\sim \Poisson(\exp(\lambda_i^U))  \\
    D_i &=  (\text{number of downregulated genes in category $i$})  \\
        &\sim \Poisson(\exp(\lambda_i^D)) .
\end{aligned}$$

. . .

and

$$\begin{aligned}
    \lambda_i^N &= \alpha_i \\
    \lambda_i^U &= \alpha_i + \delta_U + \beta_1[\text{top}_i] + \beta_2[i] \\
    \lambda_i^D &= \alpha_i + \delta_D + \beta_1[\text{top}_i] + \beta_2[i] + \gamma_i.
\end{aligned}$$


## Our Stan code: flat priors

```{r write_gostan, cache=TRUE}
go_model_code <- "
data {
    int nterms;
    int ntop; // number of 'top' categories
    int N[nterms];
    int U[nterms];
    int D[nterms];
    int top[nterms];
}
parameters {
    real alpha[nterms];
    real delta[2]; // up, down regulation
    real beta1[ntop];
    real beta2[nterms];
    real gamma[nterms];
}
model {
    real lambda_n;
    real lambda_u;
    real lambda_d;
    for (i in 1:nterms) {
        lambda_n = alpha[i];
        lambda_u = alpha[i] + delta[1] + beta1[top[i]] + beta2[i];
        lambda_d = alpha[i] + delta[2] + beta1[top[i]] + beta2[i] + gamma[i];
        N[i] ~ poisson(exp(lambda_n));
        U[i] ~ poisson(exp(lambda_u));
        D[i] ~ poisson(exp(lambda_d));
    }
}
"
go_model <- stan_model(model_code=go_model_code)
```

## 

```{r fit_gostan, cache=TRUE, dependson='write_gostan'}
go_fit <- sampling(go_model, chains = 4, iter = 2000,
                   data=list(nterms = nrow(goterms),
                             ntop = nlevels(goterms$top),
                             N = goterms$num_genes - goterms$upregulated - goterms$downregulated,
                             U = goterms$upregulated,
                             D = goterms$downregulated,
                             top = as.numeric(goterms$top)))
```
```{r summary, echo=FALSE}
rstan::summary(go_fit)$summary
```

## Uh-oh!! Rhat values:

```{r show_rhat, echo=FALSE}
rstan::summary(go_fit)$summary[,"Rhat"]
```

## Traces, alpha

```{r trace_alpha, echo=FALSE, fig.width=3*fig.dim, fig.height=2*fig.dim}
stan_trace(go_fit, pars=c('alpha', 'lp__'))
```

## Traces, gamma

```{r trace_gamma, echo=FALSE, fig.width=3*fig.dim, fig.height=2*fig.dim}
stan_trace(go_fit, pars=c('gamma', 'lp__'))
```

## `pairs()`

```{r show_pairs, echo=FALSE, fig.width=3*fig.dim, fig.height=2*fig.dim}
pairs(go_fit, pars=c('delta', 'alpha[1]', 'beta1[1]', 'beta2[1]', 'gamma[1]'))
```

# R interlude

## Indexing with names is good!!

Number of 'high' values:
```{r naming_things}
y <- rnorm(1e4)
# cutoffs:
x <- c(1, 2, 3)
sum(y > x[3])
```

. . .

Wait, let's make more cutoffs:
```{r naming_things3}
x <- c(1, 1.5, 2.5, 3)
sum(y > x[3])
```

. . . whoops!

##


Wait, let's add another level:
```{r naming_things2}
x <- c(low=1, "lowish"=1.5, 'highish'=2.5, high=3)
sum(y > x['high'])
```


## BUT: A warning about factors

```{r warning}
f <- factor(c('low', 'med', 'high'))
x <- c(low=1, med=2, high=3)
```

What is `x[f[3]]`?

. . .

```{r warning2}
x[f[3]]
```

## Defensive programming

```{r checkit}
xf <- x[f]
stopifnot(all(names(xf) == f))
```

. . .

```{r checkit2}
xf <- x[as.character(f)]
stopifnot(all(names(xf) == f))
```

# The effect of priors

## Same model, with priors

```{r write_gostan2, cache=TRUE}
go_model_code2 <- "
data {
    int nterms;
    int ntop; // number of 'top' categories
    int N[nterms];
    int U[nterms];
    int D[nterms];
    int top[nterms];
}
parameters {
    real alpha[nterms];
    real delta[2]; // up, down regulation
    real beta1[ntop];
    real beta2[nterms];
    real gamma[nterms];
}
model {
    real lambda_n;
    real lambda_u;
    real lambda_d;
    for (i in 1:nterms) {
        lambda_n = alpha[i];
        lambda_u = alpha[i] + delta[1] + beta1[top[i]] + beta2[i];
        lambda_d = alpha[i] + delta[2] + beta1[top[i]] + beta2[i] + gamma[i];
        N[i] ~ poisson(exp(lambda_n));
        U[i] ~ poisson(exp(lambda_u));
        D[i] ~ poisson(exp(lambda_d));
    }
    alpha ~ normal(0, 5); // flat-ish
    delta ~ normal(0, 5); // flat-ish
    beta1 ~ cauchy(0, 0.1); // very peaked
    beta2 ~ cauchy(0, 0.1); // very peaked
    gamma ~ cauchy(0, 0.1); // very peaked
}
"
go_model2 <- stan_model(model_code=go_model_code2)
```

## Fit, again

```{r fit_gostan2, cache=TRUE, dependson='write_gostan2'}
go_fit2 <- sampling(go_model2, chains = 4, iter = 2000,
                    data=list(nterms = nrow(goterms),
                              ntop = nlevels(goterms$top),
                              N = goterms$num_genes - goterms$upregulated - goterms$downregulated,
                              U = goterms$upregulated,
                              D = goterms$downregulated,
                              top = as.numeric(goterms$top)))
```
```{r summary2, echo=FALSE}
rstan::summary(go_fit2)$summary
```


## `stan_trace()`, alpha

```{r trace_alpha2, echo=FALSE, fig.width=3*fig.dim, fig.height=2*fig.dim}
stan_trace(go_fit2, pars=c('alpha', 'lp__'))
```

## `stan_trace()`, gamma

```{r trace_gamma2, echo=FALSE, fig.width=3*fig.dim, fig.height=2*fig.dim}
stan_trace(go_fit2, pars=c('gamma', 'lp__'))
```

## `pairs()`

```{r show_pairs2, echo=FALSE, fig.width=3*fig.dim, fig.height=2*fig.dim}
pairs(go_fit2, pars=c('delta', 'alpha[1]', 'beta1[1]', 'beta2[1]', 'gamma[1]'))
```

## Parameter estimates: main effects

```{r show_params1, fig.width=2.5*fig.dim, message=FALSE, echo=FALSE}
post2 <- as.array(go_fit2)
bayesplot::mcmc_intervals(post2, regex_pars=c('alpha', 'delta')) + scale_y_discrete(labels=c(paste("alpha:", goterms$term), paste("delta:", c("up", "down"))))
```

## Parameter estimates: differential regulation

```{r show_params2, fig.width=2.5*fig.dim, message=FALSE, echo=FALSE}
bayesplot::mcmc_intervals(post2, regex_pars=c('beta1', 'beta2')) + scale_y_discrete(labels=c(paste("beta1:", levels(goterms$top)), paste("beta2:", goterms$term)))
```

## Parameter estimates: down- versus upregulation

```{r show_params3, fig.width=2.5*fig.dim, message=FALSE, echo=FALSE}
bayesplot::mcmc_intervals(post2, regex_pars='gamma') + scale_y_discrete(labels=paste("gamma:", goterms$term))
```

## in-class: conclusions

In conclusion, 
we used Stan fit a Poisson model in which the cell means are given by the equation above.
(say Bayesian somehow)
The priors on each parameter are specified above.
Standard methods were used for assessing convergence of the Hamiltonian Monte Carlo algorithm.
Most genes are not differentially regulated:
in the data, X percent were; our model fit a 95% credible interval of A to B for this effect.
However, apoptosis and cell motility genes are strongly differentially regulated,
showing relative enrichments of about 25-fold and 3.3-fold, respectively, over "not differentiated".
(TODO: get the actual numbers, as exp(beta))
95% credible intervals for these two effects ranged from X to Y. (look those up)
TODO: up vs down for apoptosis; sperm motility;
Remaining effects were not statistically signficant.




# Crossvalidation

## A common workflow

It's often important to consider more than one model, e.g.:

> - different explanatory variables
> - simple/complex
> - different response distributions
> - different priors

. . .

The problem is that then there's *more than one answer*.

. . .

How do we compare models?



# Variable selection


## Example data

![from Efron, Hastie, Johnstone, \& Tibshirani](images/lars_diabetes_data.png)

-----------

```{r lars_data}
library(lars)
data(diabetes)
class(diabetes$x2) <- "matrix"
```
```
diabetes                 package:lars                  R Documentation

Blood and other measurements in diabetics

Description:

     The ‘diabetes’ data frame has 442 rows and 3 columns. These are
     the data used in the Efron et al "Least Angle Regression" paper.

Format:

     This data frame contains the following columns:

     x a matrix with 10 columns

     y a numeric vector

     x2 a matrix with 64 columns
```

---------------

The dataset has

- 442 diabetes patients
- 10 main variables: age, gender, body mass index, average blood pressure (map), 
  and six blood serum measurements (tc, ldl, hdl, tch, ltg, glu)
- 45 interactions, e.g. `age:ldl`
- 9 quadratic effects, e.g. `age^2`
- measure of disease progression taken one year later: `y`

##

```{r show_cors, fig.width=2.3*fig.dim, fig.height=2.3*fig.dim, echo=FALSE}
pairs(cbind(diabetes$x, y=diabetes$y))
```

##

```{r print_cors}
cor(cbind(diabetes$x, y=diabetes$y))
```

## Crossvalidation plan

1. Put aside 20% of the data for *testing*.

2. Refit the model.

3. Predict the test data; compute
   $$\begin{aligned}
    S = \sqrt{\frac{1}{M} \sum_{k=1}^M (\hat y_i - y_i)^2}
   \end{aligned}$$

. . .


4. Repeat for the other four 20%s.

5. Compare.


## Crossvalidation

First let's split the data into testing and training just once:
```{r split_data}
test_indices <- (rbinom(length(diabetes$y), size=1, prob=0.2) == 1)
test_d <- cbind(data.frame(y=diabetes$y[test_indices]),
                diabetes$x2[test_indices,])
training_d <- cbind(data.frame(y=diabetes$y[!test_indices]),
                diabetes$x2[!test_indices,])
```

## Ordinary linear regression

```{r ols}
ols <- lm(y ~ ., data=training_d)
summary(ols)
```

-------------

```{r crossval_ols}
ols_pred <- predict(ols, newdata=test_d)
ols_mse <- sqrt(mean((ols_pred - test_d$y)^2))
```
With ordinary linear regression, we got a root-mean-square-prediction-error of `r ols_mse` (on the *test* data),
compared to a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` for the *training* data.

. . .

This suggests there's some overfitting going on.

------------

```{r plot_ols, echo=-1}
layout(t(1:2))
plot(training_d$y, predict(ols), xlab="true values", ylab="OLS predicted", main="training data", pch=20)
abline(0,1)
plot(test_d$y, ols_pred, xlab="true values", ylab="OLS predicted", main="test data", pch=20)
abline(0,1)
```

## A sparsifying prior

We have a lot of predictors: 64 of them.
A good guess is that only a few are really useful.
So, we can put a *sparsifying* prior on the coefficients,
i.e.,  $\beta$s in
$$\begin{aligned}
    y = \beta_0 + \beta_1 x_1 + \cdots \beta_n x_n + \epsilon
\end{aligned}$$


# Crossvalidation exploration

## Who says we don't do experiments?

1. Simulate data with `y ~ a + b x[1] + c x[2]`, and fit a linear model.
2. Measure in-sample and out-of-sample prediction error.
3. Add spurious variables, and report the above as a function of number of variables.

. . .

4. Simulate data with *many, weakly predictive* explanatory variables.
5. Compare different methods for fitting linear models.


## (in-class)

```{r in_class_sims, cache=TRUE}
set.seed(23)
N <- 500
df <- data.frame(x1 = rnorm(N),
                 x2 = runif(N))
params <- list(intercept = 2.0,
               x1 = 7.0,
               x2 = -3.0,
               sigma = 0.1 * 10)

pred_y <- params$intercept + params$x1 * df$x1 + params$x2 * df$x2 
df$y <- rnorm(N, mean=pred_y, sd=params$sigma)

pairs(df)

# write the crossvalidation error function

kfold <- function (K, df) {
    N <- nrow(df)
    Kfold <- sample(rep(1:K, N/K))

    do_xval <- function (k) {
        the_lm <- lm(y ~ ., data=df, subset=(Kfold != k))
        train_error <- sqrt(mean(resid(the_lm)^2))
        test_y <- df$y[Kfold == k]
        test_error <- sqrt(mean( (test_y - predict(the_lm, newdata=subset(df, Kfold==k)))^2 ))
        return(c('test'=test_error, 'train'=train_error))
    }

    results <- sapply(1:K, do_xval)
    return(results)
}

K <- 10
max_M <- 300
do_m <- floor(seq(from=2, to=max_M-1, length.out=40))
all_results <- matrix(NA, nrow=length(do_m) + 1, ncol=2)
first_results <- rowMeans(kfold(K, df))
all_results[1,] <- first_results
colnames(all_results) <- names(first_results)

noise_df <- matrix(rnorm(N * (N-2)), nrow=N)
colnames(noise_df) <- paste0('z', 1:ncol(noise_df))
new_df <- cbind(df, noise_df)
for (j in seq_along(do_m)) {
    m <- do_m[j]
    all_results[j,] <- rowMeans(kfold(K, new_df[,1:(m+1)]))
}

matplot(c(2, do_m), all_results, type='l', xlab='number of variables', ylab='root mean square error')
legend("topleft", lty=1, col=1:2, legend=colnames(all_results))




```

# Sparseness and scale mixtures

## Encouraging sparseness

Suppose we do regression with a *large* number of predictor variables.

. . .

The resulting coefficients are **sparse** if most are zero.

. . .

The idea is to "encourage" all the coefficients to be zero,
**unless**  they *really* want to be nonzero,
in which case we let them be whatever they want.

. . .

This tends to discourage overfitting.

-------------

> The idea is to "encourage" all the coefficients to be zero,
> **unless**  they *really* want to be nonzero,
> in which case we let them be whatever they want.


To do this, we want a prior which is very peak-ey at zero
*but* flat away from zero ("spike-and-slab").


-------------------

:::::::::::::: {.columns}
::: {.column width="50%"}


Compare the Normal

$$\begin{aligned}
    X \sim \Normal(0,1)
\end{aligned}$$

to the "exponential scale mixture of Normals",

$$\begin{aligned}
    X &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```{r scale_mixtures, fig.width=1.5*fig.dim, echo=FALSE}
shadecurve(pnorm, xlim=c(-5,5), main='Normal(0,1)')
sdvals <- qexp(seq(0,1,length.out=10)[-c(1,10)])
for (u in sdvals) {
    shadecurve(pnorm, xlim=c(-5,5), sd=u, plot=(u==sdvals[1]), main='Normal(0, Exp(1))')
}
```

:::
::::::::::::::

## Why use a scale mixture?

1. Lets the data choose the appropriate scale of variation.

2. Weakly encourages $\sigma$ to be small: so, 
   as much variation as possible is explained by *signal* instead of *noise*.

3. Gets you a prior that is more peaked at zero and flatter otherwise.


## Implementation

:::::::::::::: {.columns}
::: {.column width="50%"}


Note that

$$\begin{aligned}
    \beta &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

is equivalent to

$$\begin{aligned}
    \beta &= \sigma \gamma \\
    \gamma &\sim \Normal(0,1) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
    sigma ~ exponential(1);
}
```

is equivalent to

```
parameters {
    real gamma;
    real<lower=0> sigma;
}
transformed parameters {
    real beta;
    beta = gamma * sigma;
}
model {
    gamma ~ normal(0, 1);
    sigma ~ exponential(1);
}
```

The second version **is better** for Stan.


:::
:::::::::::::::

-------------

:::::::::::::: {.columns}
::: {.column width="50%"}

Why is it better?


```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
    sigma ~ exponential(1);
}
```

In the first, the optimal step size
*depends on `sigma`*.

```
parameters {
    real gamma;
    real<lower=0> sigma;
}
transformed parameters {
    real beta;
    beta = gamma * sigma;
}
model {
    gamma ~ normal(0, 1);
    sigma ~ exponential(1);
}
```


:::
::: {.column width="50%"}

```{r sigma_phase, echo=FALSE, fig.width=1.5*fig.dim}
xx <- seq(0, 0.5, length.out=401)[-1]
yy <- seq(-4,4,length.out=401)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=x) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="beta")
contour(xx, yy, -log10(xy), add=TRUE)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=1) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="gamma")
contour(xx, yy, -log10(xy), add=TRUE)
```

:::
:::::::::::

## A strongly sparsifying prior



:::::::::::::: {.columns}
::: {.column width="50%"}

The "horseshoe":

$$\begin{aligned}
    \beta_j &\sim \Normal(0, \lambda_j) \\
    \lambda_j &\sim \Cauchy(0, \tau) \\
    \tau &\sim \Unif(0, 1) 
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    vector[p] d_beta;
    vector[p] d_lambda;
    real<lower=0, upper=1> tau;
}
transformed parameters {
    vector[p] beta;
    beta = d_beta .* d_lambda * tau;
}
model {
    d_beta ~ normal(0, 1);
    d_lambda ~ cauchy(0, 1);
    // tau ~ uniform(0, 1); // uniform
}
```

:::
:::::::::::

## The Cauchy as a scale mixture

Recall that if

$$\begin{aligned}
    \beta &\sim \Normal(0, 1/\sqrt{\lambda}) \\
    \lambda &\sim \Gam(1/2, 1/2)
\end{aligned}$$

then

$$\begin{aligned}
    \beta &\sim \Cauchy(0, 1).
\end{aligned}$$


# With brms

## 

```{r do_brms, cache=TRUE}
xy <- cbind(data.frame(y=diabetes$y), diabetes$x2)
names(xy) <- gsub("[:^]", "_", names(xy))

# ols
blm <- brm(y ~ ., data=xy, family=gaussian(link='identity'))
# horseshoe
bhs <- brm(y ~ ., data=xy, family=gaussian(link='identity'),
           prior=c(set_prior(horseshoe(), class="b")))
```

## 

:::::::::::::: {.columns}
::: {.column width="50%"}

```{r brmcrossval, cache=TRUE, dependson="do_brms"}
(klm <- kfold(blm, K=5))
```


:::
::: {.column width="50%"}

```{r brmcrossval2, cache=TRUE, dependson="do_brms"}
(khs <- kfold(bhs, K=5))
```

:::
::::::::::::::

## 

:::::::::::::: {.columns}
::: {.column width="50%"}

```{r brmresults, echo=FALSE, fig.height=2*fig.dim}
bhs_samps <- fixef(blm, summary=FALSE)
bayesplot::mcmc_intervals(bhs_samps) + ggtitle("OLS")
```


:::
::: {.column width="50%"}

```{r brmresults2, echo=FALSE}
bhs_samps <- fixef(bhs, summary=FALSE, fig.height=2*fig.dim)
bayesplot::mcmc_intervals(bhs_samps) + ggtitle("horseshoe")
```

:::
::::::::::::::




# Using the horseshoe

## What's an appropriate noise distribution?

```{r show_y, echo=FALSE, fig.width=3*fig.dim, fig.height=1.5*fig.dim}
layout(t(1:2))
hist(diabetes$y, breaks=30, main="diabetes, response")
qqnorm(diabetes$y)
qqline(diabetes$y)
```

## Aside: quantile-quantile plots

The idea is to plot the *quantiles* of each distribution against each other.

If these are *datasets*, this means just plotting their *sorted values* against each other.

```{r qq, fig.width=3*fig.dim, echo=c(1,2,4)}
x <- rnorm(1e4)
y <- rbeta(1e4, 2, 2)
layout(t(1:3))
plot(sort(x), sort(y)); qqplot(x, y, main="qqplot"); qqnorm(y, main="qnorm")
```


## Regression with a horseshoe prior

Uses a [reparameterization](https://betanalpha.github.io/assets/case_studies/fitting_the_cauchy.html) of the Cauchy as a scale mixture of normals.


```{r horseshoe_model, cache=TRUE}
horseshoe_block <- "
data {
    int N;
    int p;
    vector[N] y;
    matrix[N,p] x;
}
parameters {
    real b0;
    vector[p] d_beta;
    vector[p] d_a;
    vector<lower=0>[p] d_b;
    real<lower=0, upper=1> tau;
    real<lower=0> sigma;
}
transformed parameters {
    vector[p] beta;
    vector[N] f;
    beta = d_beta .* d_a .* sqrt(d_b) * tau;
    f = b0 + x * beta;
}
model {
    y ~ normal(f, sigma);
    // HORSESHOE PRIOR:
    d_beta ~ normal(0, 1);
    d_a ~ normal(0, 1);
    d_b ~ inv_gamma(0.5, 0.5);
    // tau ~ uniform(0, 1); // uniform
    // priors on noise distribution:
    sigma ~ normal(0, 10);
}"
```

------------------

Note the data have already been normalized,
with the exception of $y$:

```{r data_summary}
summary(training_d)
```

------------------

```{r run_horseshoe, cache=TRUE, depends="horseshoe_model"}
horseshoe_fit <- stan(model_code=horseshoe_block,
                      data=list(N=nrow(training_d),
                                p=ncol(training_d)-1,
                                y=(training_d$y 
                                   - median(training_d$y))
                                  /mad(training_d$y),
                                x=as.matrix(training_d[,-1])),
                      iter=1000,
                      control=list(adapt_delta=0.999,
                                   max_treedepth=15))
```

--------------

```{r summary_hs}
(hs_summary <- rstan::summary(horseshoe_fit, pars=c("b0", "sigma", "beta"))$summary)
```

--------------

First compare the resulting regression parameters to OLS values.

```{r compare_betas, echo=1:5, fig.width=3*fig.dim}
hs_samples <- rstan::extract(horseshoe_fit, pars=c("b0", "sigma", "beta"))
# rescale to real units
post_med_intercept <- median(hs_samples$b0) * mad(training_d$y) + median(training_d$y)
post_med_sigma <- median(hs_samples$sigma) * mad(training_d$y)
post_med_slopes <- colMedians(hs_samples$beta) * mad(training_d$y)

ols_ses <- summary(ols)$coefficients[,2]

layout(t(1:2))
plot(coef(ols), xlab="coefficient", 
     ylab="estimate", main="OLS", pch=20)
segments(x0=1:65,
         y0=coef(ols) - 2 * ols_ses,
         y1=coef(ols) + 2 * ols_ses,
         col=adjustcolor("black", 0.5))
plot(c(post_med_intercept, post_med_slopes), 
     xlab='coefficient', ylab="estimate",
     col='red', pch=20, main="sparse")
segments(x0=1, 
         y0=mad(training_d$y) * quantile(hs_samples$b0, probs=0.025) + median(training_d$y), 
         y1=mad(training_d$y) * quantile(hs_samples$b0, probs=0.975) + median(training_d$y), 
         col=adjustcolor('red', 0.5))
segments(x0=2:65, 
         y0=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.025), 
         y1=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.975),
         col=adjustcolor('red', 0.5))
```


--------------

The coefficient estimates from OLS are *wierd*.

```{r what_coefs, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$ols[-1]), decreasing=TRUE)),]
```

--------------

And, quite different than what Stan gets.

```{r what_coefs2, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$stan[-1]), decreasing=TRUE)),]
```



--------------

Now let's look at out-of-sample prediction error,
using the posterior median coefficient estimates:

```{r pred_stan}
pred_stan <- function (x) {
    post_med_intercept + as.matrix(x) %*% post_med_slopes
}
pred_y <- pred_stan(test_d[,-1])
stan_pred_error <- sqrt(mean((test_d$y - pred_y)^2))
stan_mse_resid <- sqrt(mean((training_d$y - pred_stan(training_d[,-1]))^2))

plot(test_d$y, pred_y, xlab="true values", ylab="predicted values", main="test data")
abline(0,1)
```

## Conclusions?


1. Our "sparse" model is certainly more sparse, and arguably more interpretable.

2. It has a root-mean-square prediction error of 
    `r stan_pred_error`
    on the *test* data, and
    `r stan_mse_resid`
    on the training data.

3. This is substantially better than ordinary linear regression, 
   which had a root-mean-square prediction error of `r ols_mse` on the test data,
   and a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` on the training data.


. . .

The sparse model is more interpretable,
and more generalizable.


# Interlude

## 

<!-- from https://www.tandfonline.com/doi/full/10.1080/24749508.2018.1481633 -->

![Estimation of infiltration rate from soil properties using regression model for cultivated land](images/infiltration_title.png){ width=50% }

![EIR = 14,195.35 - 141.75 (sand%) - 142.10 (silt%) - 142.56 (clay%)](images/infiltration_figure.png)

##

Use [the data](infiltration_data.tsv)
to try to reproduce their model:

```
BIR = 14,195.35 - 141.75 (sand%) - 142.10 (silt%) - 142.56 (clay%)
```

They're not wrong! What's going on?

```{r infil_data, include=FALSE}
head(infil <- read.table("data/infiltration_data.tsv", header=TRUE))
```


# Generated quantities

## Another approach

For interpretation,
we looked at parameter estimates,
and relied on strong priors to deal with model nonidentifiability.

A possibly better alternative 
is to ask for the posterior distribution of the quantities 
we actually want to know (not the same as the parameters, in this case).

## Once more:

```{r write_gostan3, cache=TRUE}
go_model_code3 <- "
data {
    int nterms;
    int ntop; // number of 'top' categories
    int N[nterms];
    int U[nterms];
    int D[nterms];
    int top[nterms];
}
parameters {
    vector[nterms] alpha;
    vector[2] delta; // up, down regulation
    vector[ntop] beta1;
    vector[nterms] beta2;
    vector[nterms] gamma;
}
transformed parameters {
    vector[nterms] mean_n;
    vector[nterms] mean_u;
    vector[nterms] mean_d;
    mean_n = exp(alpha);
    mean_u = exp(alpha + delta[1] + beta1[top] + beta2);
    mean_d = exp(alpha + delta[2] + beta1[top] + beta2 + gamma);
}
model {
    N ~ poisson(mean_n);
    U ~ poisson(mean_u);
    D ~ poisson(mean_d);
    alpha ~ normal(0, 5); // flat-ish
    delta ~ normal(0, 5); // flat-ish
    beta1 ~ cauchy(0, 1.0);
    beta2 ~ cauchy(0, 1.0);
    gamma ~ cauchy(0, 1.0);
}
"
go_model3 <- stan_model(model_code=go_model_code3)
```

## Fit, again

```{r fit_gostan3, cache=TRUE, dependson='write_gostan3'}
go_fit3 <- sampling(go_model3, chains = 4, iter = 4000,
                    data=list(nterms = nrow(goterms),
                              ntop = nlevels(goterms$top),
                              N = goterms$num_genes - goterms$upregulated - goterms$downregulated,
                              U = goterms$upregulated,
                              D = goterms$downregulated,
                              top = as.numeric(goterms$top)))
```
```{r summary3, echo=FALSE}
rstan::summary(go_fit3)$summary
```

## What do we want to know?

Let $\mu^N_i = \exp(\lambda^N_i)$ and $\mu^N = \sum_i \mu^N_i$.

. . .

Is term $i$ overrepresented among differentially regulated genes?
$$\begin{aligned}
    \frac{ (\mu^U_i + \mu^D_i) / (\mu^U + \mu^D) }
         { \mu^N_i / \mu^N }
\end{aligned}$$

. . .

Is term $i$ more up- than down-regulated?
$$\begin{aligned}
    \frac{ \mu^U_i / \mu^U }
         { \mu^D_i / \mu^D }
\end{aligned}$$


## Posterior distributions: GO term enrichments

```{r get_posts}
samps <- rstan::extract(go_fit3)
diff_enrichment <- ((samps$mean_d + samps$mean_u) / (rowSums(samps$mean_d) + rowSums(samps$mean_u))) / (samps$mean_n / rowSums(samps$mean_n))
up_enrichment <- (samps$mean_u / rowSums(samps$mean_u)) / (samps$mean_d / rowSums(samps$mean_d))
colnames(diff_enrichment) <- paste0("diff:", goterms$term)
colnames(up_enrichment) <- paste0("up:", goterms$term)

diff_top_enrichment <- (simplify2array(by(t(samps$mean_u + samps$mean_d), goterms$top, colSums))  / rowSums(samps$mean_u)) / (simplify2array(by(t(samps$mean_n), goterms$top, colSums))/ rowSums(samps$mean_n))
up_top_enrichment <- (simplify2array(by(t(samps$mean_u), goterms$top, colSums))  / rowSums(samps$mean_u)) / (simplify2array(by(t(samps$mean_d), goterms$top, colSums))/ rowSums(samps$mean_d))
colnames(diff_top_enrichment) <- paste0("diff:", levels(goterms$top))
colnames(up_top_enrichment) <- paste0("up:", levels(goterms$top))
```

## Posterior distributions: GO term enrichments

::: {.columns}
::::::: {.column width=50%}

```{r plot_posts}
bayesplot::mcmc_intervals(cbind(up_enrichment, diff_enrichment))
```

::: 
::::::: {.column width=50%}

```{r plot_posts2}
bayesplot::mcmc_intervals(cbind(up_top_enrichment, diff_top_enrichment))
```

::: 
:::::::

## log scale

::: {.columns}
::::::: {.column width=50%}

```{r plot_posts3}
bayesplot::mcmc_intervals(log(cbind(up_enrichment, diff_enrichment)))
```

::: 
::::::: {.column width=50%}

```{r plot_posts4}
bayesplot::mcmc_intervals(log(cbind(up_top_enrichment, diff_top_enrichment)))
```

::: 
:::::::

