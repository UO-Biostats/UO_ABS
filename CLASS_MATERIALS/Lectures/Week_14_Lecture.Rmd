---
title: "Sparsifying priors, and variable selection"
author: "Peter Ralph"
date: "28 January 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```

```{r helpers, include=FALSE}
shadecurve <- function (pf, xlim, plot=TRUE, xlab='', ylab='', main='',
                        border="black", col=adjustcolor(border, 0.25), ...) {
    x <- seq(xlim[1], xlim[2], length.out=400)
    mids <- x[-1] - diff(x)/2
    df <- diff(pf(x, ...))
    if (plot) { plot(0, type='n', xlim=range(x), ylim=range(df),
                     main=main, xlab=xlab, ylab=ylab) }
    polygon(c(mids, x[length(x)], x[1]), c(df, 0, 0), col=col, border=border) 
}
```


# Overview

## Problems with linear models

1. "Too much" noise (i.e., non-Normal noise).

2. Too many variables.


## Problems with linear models

1. <del>"Too much" noise (i.e., non-Normal noise).</del>


2. Too many variables. **(today)**


# Variable selection


## Example data

![from Efron, Hastie, Johnstone, \& Tibshirani](lars_diabetes_data.png)

-----------

```{r lars_data}
library(lars)
data(diabetes)
class(diabetes$x2) <- "matrix"
```
```
diabetes                 package:lars                  R Documentation

Blood and other measurements in diabetics

Description:

     The ‘diabetes’ data frame has 442 rows and 3 columns. These are
     the data used in the Efron et al "Least Angle Regression" paper.

Format:

     This data frame contains the following columns:

     x a matrix with 10 columns

     y a numeric vector

     x2 a matrix with 64 columns
```

---------------

The dataset has

- 442 diabetes patients
- 10 main variables: age, gender, body mass index, average blood pressure (map), 
  and six blood serum measurements (tc, ldl, hdl, tch, ltg, glu)
- 45 interactions, e.g. `age:ldl`
- 9 quadratic effects, e.g. `age^2`
- measure of disease progression taken one year later: `y`

##

```{r show_cors, fig.width=2.3*fig.dim, fig.height=2.3*fig.dim, echo=FALSE}
pairs(cbind(diabetes$x, y=diabetes$y))
```

##

```{r print_cors}
cor(cbind(diabetes$x, y=diabetes$y))
```

## Crossvalidation plan

1. Put aside 20% of the data for *testing*.

2. Refit the model.

3. Predict the test data; compute
   $$\begin{aligned}
    S = \sqrt{\frac{1}{M} \sum_{k=1}^M (\hat y_i - y_i)^2}
   \end{aligned}$$

. . .

To be more thorough, we'd:

4. Repeat for the other four 20%s.

5. Compare.


## Crossvalidation

First let's split the data into testing and training just once:
```{r split_data}
test_indices <- (rbinom(length(diabetes$y), size=1, prob=0.2) == 1)
test_d <- cbind(data.frame(y=diabetes$y[test_indices]),
                diabetes$x2[test_indices,])
training_d <- cbind(data.frame(y=diabetes$y[!test_indices]),
                diabetes$x2[!test_indices,])
```

## Ordinary linear regression

```{r ols}
ols <- lm(y ~ ., data=training_d)
summary(ols)
```

-------------

```{r crossval_ols}
ols_pred <- predict(ols, newdata=test_d)
ols_mse <- sqrt(mean((ols_pred - test_d$y)^2))
```
With ordinary linear regression, we got a root-mean-square-prediction-error of `r ols_mse` (on the *test* data),
compared to a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` for the *training* data.

. . .

This suggests there's some overfitting going on.

------------

```{r plot_ols}
layout(t(1:2))
plot(training_d$y, predict(ols), xlab="true values", ylab="OLS predicted", main="training data")
abline(0,1)
plot(test_d$y, ols_pred, xlab="true values", ylab="OLS predicted", main="test data")
abline(0,1)
```

## A sparsifying prior

We have a lot of predictors: 64 of them.
A good guess is that only a few are really useful.
So, we can put a *sparsifying* prior on the coefficients,
i.e.,  $\beta$s in
$$\begin{aligned}
    y = \beta_0 + \beta_1 x_1 + \cdots \beta_n x_n + \epsilon
\end{aligned}$$


# Interlude

## 

<!-- from https://www.tandfonline.com/doi/full/10.1080/24749508.2018.1481633 -->

![Estimation of infiltration rate from soil properties using regression model for cultivated land](infiltration_title.png){ width=50% }

![EIR = 14,195.35 - 141.75 (sand%) - 142.10 (silt%) - 142.56 (clay%)](infiltration_figure.png)

##

Use [the data](infiltration_data.tsv)
to try to reproduce their model:

```
BIR = 14,195.35 - 141.75 (sand%) - 142.10 (silt%) - 142.56 (clay%)
```

They're not wrong! What's going on?

```{r infil_data, include=FALSE}
head(infil <- read.table("infiltration_data.tsv", header=TRUE))
```


# Sparseness and scale mixtures

## Encouraging sparseness

Suppose we do regression with a *large* number of predictor variables.

. . .

The resulting coefficients are **sparse** if most are zero.

. . .

The idea is to "encourage" all the coefficients to be zero,
**unless**  they *really* want to be nonzero,
in which case we let them be whatever they want.

. . .

This tends to discourage overfitting.

-------------

> The idea is to "encourage" all the coefficients to be zero,
> **unless**  they *really* want to be nonzero,
> in which case we let them be whatever they want.


To do this, we want a prior which is very peak-ey at zero
*but* flat away from zero ("spike-and-slab").


-------------------

:::::::::::::: {.columns}
::: {.column width="50%"}


Compare the Normal

$$\begin{aligned}
    X \sim \Normal(0,1)
\end{aligned}$$

to the "exponential scale mixture of Normals",

$$\begin{aligned}
    X &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```{r scale_mixtures, fig.width=1.5*fig.dim, echo=FALSE}
shadecurve(pnorm, xlim=c(-5,5), main='Normal(0,1)')
sdvals <- qexp(seq(0,1,length.out=10)[-c(1,10)])
for (u in sdvals) {
    shadecurve(pnorm, xlim=c(-5,5), sd=u, plot=(u==sdvals[1]), main='Normal(0, Exp(1))')
}
```

:::
::::::::::::::

## Why use a scale mixture?

1. Lets the data choose the appropriate scale of variation.

2. Weakly encourages $\sigma$ to be small: so, 
   as much variation as possible is explained by *signal* instead of *noise*.

3. Gets you a prior that is more peaked at zero and flatter otherwise.


## Implementation

:::::::::::::: {.columns}
::: {.column width="50%"}


Note that

$$\begin{aligned}
    \beta &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

is equivalent to

$$\begin{aligned}
    \beta &= \sigma \gamma \\
    \gamma &\sim \Normal(0,1) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
    sigma ~ exponential(1);
}
```

is equivalent to

```
parameters {
    real gamma;
    real<lower=0> sigma;
}
transformed parameters {
    real beta;
    beta = gamma * sigma;
}
model {
    gamma ~ normal(0, 1);
    sigma ~ exponential(1);
}
```

The second version **is better** for Stan.


:::
:::::::::::::::

-------------

:::::::::::::: {.columns}
::: {.column width="50%"}

Why is it better?


```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
}
```

In the first, the optimal step size
*depends on `sigma`*.

:::
::: {.column width="50%"}

```{r sigma_phase, echo=FALSE, fig.width=1.5*fig.dim}
xx <- seq(0, 0.5, length.out=401)[-1]
yy <- seq(-4,4,length.out=401)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=x) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="beta")
contour(xx, yy, -log10(xy), add=TRUE)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=1) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="gamma")
contour(xx, yy, -log10(xy), add=TRUE)
```

:::
:::::::::::

## A strongly sparsifying prior



:::::::::::::: {.columns}
::: {.column width="50%"}

The "horseshoe":

$$\begin{aligned}
    \beta_j &\sim \Normal(0, \lambda_j) \\
    \lambda_j &\sim \Cauchy(0, \tau) \\
    \tau &\sim \Unif(0, 1) 
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    vector[p] d_beta;
    vector[p] d_lambda;
    real<lower=0, upper=1> tau;
}
transformed parameters {
    vector[p] beta;
    beta = d_beta .* d_lambda * tau;
}
model {
    d_beta ~ normal(0, 1);
    d_lambda ~ cauchy(0, 1);
    // tau ~ uniform(0, 1); // uniform
}
```

:::
:::::::::::

## The Cauchy as a scale mixture

Recall that if

$$\begin{aligned}
    \beta &\sim \Normal(0, 1/\sqrt{\lambda}) \\
    \lambda &\sim \Gam(1/2, 1/2)
\end{aligned}$$

then

$$\begin{aligned}
    \beta &\sim \Cauchy(0, 1).
\end{aligned}$$



# Using the horseshoe

## What's an appropriate noise distribution?

```{r show_y, echo=FALSE, fig.width=3*fig.dim, fig.height=1.5*fig.dim}
layout(t(1:2))
hist(diabetes$y, breaks=30, main="diabetes, response")
qqnorm(diabetes$y)
qqline(diabetes$y)
```

## Aside: quantile-quantile plots

The idea is to plot the *quantiles* of each distribution against each other.

If these are *datasets*, this means just plotting their *sorted values* against each other.

```{r qq, fig.width=3*fig.dim, echo=c(1,2,4)}
x <- rnorm(1e4)
y <- rbeta(1e4, 2, 2)
layout(t(1:3))
plot(sort(x), sort(y)); qqplot(x, y, main="qqplot"); qqnorm(y, main="qnorm")
```


## Regression with a horseshoe prior

Uses a [reparameterization](https://betanalpha.github.io/assets/case_studies/fitting_the_cauchy.html) of the Cauchy as a scale mixture of normals.


```{r horseshoe_model, cache=TRUE}
horseshoe_block <- "
data {
    int N;
    int p;
    vector[N] y;
    matrix[N,p] x;
}
parameters {
    real b0;
    vector[p] d_beta;
    vector[p] d_a;
    vector<lower=0>[p] d_b;
    real<lower=0, upper=1> tau;
    real<lower=0> sigma;
}
transformed parameters {
    vector[p] beta;
    vector[N] f;
    beta = d_beta .* d_a .* sqrt(d_b) * tau;
    f = b0 + x * beta;
}
model {
    y ~ normal(f, sigma);
    // HORSESHOE PRIOR:
    d_beta ~ normal(0, 1);
    d_a ~ normal(0, 1);
    d_b ~ inv_gamma(0.5, 0.5);
    // tau ~ uniform(0, 1); // uniform
    // priors on noise distribution:
    sigma ~ normal(0, 10);
}"
```

------------------

Note the data have already been normalized,
with the exception of $y$:

```{r data_summary}
summary(training_d)
```

------------------

```{r run_horseshoe, cache=TRUE, depends="horseshoe_model"}
horseshoe_fit <- stan(model_code=horseshoe_block,
                      data=list(N=nrow(training_d),
                                p=ncol(training_d)-1,
                                y=(training_d$y 
                                   - median(training_d$y))
                                  /mad(training_d$y),
                                x=as.matrix(training_d[,-1])),
                      iter=1000,
                      control=list(adapt_delta=0.999,
                                   max_treedepth=15))
```

--------------

```{r summary_hs}
(hs_summary <- rstan::summary(horseshoe_fit, pars=c("b0", "sigma", "beta")))
```

--------------

First compare the resulting regression parameters to OLS values.

```{r compare_betas, echo=1:5, fig.width=3*fig.dim}
hs_samples <- extract(horseshoe_fit, pars=c("b0", "sigma", "beta"))
# rescale to real units
post_med_intercept <- median(hs_samples$b0) * mad(training_d$y) + median(training_d$y)
post_med_sigma <- median(hs_samples$sigma) * mad(training_d$y)
post_med_slopes <- colMedians(hs_samples$beta) * mad(training_d$y)

ols_ses <- summary(ols)$coefficients[,2]

layout(t(1:2))
plot(coef(ols), xlab="coefficient", 
     ylab="estimate", main="OLS", pch=20)
segments(x0=1:65,
         y0=coef(ols) - 2 * ols_ses,
         y1=coef(ols) + 2 * ols_ses,
         col=adjustcolor("black", 0.5))
plot(c(post_med_intercept, post_med_slopes), 
     xlab='coefficient', ylab="estimate",
     col='red', pch=20, main="sparse")
segments(x0=1, 
         y0=mad(training_d$y) * quantile(hs_samples$b0, probs=0.025) + median(training_d$y), 
         y1=mad(training_d$y) * quantile(hs_samples$b0, probs=0.975) + median(training_d$y), 
         col=adjustcolor('red', 0.5))
segments(x0=2:65, 
         y0=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.025), 
         y1=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.975),
         col=adjustcolor('red', 0.5))
```


--------------

The coefficient estimates from OLS are *wierd*.

```{r what_coefs, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$ols[-1]), decreasing=TRUE)),]
```

--------------

And, quite different than what Stan gets.

```{r what_coefs2, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$stan[-1]), decreasing=TRUE)),]
```



--------------

Now let's look at out-of-sample prediction error,
using the posterior median coefficient estimates:

```{r pred_stan}
pred_stan <- function (x) {
    post_med_intercept + as.matrix(x) %*% post_med_slopes
}
pred_y <- pred_stan(test_d[,-1])
stan_pred_error <- sqrt(mean((test_d$y - pred_y)^2))
stan_mse_resid <- sqrt(mean((training_d$y - pred_stan(training_d[,-1]))^2))

plot(test_d$y, pred_y, xlab="true values", ylab="predicted values", main="test data")
abline(0,1)
```

## Conclusions?


1. Our "sparse" model is certainly more sparse, and arguably more interpretable.

2. It has a root-mean-square prediction error of 
    `r stan_pred_error`
    on the *test* data, and
    `r stan_mse_resid`
    on the training data.

3. This is substantially better than ordinary linear regression, 
   which had a root-mean-square prediction error of `r ols_mse` on the test data,
   and a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` on the training data.


. . .

The sparse model is more interpretable,
and more generalizable.


# Exercises


## Pick a situation

1. Number of mosquitos caught in traps
   at 20 different time points at 4 locations;
   temperature and rainfall are also measured.

2. Transpiration rates of 5 trees each of 100 strains,
   along with genotype at five SNPs putatively linked to stomatal efficiency.

3. Presence or absence of *Wolbachia* parasites
   in fifty flies are sampled from each of 100 populations,
   along with the sex and transcription levels of ten immune-related genes of each fly.

4. HW1/2: exponential regression



# Case 1: Mosquitos

## The situation

Number of mosquitos caught in traps
at 20 different time points at 4 locations;
temperature and rainfall are also measured.

## Data


- $Z_i$ : number of mosquitos caught in trap $i$

- $\text{time}_i$: which day that trap $i$ was run on (out of 20) - categorical, between 1 and 20, same for each location

- $\text{loc}_i$: location (out of four) of trap $i$

- $\text{temp}_i$ : temperature at 7am when trap $i$ was run

- $\text{rain}_i$ : rainfall in previous 24hrs to running trap $i$

----------------

- include temp and rain as predictors

- use an exponential link function so that effects are multiplicative


$$\begin{aligned}
    Z_i &\sim \Poisson(\exp(\lambda_i)) \\
    \lambda_i &\sim \Normal(\mu_i, \sigma) \\
    \mu_i &= \exp\left( 
                  b_2 \times \text{rain}_i +
                  b_3 \times \text{temp}_i +
                  b_0[\text{loc}_i] 
                  + b_1[\text{time}_i] \\
    b_0 &\sim \Normal(\nu_0, \sigma_0) \\
    b_1 &\sim \Normal(\nu_1, \sigma_1) \\
\end{aligned}$$

----------------

```{r mosquito_stan}
moz_block <- "
data {
    int N; // number of trap runs
    int Z[N];  // number of mozzies
    int loc[N]; // location index
    int time[N]; // index of sampling day
    vector[N] temp; // temperature
    vector[N] rain; // rainfall
}
parameters {
    vector[N] lambda;
    real<lower=0> sigma;
    vector[4] b0;  // four locations
    vector[20] b1; // twenty times
    real b2;
    real b3;
    real nu0;
    real nu1;
    real<lower=0> sigma0;
    real<lower=0> sigma1;
}
model {
    vector[N] mu;
    Z ~ poisson_log(lambda);
    mu = b2 * rain + b3 * temp + b0[loc] + b1[time];
    lambda ~ normal(mu, sigma);
    b0 ~ normal(nu0, sigma0);
    b1 ~ normal(nu1, sigma1);
    b2 ~ normal(0, 10);
    b3 ~ normal(0, 10);
    nu0 ~ normal(0, 10);
    nu1 ~ normal(0, 10);
    sigma ~ normal(0, 10);
    sigma0 ~ normal(0, 10);
    sigma1 ~ normal(0, 10);
}"
```

## Simulate data

```{r sim_mozzies}
tp <- list(b2=0, b3=0.25, b0=1:4, b1=rep(0,20))
x <- expand.grid(time=1:20, loc=1:4)
x$temp <- 20 + rnorm(80, 5)
x$rain <- round(rgamma(80, 1/10, 1/10), 2)
x$mu <- exp(tp$b0[x$loc] + tp$b1[x$time] 
            + tp$b2 * x$rain + tp$b3 * x$temp)
x$Z <- rpois(80, x$mu)
```

## Run Stan

```{r stan_moz, cache=TRUE}
moz_data <- list(
              N=80,
              Z=x$Z,
              loc=x$loc,
              time=x$time,
              temp=(x$temp-mean(x$temp))/sd(x$temp),
              rain=(x$rain)/10)

moz_fit <- stan(model_code=moz_block,
                data=moz_data,
                iter=1000, chains=4)

```

*Conclusion:* it runs; but need to adjust things
to get good results.

# Case #2: trees

## The situation

Transpiration rates of 5 trees each of 100 strains,
along with genotype at five SNPs putatively linked to stomatal efficiency.

## Data

- $T_i$ : transpiration rate of tree $i$, for $1 \le i \le 500$

- $S_i$ : strain of tree $i$, an index between 1 and 100

- $G_{ij}$ : genotype of SNP $j$ in tree $i$, for $1 \le i \le 500$ and $1 \le j \le 5$;
  takes the value 0, 1, or 2.

---------------

To make things easier, compute
$$\begin{aligned}
    H_{ij} &= 1 \qquad \text{ if } G_{ij} = 1 \\
    D_{ij} &= 1 \qquad \text{ if } G_{ij} = 2
\end{aligned}$$
which are zero otherwise.

---------------


$$\begin{aligned}
    T_i &\sim \log\Normal(\mu_i, \sigma) \\
    \mu_i &= b_{0,S_i} + 
             b_{1,S_i} H_{i,1} +
             b_{2,S_i} H_{i,2} +
             b_{3,S_i} H_{i,3} +
             b_{4,S_i} H_{i,4} +
             b_{5,S_i} H_{i,5} 
             \\ & \qquad {} + 
             c_{1,S_i} D_{i,1}  +
             c_{2,S_i} D_{i,2}  +
             c_{3,S_i} D_{i,3}  +
             c_{4,S_i} D_{i,4}  +
             c_{5,S_i} D_{i,5} \\
    b_k[s] &= \text{effect of het SNP $k$ in strain $s$} \\
           &\sim \Cauchy(\nu_k, \tau_k) \\
    c_k[s] &= \text{effect of hom SNP $k$ in strain $s$} \\
           &\sim \Cauchy(\omega_k, u_k)  \\
    \nu &\sim \Normal(0, 10) \\
    \omega &\sim \Normal^+(0, 10) \\
    \tau &\sim \Normal(0, 10) \\
    u &\sim \Normal^+(0, 10)  \\
    \sigma &\sim \Normal^+(0, 20) 
\end{aligned}$$

## Simulate data

```{r sim_trees}
nsnps <- 5
nstrains <- 100
nindivs <- 5
trees <- data.frame(id=1:(nindivs * nstrains),
                    strain=rep(1:nstrains, each=nindivs))
# strain SNP proportions
snp_p <- matrix(rbeta(nsnps * nstrains, 0.5, 0.5), ncol=nsnps)
geno <- matrix(rbinom(nindivs*nstrains*nsnps, size=2, 
                      prob=snp_p[trees$strain,]), ncol=nsnps)
colnames(geno) <- paste0("g", 1:nsnps)
H <- (geno == 1)
D <- (geno == 2)

# transpiration
true_b0 <- rnorm(nstrains, mean=5, sd=0.1)
# snp effects
true_b <- c(2.0, 0, 0, 0, 0)/100
true_c <- c(4.0, 3.0, 0, 0, 0)/100
# strain * snp effects
true_bmat <- matrix(rep(true_b, each=nstrains), ncol=nsnps)
true_bmat[1:20, 3] <- -2.0/100
true_cmat <- matrix(rep(true_b, each=nstrains), ncol=nsnps)
true_cmat[1:20, 3] <- -4.0/100
# combined
trees$true_mu <- (true_b0[trees$strain] + 
                  rowSums(H * true_bmat[trees$strain,]) + 
                  rowSums(D * true_cmat[trees$strain,]))
# noise
true_sigma <- 0.05
trees$transpiration <- exp(rnorm(nstrains*nindivs, 
                                 mean=trees$true_mu, sd=true_sigma))
```


## Stan block

```{r tree_block}
tree_block <- "
data {
    int N; // number of trees
    int nsnps; // number of SNPs
    vector[N] T; // transp rates
    int S[N]; // strain index
    matrix[N, nsnps] H; // 0 or 1 if het
    matrix[N, nsnps] D; // 0 or 1 if hom
}
parameters {
    vector[100] b0;
    matrix[100,nsnps] b;
    matrix[100,nsnps] c;
    real<lower=0> sigma;
    vector[nsnps] nu;
    vector<lower=0>[nsnps] tau;
    vector[nsnps] omega;
    vector<lower=0>[nsnps] u;
}
model {
    vector[N] mu;
    mu = b0[S];
    for (j in 1:nsnps) {
        mu += (b[S,j] .* H[,j]) + (c[S,j] .* D[,j]);
        b[,j] ~ cauchy(nu[j], tau[j]);
        c[,j] ~ cauchy(omega[j], u[j]);
    }
    T ~ lognormal(mu, sigma);
    nu ~ normal(0, 10);
    omega ~ normal(0, 10);
    tau ~ normal(0, 10);
    u ~ normal(0, 10);
    sigma ~ normal(0, 20);
}"
```

---------

```{r run_trees, cache=TRUE}
tree_fit <- stan(model_code=tree_block,
                 data=list(N=nstrains*nindivs,
                           nsnps=nsnps,
                           T=trees$transpiration,
                           S=trees$strain,
                           H=matrix(as.numeric(H), ncol=nsnps),
                           D=matrix(as.numeric(D), ncol=nsnps)),
                 iter=1000)
```

