---
title: "Sparsifying priors, and variable selection"
author: "Peter Ralph"
date: "28 January 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```

```{r helpers, include=FALSE}
shadecurve <- function (pf, xlim, plot=TRUE, xlab='', ylab='', main='',
                        border="black", col=adjustcolor(border, 0.25), ...) {
    x <- seq(xlim[1], xlim[2], length.out=400)
    mids <- x[-1] - diff(x)/2
    df <- diff(pf(x, ...))
    if (plot) { plot(0, type='n', xlim=range(x), ylim=range(df),
                     main=main, xlab=xlab, ylab=ylab) }
    polygon(c(mids, x[length(x)], x[1]), c(df, 0, 0), col=col, border=border) 
}
```


# Overview

## Problems with linear models

1. "Too much" noise (i.e., non-Normal noise).

2. Too many variables.


## Problems with linear models

1. <del>"Too much" noise (i.e., non-Normal noise).</del>


2. Too many variables. **(today)**


# Variable selection


## Example data

![from Efron, Hastie, Johnstone, \& Tibshirani](lars_diabetes_data.png)

-----------

```{r lars_data}
library(lars)
data(diabetes)
class(diabetes$x2) <- "matrix"
```
```
diabetes                 package:lars                  R Documentation

Blood and other measurements in diabetics

Description:

     The ‘diabetes’ data frame has 442 rows and 3 columns. These are
     the data used in the Efron et al "Least Angle Regression" paper.

Format:

     This data frame contains the following columns:

     x a matrix with 10 columns

     y a numeric vector

     x2 a matrix with 64 columns
```

---------------

The dataset has

- 442 diabetes patients
- 10 main variables: age, gender, body mass index, average blood pressure (map), 
  and six blood serum measurements (tc, ldl, hdl, tch, ltg, glu)
- 45 interactions, e.g. `age:ldl`
- 9 quadratic effects, e.g. `age^2`
- measure of disease progression taken one year later: `y`

##

```{r show_cors, fig.width=2.3*fig.dim, fig.height=2.3*fig.dim, echo=FALSE}
pairs(cbind(diabetes$x, y=diabetes$y))
```

##

```{r print_cors}
cor(cbind(diabetes$x, y=diabetes$y))
```

## Crossvalidation plan

1. Put aside 20% of the data for *testing*.

2. Refit the model.

3. Predict the test data; compute
   $$\begin{aligned}
    S = \sqrt{\frac{1}{M} \sum_{k=1}^M (\hat y_i - y_i)^2}
   \end{aligned}$$

. . .

To be more thorough, we'd:

4. Repeat for the other four 20%s.

5. Compare.


## Crossvalidation

First let's split the data into testing and training just once:
```{r split_data}
test_indices <- (rbinom(length(diabetes$y), size=1, prob=0.2) == 1)
test_d <- cbind(data.frame(y=diabetes$y[test_indices]),
                diabetes$x2[test_indices,])
training_d <- cbind(data.frame(y=diabetes$y[!test_indices]),
                diabetes$x2[!test_indices,])
```

## Ordinary linear regression

```{r ols}
ols <- lm(y ~ ., data=training_d)
summary(ols)
```

-------------

```{r crossval_ols}
ols_pred <- predict(ols, newdata=test_d)
ols_mse <- sqrt(mean((ols_pred - test_d$y)^2))
```
With ordinary linear regression, we got a root-mean-square-prediction-error of `r ols_mse` (on the *test* data),
compared to a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` for the *training* data.

. . .

This suggests there's some overfitting going on.

------------

```{r plot_ols}
layout(t(1:2))
plot(training_d$y, predict(ols), xlab="true values", ylab="OLS predicted", main="training data")
abline(0,1)
plot(test_d$y, ols_pred, xlab="true values", ylab="OLS predicted", main="test data")
abline(0,1)
```

## A sparsifying prior

We have a lot of predictors: 64 of them.
A good guess is that only a few are really useful.
So, we can put a *sparsifying* prior on the coefficients,
i.e.,  $\beta$s in
$$\begin{aligned}
    y = \beta_0 + \beta_1 x_1 + \cdots \beta_n x_n + \epsilon
\end{aligned}$$


# Interlude

## 

<!-- from https://www.tandfonline.com/doi/full/10.1080/24749508.2018.1481633 -->

![Estimation of infiltration rate from soil properties using regression model for cultivated land](infiltration_title.png){ width=50% }

![EIR = 14,195.35 - 141.75 (sand%) - 142.10 (silt%) - 142.56 (clay%)](infiltration_figure.png)

##

Use [the data](infiltration_data.tsv)
to try to reproduce their model:

```
BIR = 14,195.35 - 141.75 (sand%) - 142.10 (silt%) - 142.56 (clay%)
```

They're not wrong! What's going on?

```{r infil_data, include=FALSE}
head(infil <- read.table("infiltration_data.tsv", header=TRUE))
```


# Sparseness and scale mixtures

## Encouraging sparseness

Suppose we do regression with a *large* number of predictor variables.

. . .

The resulting coefficients are **sparse** if most are zero.

. . .

The idea is to "encourage" all the coefficients to be zero,
**unless**  they *really* want to be nonzero,
in which case we let them be whatever they want.

. . .

This tends to discourage overfitting.

-------------

> The idea is to "encourage" all the coefficients to be zero,
> **unless**  they *really* want to be nonzero,
> in which case we let them be whatever they want.


To do this, we want a prior which is very peak-ey at zero
*but* flat away from zero ("spike-and-slab").


-------------------

:::::::::::::: {.columns}
::: {.column width="50%"}


Compare the Normal

$$\begin{aligned}
    X \sim \Normal(0,1)
\end{aligned}$$

to the "exponential scale mixture of Normals",

$$\begin{aligned}
    X &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```{r scale_mixtures, fig.width=1.5*fig.dim, echo=FALSE}
shadecurve(pnorm, xlim=c(-5,5), main='Normal(0,1)')
sdvals <- qexp(seq(0,1,length.out=10)[-c(1,10)])
for (u in sdvals) {
    shadecurve(pnorm, xlim=c(-5,5), sd=u, plot=(u==sdvals[1]), main='Normal(0, Exp(1))')
}
```

:::
::::::::::::::

## Why use a scale mixture?

1. Lets the data choose the appropriate scale of variation.

2. Weakly encourages $\sigma$ to be small: so, 
   as much variation as possible is explained by *signal* instead of *noise*.

3. Gets you a prior that is more peaked at zero and flatter otherwise.


## Implementation

:::::::::::::: {.columns}
::: {.column width="50%"}


Note that

$$\begin{aligned}
    \beta &\sim \Normal(0,\sigma) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

is equivalent to

$$\begin{aligned}
    \beta &= \sigma \gamma \\
    \gamma &\sim \Normal(0,1) \\
    \sigma &\sim \Exp(1) .
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
    sigma ~ exponential(1);
}
```

is equivalent to

```
parameters {
    real gamma;
    real<lower=0> sigma;
}
transformed parameters {
    real beta;
    beta = gamma * sigma;
}
model {
    gamma ~ normal(0, 1);
    sigma ~ exponential(1);
}
```

The second version **is better** for Stan.


:::
:::::::::::::::

-------------

:::::::::::::: {.columns}
::: {.column width="50%"}

Why is it better?


```
parameters {
    real beta;
    real<lower=0> sigma;
}
model {
    beta ~ normal(0, sigma);
}
```

In the first, the optimal step size
*depends on `sigma`*.

:::
::: {.column width="50%"}

```{r sigma_phase, echo=FALSE, fig.width=1.5*fig.dim}
xx <- seq(0, 0.5, length.out=401)[-1]
yy <- seq(-4,4,length.out=401)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=x) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="beta")
contour(xx, yy, -log10(xy), add=TRUE)
xy <- outer(xx, yy, function (x,y) { dnorm(y, sd=1) * dexp(x) })
image(xx, yy, -log10(xy), xlab="sigma", ylab="gamma")
contour(xx, yy, -log10(xy), add=TRUE)
```

:::
:::::::::::

## A strongly sparsifying prior



:::::::::::::: {.columns}
::: {.column width="50%"}

The "horseshoe":

$$\begin{aligned}
    \beta_j &\sim \Normal(0, \lambda_j) \\
    \lambda_j &\sim \Cauchy(0, \tau) \\
    \tau &\sim \Unif(0, 1) 
\end{aligned}$$

:::
::: {.column width="50%"}

```
parameters {
    vector[p] d_beta;
    vector[p] d_lambda;
    real<lower=0, upper=1> tau;
}
transformed parameters {
    vector[p] beta;
    beta = d_beta .* d_lambda * tau;
}
model {
    d_beta ~ normal(0, 1);
    d_lambda ~ cauchy(0, 1);
    // tau ~ uniform(0, 1); // uniform
}
```

:::
:::::::::::

## The Cauchy as a scale mixture

Recall that if

$$\begin{aligned}
    \beta &\sim \Normal(0, 1/\sqrt{\lambda}) \\
    \lambda &\sim \Gam(1/2, 1/2)
\end{aligned}$$

then

$$\begin{aligned}
    \beta &\sim \Cauchy(0, 1).
\end{aligned}$$



# Using the horseshoe

## What's an appropriate noise distribution?

```{r show_y, echo=FALSE, fig.width=3*fig.dim, fig.height=1.5*fig.dim}
layout(t(1:2))
hist(diabetes$y, breaks=30, main="diabetes, response")
qqnorm(diabetes$y)
qqline(diabetes$y)
```

## Aside: quantile-quantile plots

The idea is to plot the *quantiles* of each distribution against each other.

If these are *datasets*, this means just plotting their *sorted values* against each other.

```{r qq, fig.width=3*fig.dim, echo=c(1,2,4)}
x <- rnorm(1e4)
y <- rbeta(1e4, 2, 2)
layout(t(1:3))
plot(sort(x), sort(y)); qqplot(x, y, main="qqplot"); qqnorm(y, main="qnorm")
```


## Regression with a horseshoe prior

Uses a [reparameterization](https://betanalpha.github.io/assets/case_studies/fitting_the_cauchy.html) of the Cauchy as a scale mixture of normals.


```{r horseshoe_model, cache=TRUE}
horseshoe_block <- "
data {
    int N;
    int p;
    vector[N] y;
    matrix[N,p] x;
}
parameters {
    real b0;
    vector[p] d_beta;
    vector[p] d_a;
    vector<lower=0>[p] d_b;
    real<lower=0, upper=1> tau;
    real<lower=0> sigma;
}
transformed parameters {
    vector[p] beta;
    vector[N] f;
    beta = d_beta .* d_a .* sqrt(d_b) * tau;
    f = b0 + x * beta;
}
model {
    y ~ normal(f, sigma);
    // HORSESHOE PRIOR:
    d_beta ~ normal(0, 1);
    d_a ~ normal(0, 1);
    d_b ~ inv_gamma(0.5, 0.5);
    // tau ~ uniform(0, 1); // uniform
    // priors on noise distribution:
    sigma ~ normal(0, 10);
}"
```

------------------

Note the data have already been normalized,
with the exception of $y$:

```{r data_summary}
summary(training_d)
```

------------------

```{r run_horseshoe, cache=TRUE, depends="horseshoe_model"}
horseshoe_fit <- stan(model_code=horseshoe_block,
                      data=list(N=nrow(training_d),
                                p=ncol(training_d)-1,
                                y=(training_d$y 
                                   - median(training_d$y))
                                  /mad(training_d$y),
                                x=as.matrix(training_d[,-1])),
                      iter=1000,
                      control=list(adapt_delta=0.999,
                                   max_treedepth=15))
```

--------------

```{r summary_hs}
(hs_summary <- rstan::summary(horseshoe_fit, pars=c("b0", "sigma", "beta")))
```

--------------

First compare the resulting regression parameters to OLS values.

```{r compare_betas, echo=1:5, fig.width=3*fig.dim}
hs_samples <- extract(horseshoe_fit, pars=c("b0", "sigma", "beta"))
# rescale to real units
post_med_intercept <- median(hs_samples$b0) * mad(training_d$y) + median(training_d$y)
post_med_sigma <- median(hs_samples$sigma) * mad(training_d$y)
post_med_slopes <- colMedians(hs_samples$beta) * mad(training_d$y)

ols_ses <- summary(ols)$coefficients[,2]

layout(t(1:2))
plot(coef(ols), xlab="coefficient", 
     ylab="estimate", main="OLS", pch=20)
segments(x0=1:65,
         y0=coef(ols) - 2 * ols_ses,
         y1=coef(ols) + 2 * ols_ses,
         col=adjustcolor("black", 0.5))
plot(c(post_med_intercept, post_med_slopes), 
     xlab='coefficient', ylab="estimate",
     col='red', pch=20, main="sparse")
segments(x0=1, 
         y0=mad(training_d$y) * quantile(hs_samples$b0, probs=0.025) + median(training_d$y), 
         y1=mad(training_d$y) * quantile(hs_samples$b0, probs=0.975) + median(training_d$y), 
         col=adjustcolor('red', 0.5))
segments(x0=2:65, 
         y0=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.025), 
         y1=mad(training_d$y) * colQuantiles(hs_samples$beta, probs=0.975),
         col=adjustcolor('red', 0.5))
```


--------------

The coefficient estimates from OLS are *wierd*.

```{r what_coefs, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$ols[-1]), decreasing=TRUE)),]
```

--------------

And, quite different than what Stan gets.

```{r what_coefs2, echo=FALSE}
options(scipen=3)
coef_df <- data.frame(ols=coef(ols), stan=c(c(b0=post_med_intercept), post_med_slopes))
coef_df[c(1,1+order(abs(coef_df$stan[-1]), decreasing=TRUE)),]
```



--------------

Now let's look at out-of-sample prediction error,
using the posterior median coefficient estimates:

```{r pred_stan}
pred_stan <- function (x) {
    post_med_intercept + as.matrix(x) %*% post_med_slopes
}
pred_y <- pred_stan(test_d[,-1])
stan_pred_error <- sqrt(mean((test_d$y - pred_y)^2))
stan_mse_resid <- sqrt(mean((training_d$y - pred_stan(training_d[,-1]))^2))

plot(test_d$y, pred_y, xlab="true values", ylab="predicted values", main="test data")
abline(0,1)
```

## Conclusions?


1. Our "sparse" model is certainly more sparse, and arguably more interpretable.

2. It has a root-mean-square prediction error of 
    `r stan_pred_error`
    on the *test* data, and
    `r stan_mse_resid`
    on the training data.

3. This is substantially better than ordinary linear regression, 
   which had a root-mean-square prediction error of `r ols_mse` on the test data,
   and a root-mean-square-error of `r sqrt(mean(resid(ols)^2))` on the training data.


. . .

The sparse model is more interpretable,
and more generalizable.


# Exercises


## Pick a situation

1. Number of mosquitos caught in traps
   at 20 different time points at 4 locations;
   temperature and rainfall are also measured.

2. Transpiration rates of 5 trees each of 100 strains,
   along with genotype at five SNPs putatively linked to stomatal efficiency.

3. Presence or absence of *Wolbachia* parasites
   in fifty flies are sampled from each of 100 populations,
   along with the sex and transcription levels of ten immune-related genes of each fly.

4. HW1/2: exponential regression


## Mosquitos: variables

**Big picture:**
We're sampling mosquitos
in a few separate (replicate) traps
once, in the day and at night, each month, 
at each of four locations.

**Main question:**
How do mosquito populations vary seasonally
and by time of day,
after controlling for temperature and rainfall?

## Mosquitos: story

We've chosen locations to have similar rainfall and temperature means
(so that location and rain/temp aren't confounded).

There are more mosquitos out at night than during the day,
and more when it is warmer and wetter,
but there is no effect of month given temperature and rainfall.
There is an overall mean difference in abundance
by location, due to unmeasured factors.

## Mosquitos: variables

- `count` : number of mosquitos caught (0-1000)
- `rainfall` : cm of rain in the last 12 hours ($< 10$cm)
- `temperature` : average degrees C last 12 hours (25--35 C)
- `location` : factor with four levels
- `time` : day or night
- `month` : categorical, 1--12 *(note: could be numeric/sinusoidal!)*
- `replicate` : which trap number within the month/time/location combination (up to 5)
  *(unused in the model)*

*Note:* we'll have around 480 observations and 20 variables (without interactions).

