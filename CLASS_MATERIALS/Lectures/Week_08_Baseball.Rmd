---
title: "Baseball data"
author: "Peter Ralph"
date: "19 November 2020 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```



# Baseball

## Baseball

We have [a dataset](data/BattingAverage.csv) of batting averages of baseball players,
having

1. name
2. position
3. number of at bats
4. number of hits


```{r basedata}
batting <- read.csv("data/BattingAverage.csv", header=TRUE, stringsAsFactors=TRUE)
head(batting)
```

------------------------

The *overall* batting average of the `r nrow(batting)` players is `r sum(batting$Hits)/sum(batting$AtBats)`.

Here is the average by position.
```{r by_pos}
batting %>% group_by(PriPos) %>% 
    summarise(num=n(), BatAvg=sum(Hits)/sum(AtBats)) %>% 
    select(PriPos, num, BatAvg)
```

## Questions?

1. What's the overall batting average?

2. Do some positions tend to be better batters?

3. How much variation in batting average is there between players of the same position?

. . .

**Exercise:**

*Come up with some quick-and-dirty statistics to answer these questions.*


## Everyone is the same

```{r start, cache=TRUE}
first_model <- "
data {
    int N;
    int hits[N];
    int at_bats[N];
}
parameters {
    real<lower=0, upper=1> theta;
}
model {
    hits ~ binomial(at_bats, theta);
    theta ~ beta(1, 1);
} "
first_fit <- stan(model_code=first_model, chains=3, iter=1000,
                  data=list(N=nrow(batting),
                            hits=batting$Hits,
                            at_bats=batting$AtBats))
```

-----------------

```{r start_res}
stan_hist(first_fit, bins=20)
```

## Every pitcher is the same

```{r pos_model, cache=TRUE}
pos_model <- "
data {
    int N;
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
}
parameters {
    real<lower=0, upper=1> theta[npos];
}
model {
    real theta_vec[N];
    for (k in 1:N) {
        theta_vec[k] = theta[position[k]];
    }
    hits ~ binomial(at_bats, theta_vec);
    theta ~ beta(1, 1);
} "
pos_fit <- stan(model_code=pos_model, chains=3, iter=1000,
                  data=list(N=nrow(batting),
                            hits=batting$Hits,
                            at_bats=batting$AtBats,
                            npos=nlevels(batting$PriPos),
                            position=as.numeric(batting$PriPos)))
```

-----------------

```{r pos_res, fig.width=2*fig.dim, fig.height=1.5*fig.dim}
theta_samples <- extract(pos_fit)$theta
layout(matrix(1:9, nrow=3))
for (k in 1:ncol(theta_samples)) {
    hist(theta_samples[,k], main=levels(batting$PriPos)[k], xlim=c(0.1, 0.3),
         col=adjustcolor('red',0.6), xlab='batting avg', freq=FALSE)
}
```

# Reparameterization

##

Recall that if
$$\begin{aligned}
    B \sim \Beta(\alpha, \beta)
\end{aligned}$$
then
$$\begin{aligned}
   \E[B] &= \frac{\alpha}{\alpha + \beta} \\ 
   \text{and}
   \sd[B] &= \frac{1}{\alpha + \beta} \sqrt{\frac{\alpha \beta}{\alpha + \beta + 1}}.
\end{aligned}$$

-----------

Gee, it'd be nice to think about it's *location* and *scale* instead.

. . .

Well, if
$$\begin{aligned}
   \mu &= \frac{\alpha}{\alpha + \beta} \quad \text{(the mean)}\\ 
   \kappa &= \alpha + \beta \quad \text{(the concentration)},
\end{aligned}$$
then
$$\begin{aligned}
    B \sim \Beta(\mu \kappa, (1-\mu) \kappa)
\end{aligned}$$
has
$$\begin{aligned}
   \E[B] &= \mu \\
   \text{and} \qquad
   \sd[B] &\propto \frac{1}{\kappa} .
\end{aligned}$$



# More Baseball

## Another model: spot the differences!


:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\mu_{p_i} \kappa_{p_i}, (1-\mu_{p_i})\kappa_{p_i}) \\
    \mu_p &\sim \Beta(1, 1) \\
    \kappa_p &\sim \Gam(0.1, 0.1) .
\end{aligned}$$

::::::::::::::
::: {.column width="50%"}


```{r fake_stan_model, eval=FALSE}
pos_model <- "
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
}
parameters {
    real<lower=0, upper=1> theta[N];
    real<lower=0, upper=1> mu[npos];
    real<lower=0> kappa[npos];
}
model {
    real alpha;
    real beta;
    hits ~ binomial(at_bats, theta);
    for (i in 1:N) {
        alpha = mu[position[i]] * kappa[position[i]];
        beta = (1 - mu[position[i]]) * kappa[position[i]];
        theta[i] ~ beta(alpha, beta);
    }
    mu ~ beta(1,1);
    kappa ~ gamma(0.1,0.1);
}"
```


```{r stan_model, cache=TRUE, include=FALSE}
pos_model <- "
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
}
parameters {
    real<lower=0, upper=1> theta[N];
    real<lower=0, upper=1> mu[npos];
    real<lower=0> kappa[npos];
}
model {
    real alpha;
    real beta;
    hits ~ binomial(at_bats, theta);
    for (i in 1:N) {
        alpha = mu[position[i]] * kappa[position[i]];
        beta = (1 - mu[position[i]]) * kappa[position[i]];
        theta[i] ~ beta(alpha, beta);
    }
    mu ~ beta(1,1);
    kappa ~ gamma(0.1,0.1);
} "
```

:::
:::::::::::




##


```{r fit_pos_model, cache=TRUE, dependson="stan_model"}
pos_model <- stan_model(model_code=pos_model)
system.time(pos_fit <- sampling(pos_model, chains=3, iter=100,
                            data=list(N=nrow(batting),
                                      hits=batting$Hits,
                                      at_bats=batting$AtBats,
                                      npos=nlevels(batting$PriPos),
                                      position=as.numeric(batting$PriPos))))
```

## Diagnostics

```{r print_fit}
print(pos_fit, pars=c("mu", "kappa"))
```

----------------

Is it mixing?

```{r plot_trace}
stan_trace(pos_fit, pars="mu")
```


## Run longer!

```{r fit_pos_model_again, cache=TRUE, dependson="stan_model"}
system.time(pos_fit <- sampling(pos_model, chains=3, iter=1000,
                            control=list(max_treedepth=15),
                            data=list(N=nrow(batting),
                                      hits=batting$Hits,
                                      at_bats=batting$AtBats,
                                      npos=nlevels(batting$PriPos),
                                      position=as.numeric(batting$PriPos))))
```

-------------

```{r print_fit_again}
print(pos_fit, pars=c("mu", "kappa"))
```


-------------

Is it mixing?

```{r plot_trace_again}
stan_trace(pos_fit, pars="mu")
```

-------------

```{r plot_kappa_again}
stan_trace(pos_fit, pars="kappa")
```

-------------

```{r pos_pairs, fig.width=3*fig.dim, fig.height=3*fig.dim}
pairs(pos_fit, pars=c(sprintf("mu[%d]", 1:3), sprintf("kappa[%d]", 1:3), "lp__"))
```


## Let's look at the results!

```{r first_hist}
stan_hist(pos_fit, pars="mu", bins=30) + xlim(0, 0.4)
```

-----------

With labels: position means $\mu_p$:

```{r plot_mu, echo=FALSE, fig.width=2.0*fig.dim, fig.height=1.5*fig.dim}
param_samples <- extract(pos_fit)
layout(matrix(1:9, ncol=3))
par(mar=c(4,3,1,1)+.1)
for (k in 1:9) {
    hist(param_samples$mu[,k], main=levels(batting$PriPos)[k],
         xlim=c(0, 0.4), xlab='batting average', ylab=expression(mu))
}
```

-----------

Position "concentrations", $\kappa_p$:

```{r plot_kappa, echo=FALSE, fig.width=2.0*fig.dim, fig.height=1.5*fig.dim}
xh <- hist(unlist(param_samples$kappa), plot=FALSE)
layout(matrix(1:9, ncol=3))
par(mar=c(4,3,1,1)+.1)
for (k in 1:9) {
    hist(param_samples$kappa[,k], main=levels(batting$PriPos)[k],
         breaks=xh$breaks, xlim=range(xh$breaks),
         xlab=if (k %% 3 == 0) 'batting average' else '', ylab=expression(kappa))
}
```



# Simulation

## Wouldn't it be nice if we knew the truth?

. . .

**Discuss:**

Write down a procedure to simulate data
that looks like the baseball data.


------------

```{r simulate_fake_data}
batting$post_mean <- colMeans(param_samples$theta)
batting$fake_hits <- rbinom(nrow(batting), 
                            size=batting$AtBats, 
                            prob=batting$post_mean)
```


:::::::: {.columns}
::: {.column width=50%}

*Option 1:* By simulating draws from the posterior mean on $\theta$,
we can check that our model is accurately describing the data.
Here is the real data against data simulated under the posterior mean $\theta$ values.
They look similar, which is good.

:::::::::::
::: {.column width=50%}

```{r plot_simfakedata, echo=FALSE, fig.width=fig.dim}
with(batting, plot(Hits, fake_hits, xlab='actual hits', ylab='simulated hits'))
```

:::
:::::::::::

------------

**But** if we want to know if we're accurately estimating $\mu$ and $\kappa$, 
then we have to start with them, and *simulate* $\theta$.
```{r simulate_with_thetas}
post_mean_mu <- colMeans(param_samples$mu)
post_mean_kappa <- colMeans(param_samples$kappa)
names(post_mean_mu) <- names(post_mean_kappa) <- levels(batting$PriPos)
batting$sim_theta <- rbeta(nrow(batting),
                           shape1=post_mean_mu[batting$PriPos] *
                                   post_mean_kappa[batting$PriPos],
                           shape2=(1-post_mean_mu[batting$PriPos]) * 
                                   post_mean_kappa[batting$PriPos])
batting$sim_theta_hits <- rbinom(nrow(batting), 
                                 size=batting$AtBats, 
                                 prob=batting$sim_theta)
```

------------

Fit the model to the *simulated* data:

```{r fit_sim_data, cache=TRUE}
sim_fit <- sampling(pos_model, chains=3, iter=1000, control=list(max_treedepth=13),
                data=list(N=nrow(batting),
                          hits=batting$sim_theta_hits,
                          at_bats=batting$AtBats,
                          npos=nlevels(batting$PriPos),
                          position=as.numeric(batting$PriPos)))
```

----------

Can we estimate $\mu$ and $\kappa$?
```{r check_sim_mu}
sim_samples <- rstan::extract(sim_fit)
layout(t(1:2))
boxplot(sim_samples$mu, main="posterior distribution of mu")
points(1:9, post_mean_mu, col='red', pch=20, cex=2)
boxplot(sim_samples$kappa, main="posterior distribution of kappa")
points(1:9, post_mean_kappa, col='red', pch=20, cex=2)
```


## General questions with simulated data

1. Does my statistical inference method work?

. . .


2. Do the credible intervals contain the true value? 

    (i.e., Is the method "well-calibrated"?)

    *They usually should.*

. . .

3. How wide are credible intervals, typically?

    This is (one kind of) **statistical power.** 


# Posterior predictive sampling

## How to choose parameter values for simulation?

It depends, but one good choice is
*from the posterior distribution* -
then you can see how the model behaves
with data close to the real data.

This is known as **posterior predictive sampling**.

---------------------------

![Kruschke figure 13.2: posterior predictive sampilng](images/post_pred_kruschke.png)


# Sharing power // Shrinkage

## Example

Suppose that I have a large pot of coins
that are all similar to each other.
I flip each one ten times, and record the number of Heads.
What is *each coin's* probability of Heads?

- Treated *separately*,
  we would be very uncertain about each coin.

- Together, they should tell us very accurately 
  what are *likely* values of $\theta$.

- This information can improve the estimate of each separate $\theta$.

- The more similar the coins are, the more information we gain.

. . .

By *shrinking* each estimate towards their *common* mean,
we hope to gain *power*.


## Shrinkage and baseball

Some players were at bat very few times.
How does the information about their position affect our inference about their batting averages?

```{r baseball_shrinkage}
batting$post_med <- colMedians(param_samples$theta)
batting$post_Q1 <- colQuantiles(param_samples$theta, probs=0.25)
batting$post_Q3 <- colQuantiles(param_samples$theta, probs=0.75)
pos_means <- colMeans(param_samples$mu)
names(pos_means) <- levels(batting$PriPos)
pos_means
```

--------------

Pitchers had posterior mean $\mu$ of `r pos_means["Pitcher"]`

````{r pitchers}
with(subset(batting[order(batting$post_med),], PriPos=="Pitcher"), {
     plot(Hits / AtBats, main="Pitchers", xaxt='n', ylim=c(0, 0.4),
         xlab='player', ylab="posterior median theta");
     segments(x0=seq_along(Hits), y0=post_Q1, y1=post_Q3,
         col=adjustcolor('red',0.5));
     points(post_med, pch=20) })
abline(h=pos_means["Pitcher"], lwd=2, col=adjustcolor("blue", 0.5))
```

---------------

Catchers had posterior mean $\mu$ of `r pos_means["Catcher"]`

````{r catchers}
with(subset(batting[order(batting$post_med),], PriPos=="Catcher"), {
     plot(Hits / AtBats, main="Catchers", xaxt='n', ylim=c(0, 0.4),
         xlab='player', ylab="posterior median theta");
     segments(x0=seq_along(Hits), y0=post_Q1, y1=post_Q3,
         col=adjustcolor('red',0.5));
     points(post_med, pch=20) })
abline(h=pos_means["Catcher"], lwd=2, col=adjustcolor("blue", 0.5))
```

## Is shrinkage here a good idea?

With *simulated data*, compare median absolute error for

- posterior mean $\theta_i$

- empirical batting average

------------

How'd we do?  Let's look at the *true* values of $\theta$
(we know because we generated them)
against the posterior means.
Red lines are 95% credible intervals.
```{r check_sim_result}
post_sim_theta <- colMeans(sim_samples$theta)
plot(batting$sim_theta, post_sim_theta, type='n', xlab="true theta", ylab="posterior mean theta", ylim=c(0,0.4))
segments(x0=batting$sim_theta,  col='red',
         y0=colQuantiles(sim_samples$theta, probs=0.05),
         y1=colQuantiles(sim_samples$theta, probs=0.95))
points(batting$sim_theta, post_sim_theta, pch=20)
abline(0, 1, lwd=3, col=adjustcolor('blue', 0.5))
```

-------------

Did we do better?

The mean absolute difference between the *truth* and

- posterior mean: `r mean(abs(batting$sim_theta - post_sim_theta))`
- empirical batting average (hits/at bats): `r mean(abs(batting$sim_theta - batting$fake_hits/batting$AtBats))`

Using information about groups **reduced our error by a factor of 4!**


