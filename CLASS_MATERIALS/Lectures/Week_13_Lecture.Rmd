---
title: "ANOVA, and contingency tables"
author: "Peter Ralph"
date: "21 January 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```


# ANOVA

## Metric data from groups

Suppose we have numerical observations coming from $m$ different groups.
We'd like to know how different the means are between groups,
compared to variation within groups.

. . .

*For example:* 
dry leaf mass after 10d of growth
of $k=5$ genotypes in
in $m=4$ different conditions.
([data here](leaf_mass.tsv))

```{r simdata_ra, include=FALSE}
ntreat <- 4
ngeno <- 5
truth <- list(mu=200,
              geno=rnorm(ngeno, sd=5), 
              treat=rnorm(ntreat, sd=5), 
              sigma=2.0)
n <- 200
leafs <- data.frame(geno = letters[sample.int(ngeno, size=n, replace=TRUE)],
                    treat = sample.int(ntreat, size=n, replace=TRUE))
leafs$y <- pmax(0, truth$mu + truth$geno[leafs$geno] + truth$treat[leafs$treat]
                + rnorm(n, sd=truth$sigma))
write.table(leafs, file="leaf_mass.tsv", row.names=FALSE)
```



-----------------


:::::::::::::: {.columns}
::: {.column width="50%"}

Dry leaf mass after 10d of growth
of $k=5$ genotypes in
in $m=4$ different conditions.

```r
leafs <- read.table("leaf_mass.tsv", 
                    header=TRUE)
boxplot(y ~ geno + treat, 
        main='dry leaf mass', 
        xlab='genotype.treatment',
        data=leafs)
```


:::
::: {.column width="50%"}

```{r plotdata_ra, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
leafs <- read.table("leaf_mass.tsv", 
                    header=TRUE)
boxplot(y ~ geno + treat, 
        main='dry leaf mass', 
        xlab='genotype.treatment',
        col=1:ngeno,
        las=3,
        data=leafs)
abline(v=ngeno * (1:(ntreat-1)) + 0.5, lty=3)
text(x=ngeno * (1:(ntreat)) - 2, y=220, labels=paste("treatment", 1:ntreat))
```

:::
::::::::::::::


-------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

Each group has a different mean;
noise about these is Normal with the same SD.
Group means are random deviations
from a mean determined additively by genotype and treatment.

1. Make the model.
2. What posterior(s) will we look at?


:::
::: {.column width="50%"}

```{r plotdata_ra4, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
boxplot(y ~ geno + treat, 
        main='dry leaf mass', 
        xlab='genotype.treatment',
        col=1:ngeno,
        las=3,
        data=leafs)
abline(v=ngeno * (1:(ntreat-1)) + 0.5, lty=3)
text(x=ngeno * (1:(ntreat)) - 2, y=220, labels=paste("treatment", 1:ntreat))
```

:::
::::::::::::::


## a stan model

```{r anova_model, cache=TRUE}
anova_model <- stan_model(model_code="
data {
    int N;
    int ngeno;
    int ntreat;
    vector[N] leaf;
    int geno[N];
    int treat[N];
}
parameters {
    real mu;
    vector[ngeno] alpha;
    vector[ntreat] beta;
    matrix[ngeno, ntreat] gamma;
    matrix<lower=0>[ngeno, ntreat] sigma;
    real<lower=0> sigma_gamma;
}
model {
    vector[N] muvec;
    vector[N] sigmavec;
    muvec = mu + alpha[geno] + beta[treat];
    for (k in 1:N) {
        muvec[k] += sigma_gamma * gamma[geno[k], treat[k]];
        sigmavec[k] = sigma[geno[k], treat[k]];
    }
    leaf ~ normal(muvec, sigmavec);
    // DOESN'T WORK: leaf ~ normal(muvec, sigma[geno, treat]);
    mu ~ normal(205, 10);
    alpha ~ normal(0, 10);
    beta ~ normal(0, 10);
    for (k in 1:ngeno) {
        // using Matt's trick here
        gamma[k] ~ normal(0, 1);
        sigma[k] ~ gamma(20, 10);
    }
    sigma_gamma ~ gamma(0.5, 0.5);
}
")
```

##

```{r do_sampling, cache=TRUE, dependson="anova_model"}
fit <- sampling(anova_model,
            data=list(N=nrow(leafs),
                      ngeno=ngeno,
                      ntreat=ntreat,
                      leaf=leafs$y,
                      geno=as.numeric(leafs$geno),
                      treat=as.numeric(leafs$treat)))
```

##

```{r print_fit}
print(fit)
```

## 

```{r plot_trace}
stan_trace(fit, pars=c("mu", "sigma_gamma", "alpha", "beta"))
stan_plot(fit, pars=c("mu", "sigma_gamma", "alpha", "beta"))
```

##

```{r plot_trace2}
stan_trace(fit, pars=c("gamma", "sigma"))

```

```{r more}

samples <- extract(fit)

# let's get the posterior distribution of
# the group mean for (genotype a, treatment 1)

samples <- extract(fit)

post_a1 <- samples$mu + samples$alpha[,1] + samples$beta[,1] + samples$gamma[,1,1]

post_means <- as.vector(samples$mu) + samples$gamma
for (k in 1:ngeno) {
    post_means[,k,] <- post_means[,k,] + samples$alpha[,k]
}
for (k in 1:ntreat) {
    post_means[,,k] <- post_means[,,k] + samples$beta[,k]
}

dim(post_means) <- c(dim(post_means)[1], prod(dim(post_means)[2:3]))
colnames(post_means) <- outer(levels(leafs$geno), levels(leafs$treat), paste, sep=".")
post_means <- as.data.frame(post_means)

```
















# Stochastic minute

## The Dirichlet distribution

A collection of $n$ nonnegative random numbers $(P_1, \ldots, P_n)$ 
*that sums to 1*
has a **Dirichlet($\alpha_1, \ldots, \alpha_n$)** distribution
if it has probability density
$$
  \frac{1}{B(\alpha)} \prod_{i=1}^n p_i^{\alpha_i - 1} .
$$

*Facts:*

> 1. If $U \sim \Beta(\alpha, \beta)$ then $(U, 1-U) \sim \Dirichlet(\alpha, \beta)$.
> 
> 2. If $Y_1, \ldots, Y_n$ are independent, Exponential with rates $\alpha_1, \ldots, \alpha_n$
>    then
>    $$
>    Y / \sum_{i=1}^n Y_i \sim \Dirichlet(\alpha_1, \ldots, \alpha_n) .
>    $$
>
> 3. Useful? Yes, if you need a distribution on numbers that sum to one 
>    (e.g., class proportions).

## Exercise

Simulate from a

- $\Dirichlet(1, 1, 1)$
- $\Dirichlet(100, 100, 100)$

and describe the difference.
*(Hint: plot these in the $(P_1, P_2)$ plane.)*


# Categorical data

## Hair and Eye color

```{r hair_eye_data}
data(HairEyeColor)
```
```
HairEyeColor             package:datasets              R Documentation

Hair and Eye Color of Statistics Students

Description:

     Distribution of hair and eye color and sex in 592 statistics
     students.

Usage:

     HairEyeColor
     
Format:

     A 3-dimensional array resulting from cross-tabulating 592
     observations on 3 variables.  The variables and their levels are
     as follows:

       No  Name  Levels                    
        1  Hair  Black, Brown, Red, Blond  
        2  Eye   Brown, Blue, Hazel, Green 
        3  Sex   Male, Female              
      
Details:

     The Hair x Eye table comes from a survey of students at the
     University of Delaware reported by Snee (1974).  The split by
     ‘Sex’ was added by Friendly (1992a) for didactic purposes.

     This data set is useful for illustrating various techniques for
     the analysis of contingency tables, such as the standard
     chi-squared test or, more generally, log-linear modelling, and
     graphical methods such as mosaic plots, sieve diagrams or
     association plots.

Source:

     <URL:
     http://euclid.psych.yorku.ca/ftp/sas/vcd/catdata/haireye.sas>

     Snee (1974) gives the two-way table aggregated over ‘Sex’.  The
     ‘Sex’ split of the ‘Brown hair, Brown eye’ cell was changed to
     agree with that used by Friendly (2000).

References:

     Snee, R. D. (1974).  Graphical display of two-way contingency
     tables.  _The American Statistician_, *28*, 9-12.  doi:
     10.2307/2683520 (URL: http://doi.org/10.2307/2683520).

```

----------------

```{r show_he, echo=FALSE}
HairEyeColor
```

---------------

```{r melt_he}
(haireye <- data.frame(hair = dimnames(HairEyeColor)[[1]][slice.index(HairEyeColor, 1)],
                      eye = dimnames(HairEyeColor)[[2]][slice.index(HairEyeColor, 2)],
                      sex = dimnames(HairEyeColor)[[3]][slice.index(HairEyeColor, 3)],
                      number = HairEyeColor))
```

## 

*Questions:* 

1. Are hair and eye color independent in this sample?
2. Do hair and eye color proportions differ by sex?


## Independence and multiplicativity

If hair and eye color are *independent*,
then probabilities of combinations are *multiplicative*:
$$\begin{aligned}
    \P\{\text{black hair and blue eyes}\} \\
    &\qquad =
        \P\{\text{black hair}\} \times \P\{\text{blue eyes}\given\text{black hair}\} \\
\end{aligned}$$

. . .

which by independence is
$$\begin{aligned}
    &\qquad =
        \P\{\text{black hair}\} \times \P\{\text{blue eyes}\}
\end{aligned}$$


## Multiplicativity to additivity

A model of *independence* will have a *multiplicative* form:
$$
    p_{ab} = p_a \times p_b .
$$

. . .

Set $\lambda = \log(p)$, so that
$$
    \lambda_{ab} = \lambda_a + \lambda_b .
$$


# Stochastic facts

## Poisson additivity

If we have Poisson-many things of two categories:
$$\begin{aligned}
    A &\sim \Poisson(a) \\
    B &\sim \Poisson(b)
\end{aligned}$$
then the total number of things is also Poisson:
$$
    A + B \sim \Poisson(a + b)
$$
and each thing chooses its type independently:
$$
    A \given A + B \sim \Binom\left(A+B, \frac{a}{a+b}\right) .
$$

# Back to hair and eye colors

## A model

For hair color $i$ and eye color $j$,
the number of students with that combination is
$$
    N_{ij} \sim \Poisson(\mu_{ij}) .
$$

. . .

If hair and eye color are independent of each other
and of sex, then
$$\begin{aligned}
    \mu_{ij}
    &=
    \exp\left( \alpha_i + \beta_j \right) .
\end{aligned}$$


## Nonindependence?

$$\begin{aligned}
    \mu_{ij}
    &=
    \exp\left( \alpha_i + \beta_j + \gamma_{ij} \right) ?
\end{aligned}$$

. . .

Is this identifiable?


## Nonindependence.

$$\begin{aligned}
    \mu_{ij} &= \exp\left( \delta_{ij} \right) \\
    \delta_{ij} &\sim \Normal( \alpha_i + \beta_j, \sigma) 
\end{aligned}$$

. . .

*Pick appropriate priors.*

