---
title: "Prior distributions and uncertainty"
author: "Peter Ralph"
date: "11 November -- Advanced Biological Statistics"
---


```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(matrixStats)
library(lme4)
library(rstan)
```


# Biased coins

## a motivating example

Suppose I have two trick coins:

* one (coin A) comes up heads 75% of the time, and 
* the other (coin B) only 25% of the time.

. . .

But, I lost one and I don't know which!
So, I flip it **10 times** and get **6 heads**.
*Which is it, and how sure are you?*

--------------

## Possible answers:

> 1. Er, probably coin (A)?
> 
> 2. Well,
>    $$\begin{aligned}
>    \P\{ \text{6 H in 10 flips} \given \text{coin A} \}
>    &= \binom{10}{6} (.75)^6 (.25)^4 \\
>    &= 0.146
>    \end{aligned}$$
>    and
>    $$\begin{aligned}
>    \P\{ \text{6 H in 10 flips} \given \text{coin B} \}
>    &= \binom{10}{6} (.25)^6 (.75)^4 \\
>    &= 0.016
>    \end{aligned}$$
>    ... so, probably coin (A)?

---------------

For a precise answer...

3. *Before flipping*, each coin seems equally likely.  Then

    $$\begin{aligned}
    \P\{ \text{coin A} \given \text{6 H in 10 flips} \}
    &= \frac{
    \frac{1}{2} \times 0.146
    }{
    \frac{1}{2} \times 0.146
    +
    \frac{1}{2} \times 0.016
    } \\
    &= 0.9
    \end{aligned}$$


# Probability

## Bayes' rule

$$\begin{aligned}
    \P\{B \given A\} = \frac{\P\{B\} \P\{A \given B\}}{ \P\{A\} } ,
\end{aligned}$$

where

> - $B$: possible model
> - $A$: data
> - $\P\{B\}$: prior weight on model $B$
> - $\P\{A \given B\}$: likelihood of data under $B$
> - $\P\{B\} \P\{A \given B\}$: posterior weight on $B$
> - $\P\{A\}$: total sum of posterior weights



# Breaking it down with more coins

##

Suppose instead I had 9 coins, with probabilities 10%, 20%, ..., 90%;
as before I flipped one 10 times and got 6 heads.
For each $\theta$ in $0.1, 0.2, \ldots, 0.8, 0.9,$ find
$$\begin{aligned}
    \P\{\text{ coin has prob $\theta$ } \given \text{ 6 H in 10 flips } \} .
\end{aligned}$$

*Question:* which coin(s) is it, and how sure are we?
(And, what does it mean when we say how sure we are?)





## Uniform prior

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}

```{r the_prior, echo=FALSE, fig.height=2.0*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```

:::
::::::::::::::

----------------------------

## Weak prior

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}

```{r weak_prior, echo=FALSE, fig.height=2.0*fig.dim}
theta <- (1:9)/10
prior <- (9:1)/45
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```

:::
::::::::::::::

----------------------------

## Strong prior

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}

```{r strong_prior, echo=FALSE, fig.height=2.0*fig.dim}
theta <- (1:9)/10
prior <- 2^(8:0)/511
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```

:::
::::::::::::::


## The likelihood: 6 H in 10 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}

```{r ten_flips, echo=FALSE, fig.height=2.0*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```

:::
::::::::::::::

----------------

## The likelihood: 30 H in 50 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}

```{r fifty_flips, echo=FALSE, fig.height=2.0*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(30, size=50, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```

:::
::::::::::::::


----------------

## The likelihood: 60 H in 100 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}

```{r 100_flips, echo=FALSE, fig.height=2.0*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(60, size=100, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```

:::
::::::::::::::

----------------

## The likelihood: 6,000 H in 10,000 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}

```{r ten_thou_flips, echo=FALSE, fig.height=2.0*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6000, size=10000, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
names(posterior) <- theta
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```

:::
::::::::::::::


# A question

## What is the right answer to the "coin question"?

:::::::::::::: {.columns}
::: {.column width="60%"}

Recall: there were nine possible values of $\theta$.


Which coin is it, and how sure are you?


*Possible types of answer:*

1. "best guess"
2. "range of values"
3. "don't know"


:::
::: {.column width="40%"}


```{r plot_pos}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
names(posterior) <- theta
barplot(posterior, xlab='true prob of heads', main='posterior probability')
```

:::
::::::::::::::

# Unknown coins

## Motivating example

Now suppose we want to estimate the probability of heads
for a coin *without* knowing the possible values.
(or, a disease incidence, or error rate in an experiment, ...)

We flip it $n$ times and get $z$ Heads.

The *likelihood* of this, given the prob-of-heads $\theta$, is:
$$p(z \given \theta) = \binom{n}{z}\theta^z (1-\theta)^{n-z} . $$

How to weight the possible $\theta$?
Need a flexible set of weighting functions, i.e.,
**prior distributions** on $[0,1]$.

. . .

* **Beta** distributions.

--------------

::: {.columns}
::::::: {.column width=50%}


What $\alpha$ and $\beta$ would we use for a $\Beta(\alpha, \beta)$ prior if:

- the coin is probably close to fair.

- the disease is probably quite rare.

- no idea whatsoever.

:::
::::::: {.column width=50%}

```{r beta_stuff, echo=FALSE, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim}
shadecurve <- function (pf, xlim, plot=TRUE, xlab='', ylab='', main='', yaxt='n',
                        border="black", col=adjustcolor(border, 0.25), ...) {
    x <- seq(xlim[1], xlim[2], length.out=401)
    mids <- x[-1] - diff(x)/2
    df <- diff(pf(x, ...))
    if (plot) { plot(0, type='n', xlim=range(x), ylim=range(df),
                     main=main, xlab=xlab, ylab=ylab, yaxt=yaxt) }
    polygon(c(mids, x[length(x)], x[1]), c(df, 0, 0), col=col, border=border) 
    return(invisible(list(x=x, y=df)))
}

a <- 20
b <- 40
par(mar=c(3,1,3,0)+.1)
xy <- shadecurve(pbeta, c(0, 1), shape1=a, shape2=b, border="blue", xlab='', ylab='')
title(main="Beta(α, β)")
abline(v=a/(a+b), lty=3, lwd=2)
text(x=a/(a+b)+0.05, y=max(xy$y)*0.8,
     labels=expression(mu == frac(alpha,alpha+beta)), pos=4)
qy <- xy$y[findInterval(qbeta(0.05, a, b), xy$x)]
lines(x=xy$x[diff(xy$y > qy) != 0],
      y=rep(xy$y[diff(xy$y > qy) > 0], 2), lwd=2)
text(x=xy$x[diff(xy$y > qy) < 0],
     y=xy$y[diff(xy$y > qy) > 0],
     labels=expression(sqrt(frac(mu * (1-mu), alpha + beta + 1))), pos=4)
```

:::
:::::::


## Beta-Binomial Bayesian analysis

If
$$\begin{aligned}
P &\sim \text{Beta}(a,b) \\
Z &\sim \text{Binom}(n,P) ,
\end{aligned}$$

then "miraculously",

$$\begin{aligned}
(P \given Z = z) \sim \text{Beta}(a+z, b+n-z) .
\end{aligned}$$


## Discuss:

We flip an odd-looking coin 100 times,
and get 65 heads.
What is it's true* probability of heads?

1. What prior to use?

2. Plot the prior and the posterior.

3. Is it reasonable that $\theta = 1/2$?

4. Best guess at $\theta$?

5. How far off are we, probably?

*Tools include:* `rbeta( )`


# Reporting uncertainty

## How do we communicate results?

If we want a *point estimate*:

1. posterior mean,
2. posterior median, or
3. maximum *a posteriori* estimate ("MAP": highest posterior density).

These all convey "where the posterior distribution is", more or less.

. . .

What about uncertainty?


## Credible region

**Definition:** A 95% *credible region* is a portion of parameter space
having a total of 95% of the *posterior probability*.

. . .

(same with other numbers for "95%")

## Interpretation \#1

If we construct a 95% credible interval for $\theta$
for each of many datasets;
*and* the coin in each dataset has $\theta$ drawn independently from the prior,
*then* the true $\theta$ will fall in its credible interval for 95% of the datasets.


## Interpretation \#2

If we construct a 95% credible interval for $\theta$ with a dataset,
and the distribution of the coin's true $\theta$ across many parallel universes
is given by the prior,
then the true $\theta$ will be in the credible interval
in 95% of those universes.



## Interpretation \#3

Given my prior beliefs (prior distribution),
the posterior distribution is the most rational${}^*$ 
way to update my beliefs to account for the data.

. . .

${}^*$ if you do this many times you will be wrong least often

. . .

${}^*$ **or** you will be wrong in the fewest possible universes


## But which credible interval?

**Definition:** The "95\% highest density interval" is the 95\% credible interval
with the highest posterior probability density at each point.

. . .

((back to the coins))


# Hierarchical coins

## Motivating problem: more coins


## Motivating problem: more coins

Suppose now we have data from $n$ different coins from the same source.
We don't assume they have the *same* $\theta$,
but don't know what its distribution is,
so try to *learn* it.

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\alpha, \beta) \\
    \alpha &\sim \Unif(0, 100) \\
    \beta &\sim \Unif(0, 100)
\end{aligned}$$

**Goal:** find the posterior distribution of $\alpha$, $\beta$.


## Binomial versus Beta-Binomial

What is different between:

1. Pick a value of $\theta$ at random from $\Beta(3,1)$.\
   Flip one thousand $\theta$-coins, 500 times each.

2. Pick one thousand random $\theta_i \sim \Beta(3,1)$ values.\
   Flip one thousand coins, one for each $\theta_i$, 500 times each.


--------------

```{r beta_or_binom, fig.height=1.2*fig.dim, fig.width=3*fig.dim}
ncoins <- 1000
nflips <- 100
theta1 <- rbeta(1,3,1)
binom_Z <- rbinom(ncoins, size=nflips, prob=theta1)
theta2 <- rbeta(ncoins,3,1)
bb_Z <- rbinom(ncoins, size=nflips, prob=theta2)
hist(binom_Z, breaks=30, col=adjustcolor("blue", 0.5), main='', xlim=c(0,nflips), freq=FALSE, xlab='number of Heads')
hist(bb_Z, breaks=30, col=adjustcolor("red", 0.5), add=TRUE, freq=FALSE)
legend("topleft", fill=adjustcolor(c('blue', 'red'), 0.5), legend=c('one theta', 'many thetas'))
```

---------------

**Problem:** Find the posterior distribution of $\alpha$ and $\beta$,
given some data $(Z_1, N_1), \ldots, (Z_k, N_k)$,
under the model:
$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\alpha, \beta) \\
    \alpha &\sim \Unif(0, 100) \\
    \beta &\sim \Unif(0, 100) .
\end{aligned}$$


. . .

**Problem:** we don't have a nice mathematical expression for the posterior distribution.


# MC Stan

## Stan

![Stanislaw Ulam](images/stan.jpeg){height=10em}


## The skeletal Stan program

```
data {
    // stuff you input
}
transformed data {
    // stuff that's calculated from the data (just once, at the start)
}
parameters {
    // stuff you want to learn about
}
transformed parameters {
    // stuff that's calculated from the parameters (at every step)
}
model {
    // the action!
}
generated quantities {
    // stuff you want computed also along the way
}
```


# Markov Chain Monte Carlo

## When you can't do the integrals: MCMC

**Goal:** 
Given:

- a model with parameters $\theta$,
- a prior distribution $p(\theta)$ on $\theta$, and
- data, $D$,


"find"/ask questions of the posterior distribution on $\theta$,

$$\begin{aligned}
    p(\theta \given D) = \frac{ p(D \given \theta) p(\theta) }{ p(D) } .
\end{aligned}$$

. . .

**Problem:** usually we can't write down an expression for this
(because of the "$p(D)$").

. . .

**Solution:**
we'll make up a way to *draw random samples* from it.

-------------

**Toy example:** 

*(from beta-binomial coin example)*

Do we think that $\theta < 0.5$?

*(before:)* 
```r
pbeta(0.5, post_a, post_b)
```

*(now:)* 
```r
mean(rbeta(1e6, post_a, post_b) < 0.5)
```


## How? Markov chain Monte Carlo!

i.e., "random-walk-based stochastic integration"


**Example:**
Gibbs sampling for uniform distribution on a region.
*(picture)*


## Overview of MCMC

Produces a random sequence of samples $\theta_1, \theta_2, \ldots, \theta_N$.

0. Begin somewhere (at $\theta_1$).

At each step, starting at $\theta_k$:

1. **Propose** a new location (nearby?): $\theta_k'$

2. Decide whether to **accept** it.

    - if so: set $\theta_{k+1} \leftarrow \theta_k'$
    - if not: set $\theta_{k+1} \leftarrow \theta_k$

3. Set $k \leftarrow k+1$; if $k=N$ then stop.

. . .

The magic comes from doing *proposals* and *acceptance* 
so that the $\theta$'s are samples from the distribution we want.

## Key concepts

- Rules are chosen so that $p(\theta \given D)$ is the *stationary* distribution
  (long-run average!) of the random walk (the "Markov chain").

- The chain must *mix* fast enough so the distribution of visited states
  *converges* to $p(\theta \given D)$.

- Because of *autocorrelation*, $(\theta_1, \theta_2, \ldots, \theta_N)$ 
  are not $N$ independent samples:
  they are roughly equivalent to $N_\text{eff} < N$ independent samples.

- For better *mixing*, acceptance probabilities should not be too high or too low.

- Starting *several chains* far apart can help diagnose failure to mix:
  Gelman's $r$ quantifies how different they are.


## Let's "walk around":

::: {.columns}
::::::: {.column width=50%}


In each group, have one *walker* directed by others:

0. Start somewhere at "random", then repeat:

1. Pick a random $\{N,S,E,W\}$.

2. Take a step in that direction,

    * *unless* you'd run into a wall or a table.


Take a screenshot of your path after:

- 10 steps
- 50 steps


:::
::::::: {.column width=50%}

![image of floor plan of Knight library](images/knight-library-floor-plan.png)

:::
:::::::


------------------------



::: {.columns}
::::::: {.column width=50%}

Imagine the heatmap of how much time your "walker" has spent in each place in the library.

**Question:** What distribution does this sample from?


:::
::::::: {.column width=50%}

![image of floor plan of Knight library](images/knight-library-floor-plan.png)

:::
:::::::

-------------------


**Now:**

1. Pick a random $\{N,S,E,W\}$.

2. Take a $1+\Poisson(5)$ number of steps in that direction,

    * *unless* you'd run into a wall.


Again, take a screenshot of your path after:

- 10 steps
- 50 steps


--------------------

Does it mix faster?

. . .

Would $1 + \Poisson(50)$ steps be better?


## How it works

Imagine the walkers are on a hill, and:

1. Pick a random $\{N,S,E,W\}$.

2. If 

    * the step is *uphill*, then take it.
    * the step is *downhill*, then flip a $p$-coin;
      if you get Heads, stay were you are.


What would *this* do?

. . .

Thanks to *Metropolis-Hastings*,
if "elevation" is $p(\theta \given D)$, 
then setting $p = p(\theta' \given D) / p(\theta \given D)$
makes the stationary distribution $p(\theta \given D)$. 



# Beta-Binomial with Stan

## First, in words:

We've flipped a coin 10 times and got 6 Heads.
We think the coin is close to fair, so put a $\Beta(20,20)$ prior on
it's probability of heads,
and want the posterior distribution.

$$\begin{aligned}
    Z &\sim \Binom(10, \theta) \\
    \theta &\sim \Beta(20, 20) 
\end{aligned}$$
What's our *best guess* at $\theta$?

-------------


:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z &\sim \Binom(10, \theta) \\
    \theta &\sim \Beta(20, 20) 
\end{aligned}$$

What's our *best guess* at $\theta$?

:::
::: {.column width="50%"}


```
data {
    // stuff you input
}
parameters {
    // stuff you want to learn 
    // the posterior distribution of
}
model {
    // the action!
}
```


:::
::::::::::::::



-------------

:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z &\sim \Binom(10, \theta) \\
    \theta &\sim \Beta(20, 20) 
\end{aligned}$$

What's our *best guess* at $\theta$?

:::
::: {.column width="50%"}

```
data {
    int N;   // number of flips
    int Z;   // number of heads
}
parameters {
    // stuff you want to learn 
    // the posterior distribution of
}
model {
    // the action!
}
```

:::
::::::::::::::

-------------

:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z &\sim \Binom(10, \theta) \\
    \theta &\sim \Beta(20, 20) 
\end{aligned}$$

What's our *best guess* at $\theta$?

:::
::: {.column width="50%"}


```
data {
    int N;   // number of flips
    int Z;   // number of heads
}
parameters {
    // probability of heads
    real<lower=0,upper=1> theta;  
}
model {
    // the action!
}
```


:::
::::::::::::::


-------------

:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z &\sim \Binom(10, \theta) \\
    \theta &\sim \Beta(20, 20) 
\end{aligned}$$

What's our *best guess* at $\theta$?

:::
::: {.column width="50%"}


```
data {
    int N;   // number of flips
    int Z;   // number of heads
}
parameters {
    // probability of heads
    real<lower=0,upper=1> theta;
}
model {
    Z ~ binomial(N, theta);
    theta ~ beta(20, 20);
}
```

:::
::::::::::::::


## Compiling Stan model, in R

```{r stan_setup, cache=TRUE}
library(rstan)
stan_block <- "
data {
    int N;   // number of flips
    int Z;   // number of heads
}
parameters {
    // probability of heads
    real<lower=0,upper=1> theta;
}
model {
    Z ~ binomial(N, theta);
    theta ~ beta(20, 20);
}
"
bb_model <- stan_model(
               model_code=stan_block)
```

## Optimization: maximum likelihood

With a uniform prior, the "maximum posterior" parameter values
are also the *maximum likelihood* values.

. . .

```r
> help(optimizing)

optimizing                package:rstan                R Documentation

Obtain a point estimate by maximizing the joint posterior

Description:

     Obtain a point estimate by maximizing the joint posterior from the
     model defined by class 'stanmodel'.

Usage:

     ## S4 method for signature 'stanmodel'
     optimizing(object, data = list(),

```


## Maximum posterior


```{r optim_rstan, cache=TRUE, dependson="stan_setup"}
library(rstan)
(fit <- optimizing(bb_model,  # stan model from above
                   data=list(N=10, Z=6)))

```

## The answer!

:::::::::::::: {.columns}
::: {.column width="50%"}

The *maximum a posteriori probability estimate* (MAP) is `r fit$par`.

```{r stan_check_fake, eval=FALSE}
post_fun <- function (p) {
    n <- 10; z <- 6; a <- b <- 20
    lik <- dbinom(z, size=n, prob=p)
    prior <- dbeta(p, a, b)
    return( prior * lik )
}
curve(post_fun, 0, 1, xlab=expression(theta), ylab='posterior prob')
points(fit$par, post_fun(fit$par), col='red', pch=20)
```

*Note:* If the prior was *uniform*, then this would be the
*maximum likelihood estimate* (MLE) also.

:::
::: {.column width="50%"}

```{r stan_check, echo=FALSE, fig.height=2*fig.dim}
post_fun <- function (p) {
    n <- 10; z <- 6; a <- b <- 20
    lik <- dbinom(z, size=n, prob=p)
    prior <- dbeta(p, a, b)
    return( prior * lik )
}
curve(post_fun, 0, 1, xlab=expression(theta), ylab='posterior prob')
points(fit$par, post_fun(fit$par), col='red', pch=20, cex=2)
```

:::
::::::::::::::


## Sampling from the posterior distribution


```{r run_rstan, cache=TRUE}
library(rstan)
fit <- stan(model_code=stan_block,  # stan block from above
            data=list(N=10, Z=6),
            chains=3, iter=10000)

```

---------------

`lp__` is the log posterior density.
Note `n_eff`.

```{r print_rstan}
print(fit)
```

---------------

Fuzzy caterpillars are good.

```{r trace_rstan}
stan_trace(fit)
```

---------------

Stan uses ggplot2.

```{r plot_rstan}
stan_hist(fit, bins=20) + xlim(0,1)
```

---------------

What's the posterior probability that $\theta < 0.5$?

```{r results_rstan}
samples <- extract(fit)
mean(samples$theta < 0.5)

# compare to analytic solution
pbeta(0.5, shape1=10+6, shape2=10+4)
```


# Hierarchical Coins

:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\alpha, \beta) \\
    \alpha &\sim \Unif(0, 100) \\
    \beta &\sim \Unif(0, 100)
\end{aligned}$$

:::
::: {.column width="50%"}


```
data {
    // the data (input)
}
parameters {
    // the parameters (output)
}
model {
    // how they are related
}
```

:::
::::::::::::::

-----------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\alpha, \beta) \\
    \alpha &\sim \Unif(0, 100) \\
    \beta &\sim \Unif(0, 100)
\end{aligned}$$

:::
::: {.column width="50%"}


```
data {
    int n;      // number of coins
    int N[n];   // number of flips
    int Z[n];   // number of heads
}
parameters {
    // the parameters (output)
}
model {
    // how they are related
}
```

:::
::::::::::::::

-----------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\alpha, \beta) \\
    \alpha &\sim \Unif(0, 100) \\
    \beta &\sim \Unif(0, 100)
\end{aligned}$$

:::
::: {.column width="50%"}


```
data {
    int n;      // number of coins
    int N[n];   // number of flips
    int Z[n];   // number of heads
}
parameters {
    // probability of heads
    real<lower=0,upper=1> theta[n];
    real<lower=0,upper=100> alpha;
    real<lower=0,upper=100> beta;
}
model {
    // how they are related
}
```

:::
::::::::::::::

-----------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\alpha, \beta) \\
    \alpha &\sim \Unif(0, 100) \\
    \beta &\sim \Unif(0, 100)
\end{aligned}$$

:::
::: {.column width="50%"}


```
data {
    int n;      // number of coins
    int N[n];   // number of flips
    int Z[n];   // number of heads
}
parameters {
    // probability of heads
    real<lower=0,upper=1> theta[n];
    real<lower=0, upper=100> alpha;
    real<lower=0, upper=100> beta;
}
model {
    Z ~ binomial(N, theta);
    theta ~ beta(alpha, beta);
    // uniform priors "go without saying"
    // alpha ~ uniform(0, 100);
    // beta ~ uniform(0, 100);
}
```

:::
::::::::::::::


## Demonstration



:::::::::::::: {.columns}
::: {.column width="50%"}

Data:
```
set.seed(23)
ncoins <- 100
true_theta <- rbeta(ncoins, 20, 50)
N <- rep(50, ncoins)
Z <- rbinom(ncoins, size=N, prob=true_theta)
```

Find the posterior distribution on `alpha` and `beta`:
check convergence with `print()` and `stan_trace()`,
then plot using `stan_hist()` and/or `stan_scat()`..


:::
::: {.column width="50%"}


```
data {
    int n;      // number of coins
    int N[n];   // number of flips
    int Z[n];   // number of heads
}
parameters {
    // probability of heads
    real<lower=0,upper=1> theta[n];
    real<lower=0,upper=100> alpha;
    real<lower=0,upper=100> beta;
}
model {
    Z ~ binomial(N, theta);
    theta ~ beta(alpha, beta);
    // uniform priors 'go without saying'
    // alpha ~ uniform(0, 100);
    // beta ~ uniform(0, 100);
}
```

:::
::::::::::::::


