---
title: "Design of Experiments, and Analysis of Variance"
author: "Peter Ralph"
date: "8 October -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```

# Outline

## Goal for the week

To compare means of some measurement between groups.

. . .

Using statistics.

. . .

- When can you do it, and how well? Power/sensitivity, false positive rate.
- How can experiments best do it? Experimental design.
- Methods: two-sample $t$-test, (one-way) ANOVA, permutation tests

# Means of two groups

## 30 min

- introduce the idea
- apply to simple example // group discussion: interpret `t.test` output
- plot data, notice outlier
- alternative: do permutation test (demo in R?)


# Power and false positives

## 35 min

- definitions: depends on the model!
- group discussion: examples when each are high, low
- demo: obtain power for two-sample $t$ by simulation
- group ex: same for false pos

# Experimental design

## 40 min

- effect size: rules of thumb for detectable/not
- confounding factors
- representative samplesj
- how to ask: what will we *actually* measure/test?
- tools: replication, independence, controls, randomization, pairs
- group exercise: design an experiment to do X - identify confounders, deal with them

<!-- THURSDAY -->

# Tidy data

## 20 min

- rectangular, semantic
- wide vs long
- metadata!
- group ex: design data format for experiment

# ANOVA

## 50 min

- goal
- example and visualization
- on-board disc of power, sens
- analyze example
- testing assumptions
- demo: verify by simulation properties when asummptions hold (maybe unequal variance?)
- group ex: see what happens when they don't


# Permutation tests

## 30 min

- definition
- demo: do it by defining a function
- group discussion: figure out how to do it in some examples



# Two variables

## "Second, look at the data"

```{r biplot}
n <- 200
tz <- data.frame(t=seq(0, 10, length.out=n))
tz$z <- cos(tz$t) + rnorm(n)/4

(ggp <- ggplot(tz, mapping=aes(x=t, y=z)) + xlab("time") + ylab("value") + 
    geom_point())
```

## "Smooths"

```{r biplot2}
(ggp +
    geom_smooth(method='loess', formula=y ~ x, span=0.2))
```

. . .

That line is produced by `loess`, which does *local weighted regression*.


## Lines?

```{r biplot3}
(ggp +
    geom_smooth(method='lm', formula=y ~ x))
```

Is this a good idea?


# Correlation and regression

## Two variables

Now consider the results of an experiment:
green fluorescent protein (GFP) concentration
at eight temperatures ([data](../Datasets/gfp_temp.tsv)):

```{r twovars}
(xy <- read.table("../Datasets/gfp_temp.tsv")
```

## Correlation?

```{r cor}
cor(xy$temp, xy$gfp)
```

. . .

![xkcd:552](images/xkcd_correlation.png)

*Correlation doesn't imply causation,
but it does waggle its eyebrows suggestively and gesture furtively
while mouthing "look over there".*

## Have a look

```{r xyplot, fig.width=fig.dim}
(ggplot(xy, mapping=aes(x=temp, y=gfp)) + xlab("temperature") + ylab("GFP concentration")
    + geom_point()
    + geom_smooth(method='lm', se=FALSE))
```

(on board: how's least-squares regression work)



