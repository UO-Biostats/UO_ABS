---
title: "Dimension reduction: PCA, t-SNE and others"
author: "Peter Ralph"
date: "18 February 2020 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(rstan)
library(brms)
library(bayesplot)
library(matrixStats)
library(tidyverse)
options(mc.cores = parallel::detectCores())
options(digits=2)
```

# Multivariate data

## 

We need **multivariate statistics** when we're dealing with lots (more than one?) variables.

**Goals:** for this and next week:

1. *Describe* and visualize multivariate data.

2. *Distinguish* groups using many variables (e.g., cases from controls).

Both things involve *dimension reduction*,
because we see things in ${} \le 3$ dimensions,
and categorization is low-dimensional.


## The setting

We have observations of *many variables*:

$$ \mathbf{x}_i = (x_{i1}, x_{i2}, \ldots, x_{im}) . $$

so

$$ x_{ij} = (\text{measurement of $j^\text{th}$ variable in $i^\text{th}$ subject}) $$

-------------------

```{r data_setup, include=FALSE}
diabetes <- read.table("../Homeworks/diabetes_data.tsv", header=TRUE)[,1:11]
cc <- read.csv("../Datasets/Cream_Cheese/cream_cheese_profile.csv")
for (vn in c("Product.number", "Panellist", "Replicate", "Session")) { cc[[vn]] <- factor(cc[[vn]]) }
```

```{r show_dia}
head(diabetes, 20)
```

--------------------

```{r show_cc}
head(cc[,-(2:6)], 20)
```



# Principal component analysis (PCA)

## Primary aims of PCA

- **Variable reduction** - reduce to a small number of "composite" variables
    that adequately summarize the original information (*dimension reduction*). 

- **Ordination** - Reveal patterns in the data
    that could not be found by analyzing each variable separately.

## Quick example: diabetes data

```{r prcomp_dia, echo=FALSE, fig.width=2*fig.dim, fig.height=2*fig.dim}
dia_pc <- princomp(diabetes[,-1], cor=TRUE, scores=TRUE)
biplot(dia_pc, choices=1:2, col=c("grey", 'red'))
```

## Quick example: diabetes data

```{r dia2, fig.width=3*fig.dim, fig.height=1.8*fig.dim, echo=FALSE}
ycol <- function (x) {
    rx <- colorRamp(c("red", "orange", "blue"), space="Lab")(x/350)/256
    rgb(rx[,1], rx[,2], rx[,3])
}
layout(t(1:2))
plot(dia_pc$scores[,1:2], pch=20, cex=2,
     col=ycol(diabetes$y),
     xlab="PC1", ylab="PC2")
yleg <- (1:3)*100
legend("topright", pch=20, cex=2, col=ycol(yleg),
       legend=sprintf("y=%0.0f", yleg))
plot(dia_pc$scores[,1],  diabetes$y, pch=20, cex=2,
     col=ycol(diabetes$y),
     xlab="PC1", ylab="diabetes progression")
```

## Quick example: cream cheese

```{r prcomp_cc, echo=FALSE, fig.width=2*fig.dim, fig.height=2*fig.dim}
cc_pc <- princomp(cc[,-(1:7)], cor=TRUE, scores=TRUE)
biplot(cc_pc, choices=1:2, xlabs=paste(cc$Product.name, cc$Panellist, sep=":"), col=c("grey", "red"))
```

## Quick example: cream cheese

```{r cc2, fig.width=3.5*fig.dim, fig.height=1.8*fig.dim, echo=FALSE}
cc_cols <- RColorBrewer::brewer.pal(name="Paired", n=10)[-1]
names(cc_cols) <- levels(cc$Product.name)
layout(t(1:3), widths=c(0.4, 1, 1))
plot(0, type='n', xlab='', ylab='', xaxt='n', yaxt='n', bty='n')
legend("topleft", col=cc_cols, legend=names(cc_cols),
       pch=20, cex=2)
plot(cc_pc$scores[,1:2], pch=20, cex=2,
     xlab='PC1', ylab='PC2',
     col=cc_cols[match(cc$Product.name, names(cc_cols))])
plot(cc_pc$loadings[,1:2], type='n', xlab="PC1 loadings", ylab="PC2 loadings",
     xlim=1.3 * range(cc_pc$loadings[,1]))
arrows(x0=0, y0=0,
       x1=cc_pc$loadings[,1],
       y1=cc_pc$loadings[,2])
text(cc_pc$loadings[,1:2], xlab="PC1 loadings", ylab="PC2 loadings",
     col='red', cex=2,
     labels=colnames(cc)[-(1:7)])
```

# PCA: how it works

## Reducing dimesions

Say we've got a *data matrix* $x_{ij}$ of $n$ observations in $m$ variables,

but we want to make do with *fewer variables* - say, only $k$ of them.

. . .

**Idea:** Let's pick a few linear combinations of the variables
that best captures variability within the dataset.
For instance, strongly correlated variables will be combined into one.


## Notation:

1. These new variables are the *principal components*,
   $$u^{(1)}, \ldots, u^{(k)} . $$

2. The importance of each variable to the principal components  -
    i.e., the coefficients of the linear combination - 
    are the *loadings*,
    $$v^{(1)}, \ldots, v^{(k)} . $$

3. So, the position of the $i$th data point on the $\ell$th PC is
   $$ u_i^{(\ell)} = v_1^{(\ell)} x_{i1} + v_2^{(\ell)} x_{i2} + \cdots v_m^{(\ell)} x_{im} . $$


## Geometric interpretation

1. The loadings are the directions in multidimensional space
   that explain most variation in the data.

2. The PCs are the coordinates of the data
   along these directions.

3. These directions are *orthogonal*,
   and the resulting variables are *uncorrelated*.

## A model for PCA

The *approximation* to the data matrix using only $k$ PCs is:
$$
    x_{ij} \approx u_i^{(1)} v_j^{(1)} + u_i^{(2)} v_j^{(2)} + \cdots + u_i^{(k)} v_j^{(k)} .
$$

. . .

This is the *best possible* $k$-dimensional approximation,
in the least-squares sense,
so is the MLE for the low-dimesional model with Gaussian noise.

## Also known as

PCA is also called "eigenanalysis"
because (as usually computed),
the PCs are *eigenvectors* of the covariance matrix.

. . .

The *eigenvalues* are
$$ \lambda_\ell = \sum_{i=1}^n \left(u_i^{(\ell)}\right)^2, $$
and their sum is the *total variance*:
$$
\lambda_1 + \lambda_2 + \cdots + \lambda_m
=
\sum_{ij} (x_{ij} - \bar x_j)^2 .
$$


## Interpretation

- PCs: Observations that are *close* in PC space are similar.
- Loadings: high values indicate a strong correlation with that PC (positive or negative).

. . .

Sometimes PCs are rotated to improve *interpretability*.

## What next?

The PCs are *nice new variables*
you can use in any analysis!

. . .

Example: Does mouse body size correlate with the top three climate PCs?

# Beer

## Example: local beer

```{r read_beer}
beer <- read.csv("data/Beer_Specs.csv")
table(beer$Beer_Type)
beer_vars <- c("Volume", "CO2", "Color", "DO", "pH", "Bitterness_Units", "ABV", "Real_Extract", "Real_Degrees_Fermentation", "Final_Gravity")
head(beer[,c("Beer_Type", beer_vars)])
```

## Goals:

1. Describe major axes of variation between beer batches.
2. Look for variation not related to taste/style.
3. Make a pretty plot.


##

```{r beerpairs, fig.width=3*fig.dim, fig.height=2*fig.dim, echo=FALSE}
pairs(beer[,beer_vars])
```


# PCA: how to do it

## Practical considerations

1. Only use **numeric variables** - omit factors.
2. Variables should all be on the **same scale**, at least roughly.
3. Variables should also probably be **centered**.

. . . 

... however,

4. Beware outliers.
5. Transformations may be a good idea.
6. Missing data must be removed or imputed.
7. Consider replacing highly skewed variables with ranks.


## `?princomp`

```
princomp                 package:stats                 R Documentation

Principal Components Analysis

Description:

     ‘princomp’ performs a principal components analysis on the given
     numeric data matrix and returns the results as an object of class
     ‘princomp’.

Usage:

     princomp(x, ...)
     
     ## S3 method for class 'formula'
     princomp(formula, data = NULL, subset, na.action, ...)
     
     ## Default S3 method:
     princomp(x, cor = FALSE, scores = TRUE, covmat = NULL,
              subset = rep_len(TRUE, nrow(as.matrix(x))), fix_sign = TRUE, ...)
     
     ## S3 method for class 'princomp'
     predict(object, newdata, ...)
     
Arguments:

 formula: a formula with no response variable, referring only to
          numeric variables.

    data: an optional data frame (or similar: see ‘model.frame’)
          containing the variables in the formula ‘formula’.  By
          default the variables are taken from ‘environment(formula)’.

  subset: an optional vector used to select rows (observations) of the
          data matrix ‘x’.

na.action: a function which indicates what should happen when the data
          contain ‘NA’s.  The default is set by the ‘na.action’ setting
          of ‘options’, and is ‘na.fail’ if that is unset. The
          ‘factory-fresh’ default is ‘na.omit’.

       x: a numeric matrix or data frame which provides the data for
          the principal components analysis.

     cor: a logical value indicating whether the calculation should use
          the correlation matrix or the covariance matrix.  (The
          correlation matrix can only be used if there are no constant
          variables.)

  scores: a logical value indicating whether the score on each
          principal component should be calculated.

  covmat: a covariance matrix, or a covariance list as returned by
          ‘cov.wt’ (and ‘cov.mve’ or ‘cov.mcd’ from package ‘MASS’).
          If supplied, this is used rather than the covariance matrix
          of ‘x’.

fix_sign: Should the signs of the loadings and scores be chosen so that
          the first element of each loading is non-negative?

     ...: arguments passed to or from other methods. If ‘x’ is a
          formula one might specify ‘cor’ or ‘scores’.

  object: Object of class inheriting from ‘"princomp"’.

 newdata: An optional data frame or matrix in which to look for
          variables with which to predict.  If omitted, the scores are
          used.  If the original fit used a formula or a data frame or
          a matrix with column names, ‘newdata’ must contain columns
          with the same names. Otherwise it must contain the same
          number of columns, to be used in the same order.

Details:

     ‘princomp’ is a generic function with ‘"formula"’ and ‘"default"’
     methods.

     The calculation is done using ‘eigen’ on the correlation or
     covariance matrix, as determined by ‘cor’.  This is done for
     compatibility with the S-PLUS result.  A preferred method of
     calculation is to use ‘svd’ on ‘x’, as is done in ‘prcomp’.

     Note that the default calculation uses divisor ‘N’ for the
     covariance matrix.

     The ‘print’ method for these objects prints the results in a nice
     format and the ‘plot’ method produces a scree plot (‘screeplot’).
     There is also a ‘biplot’ method.

     If ‘x’ is a formula then the standard NA-handling is applied to
     the scores (if requested): see ‘napredict’.

     ‘princomp’ only handles so-called R-mode PCA, that is feature
     extraction of variables.  If a data matrix is supplied (possibly
     via a formula) it is required that there are at least as many
     units as variables.  For Q-mode PCA use ‘prcomp’.

Value:

     ‘princomp’ returns a list with class ‘"princomp"’ containing the
     following components:

    sdev: the standard deviations of the principal components.

loadings: the matrix of variable loadings (i.e., a matrix whose columns
          contain the eigenvectors).  This is of class ‘"loadings"’:
          see ‘loadings’ for its ‘print’ method.

  center: the means that were subtracted.

   scale: the scalings applied to each variable.

   n.obs: the number of observations.

  scores: if ‘scores = TRUE’, the scores of the supplied data on the
          principal components.  These are non-null only if ‘x’ was
          supplied, and if ‘covmat’ was also supplied if it was a
          covariance list.  For the formula method, ‘napredict()’ is
          applied to handle the treatment of values omitted by the
          ‘na.action’.

    call: the matched call.

na.action: If relevant.

Note:

     The signs of the columns of the loadings and scores are arbitrary,
     and so may differ between different programs for PCA, and even
     between different builds of R: ‘fix_sign = TRUE’ alleviates that.

References:

     Mardia, K. V., J. T. Kent and J. M. Bibby (1979).  _Multivariate
     Analysis_, London: Academic Press.

     Venables, W. N. and B. D. Ripley (2002).  _Modern Applied
     Statistics with S_, Springer-Verlag.

See Also:

     ‘summary.princomp’, ‘screeplot’, ‘biplot.princomp’, ‘prcomp’,
     ‘cor’, ‘cov’, ‘eigen’.

```


## 

```{r prcomp_beer}
beer_pc <- princomp(na.omit(beer[, beer_vars]), cor=TRUE, scores=TRUE)
str(beer_pc)
```

## What it means

```
> str(beer_pc)
List of 7
```

- `$ sdev`: standard deviations of the PCs
- `$ loadings`: loadings of the variables on each PC
- `$ center`: subtracted from columns of the data
- `$ scale`: divided from columns of the data
- `$ scores`: the actual PCs

## How many PCs should I use?

- The full covariance matrix is contained in the PCs.
- PCA gives you as many PCs as variables.

Depends - what for?

. . .

Some answers:

1. Visualization: as many as make sense (but beware overinterpretation).
2. Further analysis: as many as your method can handle.
3. Until an obvious break in the *scree plot*.

## The scree plot

Shows the *variance explained by each PC*
(these are always decreasing).

```{r scree, fig.width=2*fig.dim}
plot(beer_pc)
```

## The top four PCs

```{r plot_prcomp_beer, fig.width=3*fig.dim, fig.height=2*fig.dim}
layout(t(1:2))
biplot(beer_pc, choices=1:2, xlabs=na.omit(beer[,c("Beer_Type", beer_vars)])$Beer_Type)
biplot(beer_pc, choices=3:4, xlabs=na.omit(beer[,c("Beer_Type", beer_vars)])$Beer_Type)
```

## 

```{r plot_b1, echo=FALSE, fig.width=4.0*fig.dim, fig.height=1.8*fig.dim}
beer_cols <- c(RColorBrewer::brewer.pal(name="Paired", n=10), "#000000")
names(beer_cols) <- levels(beer$Beer_Type)
layout(t(1:3), widths=c(0.6, 1, 1))
plot(0, type='n', xlab='', ylab='', xaxt='n', yaxt='n', bty='n')
legend("topleft", col=beer_cols, legend=names(beer_cols),
       pch=20, cex=2)
plot(beer_pc$scores[,1:2], pch=20, cex=2,
     xlab='PC1', ylab='PC2',
     col=beer_cols[match(beer$Beer_Type, names(beer_cols))])
plot(beer_pc$loadings[,1:2], type='n', xlab="PC1 loadings", ylab="PC2 loadings",
     xlim=1.5 * range(beer_pc$loadings[,1]))
arrows(x0=0, y0=0,
       x1=beer_pc$loadings[,1],
       y1=beer_pc$loadings[,2])
text(beer_pc$loadings[,1:2], xlab="PC1 loadings", ylab="PC2 loadings",
     col='red', cex=2,
     labels=beer_vars)
```

## Gotchas

- PCs are only well-defined up to sign ($\pm$) and scale.
- Some programs report PCs normalized to have SD 1.
- Many ways to do basically the same thing: e.g., `prcomp` and `princomp`.

# Your turn: wine

## 

Using this [dataset of chemical concentrations in wine](data/wine.tsv):

```{r wine}
(wine <- read.table("data/wine.tsv", header=TRUE))
```

##

1. Look at the data.
2. Do PCA, and plot the results colored by vineyard.
3. What happens if you set `cor=FALSE`?
4. Which variables most strongly differentiate the three vineyards?



# PCA: Squaring the math and the code:

## 

The data matrix $X$ can be written
using the singular value decomposition,
as 
$$ X = U \Lambda V^T, $$
where $U^T U = I$ and $V^T V = I$
and $\Lambda$ 
has the *singular values* $\lambda_1, \ldots, \lambda_m$ on the diagonal.

The best $k$-dimensional approximation to $X$ is
$$ \hat X = \sum_{i=1}^k \lambda_i u_i v_i^T , $$
where $u_i$ and $v_i$ are the $i$th columns of $U$ and $V$.

Furthermore, 
$$ \sum_{ij} X_{ij} = \sum_i \lambda_i^2 . $$

## A translation table

1. $u_\ell$ is the $\ell$th PC, standardized.
2. $v_\ell$ gives the *loadings* of the $\ell$th PC.
3. $\lambda_\ell^2 / \sum_{ij} X_{ij}^2$ is the percent variation explained by the $\ell$th PC.

Furthermore, since $\Lambda U = X V$,

4. $\lambda_\ell u_\ell$ is the vector of values given by the linear combination
   of the variables with weights given by $v_\ell$.

## Translation to `prcomp`:

```{r check_pca}
X <- cc[,-(1:6)]
X.svd <- svd(X)
X.pca <- prcomp(X, center=FALSE, scale=FALSE)

# Singular values:
#  prcomps's sdevs are singular values / sqrt(n-1)
stopifnot(all(X.svd$d / sqrt(nrow(X) - 1) == X.pca$sdev))

# Loadings:
#  prcomp's loadings (returned as "rotation") are V
stopifnot(all(X.pca$rotation == X.svd$v))

# PCs:
#  prcomp's PCs (returned as "x")
#  are Lambda * U
stopifnot(all(abs(X.pca$x - X.svd$u * X.svd$d[col(X.svd$u)]) < 1e-10))
```

## Translation to `princomp`:

`princomp` uses eigendecomposition of the covariance matrix,
and so necessarily recenters the variables.

```{r check_princomp}
X.svd2 <- svd(scale(X, center=TRUE, scale=FALSE))
X.pca2 <- princomp(X, cor=FALSE, scores=TRUE)

# Singular values:
#  princomps's sdevs are singular values / sqrt(n)
stopifnot(all(abs(sqrt(eigen(cov(X))$values * (nrow(X)-1)/nrow(X)) - X.pca2$sdev) < 1e-10))
stopifnot(all(abs(X.svd2$d / sqrt(nrow(X)) - X.pca2$sdev) < 1e-10))

# Loadings:
#  princomp's loadings are V, up to sign
the_signs <- sign(X.pca2$loadings[1,] * X.svd2$v[1,])
stopifnot(all(abs(X.pca2$loadings - X.svd2$v * the_signs[col(X.svd2$v)]) < 1e-10))

# PCs:
#  princomp's PCs (returned as "scores")
#  are Lambda * U
stopifnot(all(abs(X.pca2$scores
                  - X.svd2$u * X.svd2$d[col(X.svd2$u)] * the_signs[col(X.svd2$u)]) < 1e-10))
```


# t-SNE

## t-SNE

![http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf](images/t-sne_abstract.png)

----------------

[t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) 
is a recently published method for dimension reduction
(visualization of struture in high-dimensional data).

-----------------

It [makes very nice visualizations](https://distill.pub/2016/misread-tsne/).

------------

The *generic idea* is to find a low dimensional representation
(i.e., a *picture*)
in which *proximity* of points
reflects *similarity* of the corresponding (high dimensional) observations.

-------------

Suppose we have $n$ data points, $x_1, \ldots, x_n$
and each one has $p$ variables: $x_1 = (x_{11}, \ldots, x_{1p})$.

. . .

For each, we want to find a low-dimensional point $y$:
$$\begin{aligned}
    x_i \mapsto y_i
\end{aligned}$$
similarity of $x_i$ and $x_j$
is (somehow) reflected by proximity of $y_i$ and $y_j$

. . .

We need to define:

0. "low dimension" (usually, $k=2$ or $3$)
1. "similarity" of $x_i$ and $x_j$
2. "reflected by proximity" of $y_i$ and $y_j$

## Similarity

To measure similarity, we just use (Euclidean) distance:
$$\begin{aligned}
    d(x_i, x_j) = \sqrt{\sum_{\ell=1}^n (x_{i\ell} - x_{j\ell})^2} ,
\end{aligned}$$
after first normalizing variables to vary on comparable scales.


## Proximity

A natural choice is to require distances in the new space ($d(y_i, y_j)$) 
to be as close as possible to distances in the original space ($d(x_i, x_j)$).

. . .

That's a *pairwise* condition.
t-SNE instead tries to measure how faithfully
*relative distances* are represented in each point's *neighborhood*.


## Neighborhoods 

For each data point $x_i$, define 
$$\begin{aligned}
    p_{ij} = \frac{ e^{-d(x_i, x_j)^2 / (2\sigma^2)} 
                  }{ \sum_{\ell \neq i} e^{-d(x_i, x_\ell)^2 / (2 \sigma^2)} } ,
\end{aligned}$$
and $p_{ii} =  0$.

This is the probability that point $i$ would pick point $j$ as a neighbor
if these are chosen according to the Gaussian density centered at $x_i$.


--------------

Similarly, in the output space, define
$$\begin{aligned}
    q_{ij} = \frac{ (1 + d(y_i, y_j)^2)^{-1} 
                  }{ \sum_{\ell \neq i} (1 + d(y_i, y_\ell)^2)^{-1} }
\end{aligned}$$
and $q_{ii} =  0$.


This is the probability that point $i$ would pick point $j$ as a neighbor
if these are chosen according to the Cauchy centered at $x_i$.


## Similiarity of neighborhoods

We want neighborhoods to look similar, i.e.,
choose $y$ so that 
$q$ looks like $p$.

To do this, we minimize
$$\begin{aligned}
    \text{KL}(p \given q)
    &=
    \sum_{i} \sum_{j} p_{ij} \log\left(\frac{p_{ij}}{q_{ij}}\right) .
\end{aligned}$$

. . .

*What's KL()?*

## Kullback-Leibler divergence

$$\begin{aligned}
    \text{KL}(p \given q)
    &=
    \sum_{i} \sum_{j} p_{ij} \log\left(\frac{p_{ij}}{q_{ij}}\right) .
\end{aligned}$$

This is the average log-likelihood ratio between $p$ and $q$
for a single observation drawn from $p$.

It is a measure of how "surprised" you would be by a sequence of samples from $p$
that you think should be coming from $q$.


. . .

**Facts:**

1. $\text{KL}(p \given q) \ge 0$

2. $\text{KL}(p \given q) = 0$ only if $p_i = q_i$ for all $i$.



# Simulate data

## A high-dimensional donut

Let's first make some data.
This will be some points distributed around a ellipse in $n$ dimensions.

```{r tsim_data, cache=TRUE}
n <- 20
npts <- 1e3
xy <- matrix(rnorm(n*npts), ncol=n)
theta <- runif(npts) * 2 * pi
ab <- matrix(rnorm(2*n), nrow=2)
ab[,2] <- ab[,2] - ab[,1] * sum(ab[,1] * ab[,2]) / sqrt(sum(ab[,1]^2))
ab <- sweep(ab, 1, sqrt(rowSums(ab^2)), "/")
for (k in 1:npts) {
    dxy <- 4 * c(cos(theta[k]), sin(theta[k])) %*% ab
    xy[k,] <- xy[k,] + dxy
}
```

---------------------

Here's what the data look like:
```{r plot_data, fig.width=6*fig.dim, fig.height=4*fig.dim, echo=FALSE}
pairs(xy, pch=20)
```

---------------------

But there is hidden, two-dimensional structure:
```{r plot_ring, fig.height=2*fig.dim, echo=FALSE}
plot(xy %*% t(ab), xlab='dimension 1', ylab='dimension 2', 
     pch=20, col=rainbow(nrow(xy))[rank(theta)])
```

---------------------

Now let's build the distance matrix.

```{r dist_mat}
dmat <- as.matrix(dist(xy)) 
```

# Implementation

The quantity to be minimized is Kullback-Leibler distance between $p$ and $q$,
defined as
$$\begin{aligned}
    \text{KL}(p|q) 
        &= \sum_x p_x \log(p_x/q_x) .
\end{aligned}$$

---------------------

## A Stan block

```{r stan_tsne, cache=TRUE}
tsne_block <- '
data {
    int N; // number of points
    int n; // input dimension
    int k; // output dimension
    matrix[N,N] dsq;  // distance matrix, squared
}
parameters {
    real<lower=0> sigma_sq; // in kernel for p 
    matrix[N-2,k] y1;
    real<lower=0> y21;
}
transformed parameters {
    matrix[N,k] y;
    y[1,] = rep_row_vector(0.0, k);
    y[3:N,] = y1;
    y[2,] = rep_row_vector(0.0, k);
    y[2,1] = y21;
}
model {
    matrix[N,N] q;
    real dt;
    matrix[N,N] p;
    q[N,N] = 0.0;
    for (i in 1:(N-1)) {
        q[i,i] = 0.0;
        for (j in (i+1):N) {
            q[i,j] = 1 / (1 + squared_distance(y[i], y[j]));
            q[j,i] = q[i,j];
        }
    }
    for (i in 1:N) {
        q[i] = q[i] / sum(q[i]);
    }
    // create p matrix
    p = exp(-dsq/(2*sigma_sq));
    for (i in 1:N) {
        p[i,i] = 0.0;
        p[i] = p[i] / sum(p[i]);
    }
    // compute the target
    for (i in 1:(N-1)) {
        for (j in (i+1):N) {
            target += (-1) * (p[i,j] .* log(p[i,j] ./ q[i,j]));
            target += (-1) * (p[j,i] .* log(p[j,i] ./ q[j,i]));
        }
    }
    sigma_sq ~ normal(0, 10);
}'


tk <- 2
tsne_model <- stan_model(model_code=tsne_block)
```

## Run the model with `optimizing`

```{r run_tsne, cache=TRUE, dependson=c("stan_tsne", "tsim_data")}
runtime <- system.time(tsne_optim <- optimizing(tsne_model,
                                 data=list(N=nrow(xy),
                                           n=ncol(xy),
                                           k=tk,
                                           dsq=(dmat/max(dmat))^2)))
runtime
```

## It works!

```{r plot_tsne, fig.width=4*fig.dim, fig.height=2*fig.dim, echo=FALSE}
out_y <- do.call(cbind, lapply(1:tk, function (k) {
                                   tsne_optim$par[sprintf("y[%d,%d]", 1:nrow(xy), k)]
                           } ) )
layout(t(1:2))
plot(out_y, xlab='t-sne 1', ylab='t-sne 2', main="t-SNE",
     pch=20, col=rainbow(nrow(xy))[rank(theta)])
plot(xy %*% t(ab), xlab='input 1', ylab='input 2', main="'truth'",
     pch=20, col=rainbow(nrow(xy))[rank(theta)])

```


# Another case

## High-dimensional random walk


```{r sim_rw, cache=TRUE}
n <- 40
npts <- 1e2
# rw <- colCumsums(matrix(rnorm(n*npts), ncol=n))
rw <- matrix(rnorm(n*npts), ncol=n)
for (i in 2:npts) {
    rw[i,] <- sqrt(.9) * rw[i-1,] + sqrt(.1) * rw[i,]
}
```

-----------------

Here's what the data look like:
```{r plot_rw, fig.width=6*fig.dim, fig.height=5*fig.dim, echo=FALSE}
pairs(rw, col=rainbow(npts), pch=20)
```

--------------------------

```{r plot_rw_sub, fig.width=6*fig.dim, fig.height=5*fig.dim, echo=FALSE}
pairs(rw[,1:10], col=rainbow(npts), pch=20)
```

-----------------

Now let's build the distance matrix.

```{r dist_mat_rw}
rw_dmat <- as.matrix(dist(rw)) 
```

## Run again

```{r run_tsne_rw, cache=TRUE, dependson=c("stan_tsne", "sim_rw")}
rw_runtime <- system.time(
     rw_tsne_optim <- optimizing(tsne_model,
                                 data=list(N=nrow(rw),
                                           n=ncol(rw),
                                           k=tk,
                                           dsq=(rw_dmat/max(rw_dmat))^2),
                                 tol_rel_grad=1e5))
rw_runtime
```

## It works!

```{r plot_rw_tsne, fig.width=2*fig.dim, fig.height=2*fig.dim, echo=FALSE}
rw_y <- do.call(cbind, lapply(1:tk, function (k) {
                                   rw_tsne_optim$par[sprintf("y[%d,%d]", 1:nrow(xy), k)]
                           } ) )
plot(rw_y, xlab='t-sne 1', ylab='t-sne 2', main="t-SNE",
     pch=20, col=rainbow(nrow(rw)))

```


# Overview of dimension reduction

## The menagerie

There are *many* ways to represent high-dimensional data in a few dimensions,
for instance:

- principal components analysis (PCA)
- non-negative matrix factorization (NMF)
- canonical correpondence analysis (CCA)
- principal coordinates analysis (PCoA)
- multidimensional scaling (MDS)
- redundancy analysis (RDA)
- Sammon mapping
- kernel PCA
- t-SNE
- UMAP
- locally linear embedding (LLE)
- Laplacian eigenmaps
- autoencoders

## Using distances or similarities?

PCA uses the *covariance matrix*, which measures similarity.

t-SNE begins with the matrix of *distances*, measuring dissimilarity.

## Metric or non-Metric?

Are distances interpretable?

. . .

*metric:* In PCA, each axis is a fixed linear combination of variables.
So, distances always mean the same thing no matter where you are on the plot.

. . .

*non-metric:* In t-SNE,
distances within different clusters are not comparable.


## Why ordination?

From [ordination.okstate.edu](http://ordination.okstate.edu/overview.htm),
about ordination in ecology:


1. Graphical results often lead to intuitive interpretations of species-environment relationships.

2. A single multivariate analysis saves time, in contrast to a separate univariate analysis for each species.

3. Ideally and typically, dimensions of this 'low dimensional space' will represent important and interpretable environmental gradients.

4. If statistical tests are desired, problems of multiple comparisons are diminished when species composition is studied in its entirety.

5. Statistical power is enhanced when species are considered in aggregate, because of redundancy.

6. By focusing on 'important dimensions', we avoid interpreting (and misinterpreting) noise.



## Beware overinterpretation


1. Ordination methods
    are strongly influenced by *sampling*:
    ordination may ignore large-scale patterns in favor of describing variation within a highly oversampled area.

2. Ordination methods also describe patterns common to many variables:
   measuring the same thing many times may drive results.

3. Many methods are designed to find clusters, because our brain likes to categorize things.
   This doesn't mean those clusters are well-separated in reality.


